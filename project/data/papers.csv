paperId,title,abstract,pages,DOI,link,citationCount,referenceCount,year
133bcd7488a3c07cb0f493a87564c30e5433768c,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",,852 - 857,10.1038/s41587-019-0209-9,https://www.semanticscholar.org/paper/133bcd7488a3c07cb0f493a87564c30e5433768c,9914,47,2019.0
fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",1591 - 1594,10.1080/14697688.2020.1813475,https://www.semanticscholar.org/paper/fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,2377,4,2020.0
c082ccfcfe1afc696e371374146ba9380b84061e,The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.",62,10.3390/bdcc7020062,https://www.semanticscholar.org/paper/c082ccfcfe1afc696e371374146ba9380b84061e,102,52,2023.0
ed6473fd5294a0639d661e02092768f364d80f39,What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",12 - 13,10.1145/3575663,https://www.semanticscholar.org/paper/ed6473fd5294a0639d661e02092768f364d80f39,28,57,2023.0
fb29359d794265c0931d756858a70c9265b5693d,The R Language: An Engine for Bioinformatics and Data Science,"The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",20-118,10.3390/life12050648,https://www.semanticscholar.org/paper/fb29359d794265c0931d756858a70c9265b5693d,66,109,2022.0
8bb6a6802027c7f2489accb0559e6f02984535c9,"Smart Health Intelligent Healthcare Systems in the Metaverse, Artificial Intelligence, and Data Science Era","In recent decades, healthcare organizations around the world have increasingly appreciated the value of information technologies for a variety of applications. Three of the new technological advancements that are impacting smart health are metaverse, artificial intelligence (AI), and data science. The metaverse is the intersection of three major technologies — AI, augmented reality (AR), and virtual reality (VR). Metaverse provides new possibilities and potential that are still emerging. The increased work efficiency enabled by artificial intelligence and data science in hospitals not only improves patient care but also cuts costs and workload for healthcare providers.The availability of big data enables data scientists to use the data for descriptive, predictive, and prescriptive analytics. This article reviews multiple case studies and the literature on AI and data science applications in hospital administration. The article also presents unresolved research questions and challenges in the applications of the metaverse, AI, and data science in the smart health context.",1-14,10.4018/joeuc.308814,https://www.semanticscholar.org/paper/8bb6a6802027c7f2489accb0559e6f02984535c9,40,45,2022.0
70fe060c12b3f100148d1a2be1e8f4254022543e,The case for data science in experimental chemistry: examples and recommendations,,357 - 370,10.1038/s41570-022-00382-w,https://www.semanticscholar.org/paper/70fe060c12b3f100148d1a2be1e8f4254022543e,29,112,2022.0
6b21f79054e41060dbc3303cad2667873b20e229,Data Science,,77-122,10.1007/978-3-319-20424-6,https://www.semanticscholar.org/paper/6b21f79054e41060dbc3303cad2667873b20e229,12,0,2022.0
88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",65-125,10.1017/9781108755528,https://www.semanticscholar.org/paper/88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,291,148,2020.0
108acf9a358512a40191d857e2456aeaaac3303b,Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",1231 - 1241,10.1101/gr.276496.121,https://www.semanticscholar.org/paper/108acf9a358512a40191d857e2456aeaaac3303b,5,98,2022.0
4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",3048-3061,10.1109/tkde.2019.2962680,https://www.semanticscholar.org/paper/4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,148,78,2021.0
370d248f97b75c4040e5828a658bbe4c3b80bf1e,Eleven grand challenges in single-cell data science,,24-164,10.1186/s13059-020-1926-6,https://www.semanticscholar.org/paper/370d248f97b75c4040e5828a658bbe4c3b80bf1e,716,397,2020.0
72d3ddf1f7210d7e70144bbc09f770ec411fe909,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",4-102,10.3390/info11040193,https://www.semanticscholar.org/paper/72d3ddf1f7210d7e70144bbc09f770ec411fe909,285,252,2020.0
b79ca6fd3df135a9bcf778844be625b764fbcfb3,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,,1247 - 1253,10.1038/s41592-019-0612-7,https://www.semanticscholar.org/paper/b79ca6fd3df135a9bcf778844be625b764fbcfb3,439,57,2019.0
ede0a8039a561905f40777ec2ae66c2010e3f2bc,Cybersecurity data science: an overview from machine learning perspective,,59-139,10.1186/s40537-020-00318-5,https://www.semanticscholar.org/paper/ede0a8039a561905f40777ec2ae66c2010e3f2bc,257,194,2020.0
9bcf291c6245a3c2ee101babf4c1f0bbfa166f92,Data science approach to stock prices forecasting in Indonesia during Covid-19 using Long Short-Term Memory (LSTM),,34-175,10.1186/s40537-021-00430-0,https://www.semanticscholar.org/paper/9bcf291c6245a3c2ee101babf4c1f0bbfa166f92,57,15,2021.0
7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",55-154,10.1145/3411764.3445526,https://www.semanticscholar.org/paper/7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,48,67,2021.0
44321686d59d889af1760357940f04fbb6629597,"The role of data science in healthcare advancements: applications, benefits, and future prospects",,1473 - 1483,10.1007/s11845-021-02730-z,https://www.semanticscholar.org/paper/44321686d59d889af1760357940f04fbb6629597,43,58,2021.0
4e1ef7f2928f4350249e58b7d884c41685ffa339,Data Science,"Les information détaillées à propos de chaque cours sont disponibles en cliquant sur le code cours. En particulier, l’horaire précis, jour par jour, et les locaux correspondants sont accessibles via la rubrique “Horaire”. Detailed information about each course unit is available by clicking the course code. In particular, the detailed schedule, day by day, and the corresponding classrooms are provided under the “Schedule” sub-title.",53-160,10.1201/9781003283249,https://www.semanticscholar.org/paper/4e1ef7f2928f4350249e58b7d884c41685ffa339,1,0,2022.0
f9d403c58db99e2214f43e5b1740694b9c79002f,"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large","Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",2091-2103,10.1145/3510003.3510057,https://www.semanticscholar.org/paper/f9d403c58db99e2214f43e5b1740694b9c79002f,34,133,2021.0
3df3bacc84593e9efb86d2c4d3ce30463048159f,Data Science,,41-147,10.1007/978-3-319-32010-6_60,https://www.semanticscholar.org/paper/3df3bacc84593e9efb86d2c4d3ce30463048159f,0,0,2022.0
8bba999de25bfb288b3f7f88e1d907aab02638b6,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",8066 - 8129,10.1021/ACS.CHEMREV.0C00004,https://www.semanticscholar.org/paper/8bba999de25bfb288b3f7f88e1d907aab02638b6,248,602,2020.0
2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",566-806,10.1561/2200000079,https://www.semanticscholar.org/paper/2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,118,402,2020.0
f9b0b10713044c146caa84704b66804aa1e82d5e,Automating data science,"Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process.",76 - 87,10.1145/3495256,https://www.semanticscholar.org/paper/f9b0b10713044c146caa84704b66804aa1e82d5e,21,63,2021.0
c13147ef0b86d5ec833c272840f8f3bdacf96e7f,Data science: a game changer for science and innovation,,263 - 278,10.1007/s41060-020-00240-2,https://www.semanticscholar.org/paper/c13147ef0b86d5ec833c272840f8f3bdacf96e7f,19,51,2021.0
ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",1 - 23,10.1145/3392826,https://www.semanticscholar.org/paper/ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,189,105,2020.0
0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",85-103,10.1109/TAI.2020.3020521,https://www.semanticscholar.org/paper/0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,196,245,2020.0
1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",1,10.7287/PEERJ.PREPRINTS.27295V1,https://www.semanticscholar.org/paper/1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,984,0,2018.0
f42c69dbd792155fee6f4d2c525971f8d43f138b,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",71-134,10.1145/3318464.3389726,https://www.semanticscholar.org/paper/f42c69dbd792155fee6f4d2c525971f8d43f138b,80,50,2020.0
5b9ea2abf1c5a04b3024367409284edceb741ef2,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",53-132,,https://www.semanticscholar.org/paper/5b9ea2abf1c5a04b3024367409284edceb741ef2,61,76,2020.0
a4b6f802b3f416fb1af6d723e0549c5e6d34faae,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",78-125,10.21203/rs.3.rs-91905/v1,https://www.semanticscholar.org/paper/a4b6f802b3f416fb1af6d723e0549c5e6d34faae,74,101,2020.0
648ba966b63975c6859e1948ae3ddc30053884e4,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",99-156,10.1177/2053951720939605,https://www.semanticscholar.org/paper/648ba966b63975c6859e1948ae3ddc30053884e4,51,59,2020.0
deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",3878-3905,10.1109/TSP.2021.3069677,https://www.semanticscholar.org/paper/deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,65,268,2020.0
7dcd9585d08eb40f043ae2ffccb86897a6a031e6,Supervised and Unsupervised Learning for Data Science,,60-179,10.1007/978-3-030-22475-2,https://www.semanticscholar.org/paper/7dcd9585d08eb40f043ae2ffccb86897a6a031e6,106,0,2020.0
12f62537251cf8eb76fa11c59df68d2211008898,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",743 - 767,10.1080/17538947.2020.1743785,https://www.semanticscholar.org/paper/12f62537251cf8eb76fa11c59df68d2211008898,68,80,2020.0
82620503cacf8ff6f8f3490e7bdf7508f1ab2021,Opening practice: supporting reproducibility and critical spatial data science,,477 - 496,10.1007/s10109-020-00334-2,https://www.semanticscholar.org/paper/82620503cacf8ff6f8f3490e7bdf7508f1ab2021,60,84,2020.0
8ece479b5dfed4727d2d9b9763f777bb9a94096e,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",1 - 24,10.1145/3359313,https://www.semanticscholar.org/paper/8ece479b5dfed4727d2d9b9763f777bb9a94096e,217,72,2019.0
398b154013db9d8025bf60f910bc156dedd9b40e,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",78-187,10.1145/3290605.3300356,https://www.semanticscholar.org/paper/398b154013db9d8025bf60f910bc156dedd9b40e,195,111,2019.0
a11e157cb828b800426223f0a3d79e8fb122c8cc,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",80-199,,https://www.semanticscholar.org/paper/a11e157cb828b800426223f0a3d79e8fb122c8cc,189,14,2019.0
0669286d8d4ca8ec2fdf16b7813157c21eb690be,Heidelberg colorectal data set for surgical data science in the sensor operating room,,65-146,10.1038/s41597-021-00882-2,https://www.semanticscholar.org/paper/0669286d8d4ca8ec2fdf16b7813157c21eb690be,56,27,2020.0
e57f360d4ffd1d3aa1dfbcc92d35b506f46f3afd,Data science and AI in FinTech: an overview,,81 - 99,10.1007/s41060-021-00278-w,https://www.semanticscholar.org/paper/e57f360d4ffd1d3aa1dfbcc92d35b506f46f3afd,41,66,2020.0
3b16bcb226bb1c87a6e63e0658be30067ed03f57,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,,57-165,10.1007/978-3-030-22475-2_1,https://www.semanticscholar.org/paper/3b16bcb226bb1c87a6e63e0658be30067ed03f57,219,107,2019.0
41cf91ee13a1d15983ede066ddf6b67cc94a41f4,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",69-175,10.1162/99608f92.dd363929,https://www.semanticscholar.org/paper/41cf91ee13a1d15983ede066ddf6b67cc94a41f4,36,11,2020.0
840b60da93c2776230d3e6123d708e1c7e66ebc0,Teaching Creative and Practical Data Science at Scale,"Abstract–Nolan and Temple Lang’s Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.",S27 - S39,10.1080/10691898.2020.1860725,https://www.semanticscholar.org/paper/840b60da93c2776230d3e6123d708e1c7e66ebc0,37,97,2020.0
d88d39dd9c910105e7503aa43698c806d42d5198,Statistical Foundations of Data Science,,40-149,10.1201/9780429096280,https://www.semanticscholar.org/paper/d88d39dd9c910105e7503aa43698c806d42d5198,102,22,2020.0
c016852106ac787678105fd9dd22e57ba620517c,Human Data Science,,90-105,10.1016/j.patter.2020.100069,https://www.semanticscholar.org/paper/c016852106ac787678105fd9dd22e57ba620517c,92,8,2020.0
579b64d2179a58a8bc586c30850ea238d3c14164,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",4586-4608,10.1109/TKDE.2020.3045927,https://www.semanticscholar.org/paper/579b64d2179a58a8bc586c30850ea238d3c14164,62,237,2020.0
d9a984e15b1a86a66ecbac9e66d458dae4cb616c,What Is Data Science,,29-58,10.1007/978-3-319-95092-1_2,https://www.semanticscholar.org/paper/d9a984e15b1a86a66ecbac9e66d458dae4cb616c,406,62,2018.0
27b9d1182e913decc7ef6a3509245fa6b6fd509d,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",3920 - 3929,10.1073/pnas.1901326117,https://www.semanticscholar.org/paper/27b9d1182e913decc7ef6a3509245fa6b6fd509d,117,92,2019.0
62a9d4f1763c5071cb2476c100614ba9741f036b,Data science applications to string theory,,1-117,10.1016/j.physrep.2019.09.005,https://www.semanticscholar.org/paper/62a9d4f1763c5071cb2476c100614ba9741f036b,98,181,2020.0
e2055b85dab66c922ccf25a28046e8e559074824,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",448-460,10.1093/COMJNL/BXY082,https://www.semanticscholar.org/paper/e2055b85dab66c922ccf25a28046e8e559074824,138,76,2019.0
4b5505a54799d796ae94115409b01ee33a7e2b20,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",612 - 616,10.1136/jech-2018-211654,https://www.semanticscholar.org/paper/4b5505a54799d796ae94115409b01ee33a7e2b20,31,65,2020.0
b017bf6879e57077b4b4e180a02747b89878d7a1,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",S16 - S26,10.1080/10691898.2020.1804497,https://www.semanticscholar.org/paper/b017bf6879e57077b4b4e180a02747b89878d7a1,31,52,2020.0
28cc044d5ba938472bc53d87240583982ad21663,Data Management for Data Science - Towards Embedded Analytics,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",86-148,,https://www.semanticscholar.org/paper/28cc044d5ba938472bc53d87240583982ad21663,27,26,2020.0
8e234be4cdc34ea8deba609c31858198ad941797,Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers,"Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work.",1-11,10.1109/VDS51726.2020.00005,https://www.semanticscholar.org/paper/8e234be4cdc34ea8deba609c31858198ad941797,28,90,2020.0
3746152e023e79b7d03cf12a560e473de2945d67,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",7-148,10.1145/3406865.3418584,https://www.semanticscholar.org/paper/3746152e023e79b7d03cf12a560e473de2945d67,21,35,2020.0
38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,"Data Science in 2020: Computing, Curricula, and Challenges for the Next 10 Years","Abstract In the past 10 years, new data science courses and programs have proliferated at the collegiate level. As faculty and administrators enter the race to provide data science training and attract new students, the road map for teaching data science remains elusive. In 2019, 69 college and university faculty teaching data science courses and developing data science curricula were surveyed to learn about their curricula, computing tools, and challenges they face in their classrooms. Faculty reported teaching a variety of computing skills in introductory data science (albeit fewer computing topics than statistics topics), and that one of the biggest challenges they face is teaching computing to a diverse audience with varying preparation. The ever-evolving nature of data science is a major hurdle for faculty teaching data science courses, and a call for more data science teaching resources was echoed in many responses.",S40 - S50,10.1080/10691898.2020.1851159,https://www.semanticscholar.org/paper/38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,24,41,2020.0
4271faaa82eb722d079222211c30ab642bc734be,The data science life cycle,A cycle that traces ways to define the landscape of data science.,58 - 66,10.1145/3360646,https://www.semanticscholar.org/paper/4271faaa82eb722d079222211c30ab642bc734be,20,35,2020.0
f56425ec56586dcfd2694ab83643e9e76f314e91,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",745 - 766,10.1080/10618600.2017.1384734,https://www.semanticscholar.org/paper/f56425ec56586dcfd2694ab83643e9e76f314e91,549,160,2017.0
89f41c87c8849ce37e609c1010087291a4679a37,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",64-188,10.1098/rstb.2018.0276,https://www.semanticscholar.org/paper/89f41c87c8849ce37e609c1010087291a4679a37,128,176,2019.0
9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,The State of the Art of Data Science and Engineering in Structural Health Monitoring,,21-135,10.1016/J.ENG.2018.11.027,https://www.semanticscholar.org/paper/9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,267,53,2019.0
1ec4d0e29455e47245edaa17368257df3efb6562,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",84-105,10.1145/3290605.3300493,https://www.semanticscholar.org/paper/1ec4d0e29455e47245edaa17368257df3efb6562,61,80,2019.0
2ad13329d44c74041626a60898ccf921b0bdacd3,SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle,"Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.",9-168,,https://www.semanticscholar.org/paper/2ad13329d44c74041626a60898ccf921b0bdacd3,48,83,2019.0
92b68d5a59262971d0f4a563c6abe1be6f2dab56,Data Science,,68-190,10.1007/978-1-4842-2253-9,https://www.semanticscholar.org/paper/92b68d5a59262971d0f4a563c6abe1be6f2dab56,11,35,2020.0
1ac524c713423bc50822b34e0aa1bbfab42d2b00,Data science for entrepreneurship research: studying demand dynamics for entrepreneurial skills in the Netherlands,,651 - 672,10.1007/s11187-019-00208-y,https://www.semanticscholar.org/paper/1ac524c713423bc50822b34e0aa1bbfab42d2b00,60,108,2019.0
678da221aa156807bc2c191ed5f4bcbb0b25d421,Data science ethical considerations: a systematic literature review and proposed project framework,,1-12,10.1007/s10676-019-09502-5,https://www.semanticscholar.org/paper/678da221aa156807bc2c191ed5f4bcbb0b25d421,55,59,2019.0
0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,,8-120,10.1007/s11306-019-1588-0,https://www.semanticscholar.org/paper/0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,54,65,2019.0
5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",4-163,10.3389/fenvs.2019.00121,https://www.semanticscholar.org/paper/5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,53,82,2019.0
2081ed6854290a479f796f2432c7951ff24232fe,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",81-113,10.1145/3290607.3299018,https://www.semanticscholar.org/paper/2081ed6854290a479f796f2432c7951ff24232fe,45,37,2019.0
0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",34-105,10.1146/ANNUREV-POLISCI-090216-023229,https://www.semanticscholar.org/paper/0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,47,150,2019.0
e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",771 - 780,10.1177/0961000617742465,https://www.semanticscholar.org/paper/e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,50,38,2019.0
b00f836c62d0ea7678d0f20aeec3397138633060,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",94-127,10.2196/13043,https://www.semanticscholar.org/paper/b00f836c62d0ea7678d0f20aeec3397138633060,54,27,2019.0
2ab2796390ac12df283e218907ed0ffef232dbc7,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",1 - 10,10.1080/10508406.2019.1705664,https://www.semanticscholar.org/paper/2ab2796390ac12df283e218907ed0ffef232dbc7,47,40,2019.0
b134d892f4e76081f5fa36b0b7c2e7118be53907,Genomics and data science: an application within an umbrella,,8-153,10.1186/s13059-019-1724-1,https://www.semanticscholar.org/paper/b134d892f4e76081f5fa36b0b7c2e7118be53907,45,103,2019.0
e799d31e1c2d80a971c1f956d62b98c0a9f27031,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",101-113,10.1111/bjet.12595,https://www.semanticscholar.org/paper/e799d31e1c2d80a971c1f956d62b98c0a9f27031,125,42,2019.0
eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,Bayesian Optimization and Data Science,,60-158,10.1007/978-3-030-24494-1,https://www.semanticscholar.org/paper/eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,99,0,2019.0
702cd9a7a128706b8a6ec88e7424e06c326021e5,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",88-198,10.1017/sus.2018.16,https://www.semanticscholar.org/paper/702cd9a7a128706b8a6ec88e7424e06c326021e5,84,263,2019.0
747359803e9a734fa4f1338a83121a942f3da60e,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",7-107,10.1111/GEAN.12194,https://www.semanticscholar.org/paper/747359803e9a734fa4f1338a83121a942f3da60e,39,109,2019.0
d6801d0ffd08ba56bccfa01884bb6a126f99de2e,"Our data, our society, our health: A vision for inclusive and transparent health data science in the United Kingdom and beyond","The last 6 years have seen sustained investment in health data science in the United Kingdom and beyond, which should result in a data science community that is inclusive of all stakeholders, working together to use data to benefit society through the improvement of public health and well‐being.",29-176,10.1002/lrh2.10191,https://www.semanticscholar.org/paper/d6801d0ffd08ba56bccfa01884bb6a126f99de2e,37,95,2019.0
863a35bdd1ae803491801e283c2ae79fe973cf68,Microbiome data science,,59-147,10.1007/s12038-019-9930-2,https://www.semanticscholar.org/paper/863a35bdd1ae803491801e283c2ae79fe973cf68,37,94,2019.0
f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,From hype to reality: data science enabling personalized medicine,,9-108,10.1186/s12916-018-1122-7,https://www.semanticscholar.org/paper/f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,260,83,2018.0
590ead4aeddbf8fea8414998b2dc3b74576a71cb,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",42 - 49,10.1080/09332480.2019.1579578,https://www.semanticscholar.org/paper/590ead4aeddbf8fea8414998b2dc3b74576a71cb,265,28,2018.0
bb44d1472bb281c699ef556f6eb6ccc66889f2d3,Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",97-194,10.1201/9780367816971,https://www.semanticscholar.org/paper/bb44d1472bb281c699ef556f6eb6ccc66889f2d3,31,0,2019.0
bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,Knowledge-based Biomedical Data Science 2019,"Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.","
          23-41
        ",10.1146/annurev-biodatasci-010820-091627,https://www.semanticscholar.org/paper/bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,32,206,2019.0
8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",42-191,10.1111/GEAN.12212,https://www.semanticscholar.org/paper/8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,34,108,2019.0
daec8baf1740a09725b375729d95caebc42f61c8,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",31-111,10.1145/3287324.3287522,https://www.semanticscholar.org/paper/daec8baf1740a09725b375729d95caebc42f61c8,30,3,2019.0
b85ac20631159ca3e370afa9c1f81a4618242b4f,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",1 - 7,10.1080/00031305.2019.1668849,https://www.semanticscholar.org/paper/b85ac20631159ca3e370afa9c1f81a4618242b4f,30,28,2019.0
4f0218eb9ed62d5acc03f02bfa24b388a66067e8,Distance geometry and data science,,271-339,10.1007/s11750-020-00563-0,https://www.semanticscholar.org/paper/4f0218eb9ed62d5acc03f02bfa24b388a66067e8,26,249,2019.0
31de1d23b2f1f1c14ce7025489d2892aa10fb1ef,A review of data science in business and industry and a future view,"The aim of this paper is to frame Data Science, a fashion and emerging topic nowadays in the context of business and industry. We open with a discussion about the origin of Data Science, and its requirement for a challenging mix of capability in data analytics, information technology and business know-how. The mission of Data Science is to provide new or revised computational theory able to extract useful information from the massive volumes of data collected at an accelerating pace. In fact, besides the traditional measurements, digital data obtained from images, text, audio, sensors, etc complement the survey. Then we review the different and most popular methodologies amongst the practitioners of Data Science research and applications. And since the emerging field requires personnel with new competences, we attempt to describe the Data Scientist profile, one of the sexiest jobs of the 21st Century according to Davenport and Patil. Most people are aware of the need to embrace Data Science, but they feel intimidated that they don’t understand it and they worry that their jobs will disappear. We want to encourage them: Data Science is more likely to add value to jobs and enrich the lives of working people by helping them make better, more informed business decisions. We conclude the paper by presenting examples of Data Science in action in business and industry, to demonstrate the collection of specialist skills that must come together for this new science to be effective.",35-199,10.1002/asmb.2488,https://www.semanticscholar.org/paper/31de1d23b2f1f1c14ce7025489d2892aa10fb1ef,24,37,2019.0
46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",422-441,10.1108/dta-05-2019-0076,https://www.semanticscholar.org/paper/46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,22,60,2019.0
36708c11c2fde2efb50e75d81f174b2c205082c8,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",20-141,10.1177/2053951719858114,https://www.semanticscholar.org/paper/36708c11c2fde2efb50e75d81f174b2c205082c8,23,24,2019.0
4aeda303fa0b9beae3f6d65e052dace9d4540116,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",241 - 257,10.1080/01930826.2019.1583015,https://www.semanticscholar.org/paper/4aeda303fa0b9beae3f6d65e052dace9d4540116,20,51,2019.0
2f972d51b7c1ee4a422b9c458ac373ddd3b722ea,"Data Science in Healthcare: Benefits, Challenges and Opportunities",,3-38,10.1007/978-3-030-05249-2_1,https://www.semanticscholar.org/paper/2f972d51b7c1ee4a422b9c458ac373ddd3b722ea,18,37,2019.0
fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,The 9 Pitfalls of Data Science,"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.",20-122,10.1093/oso/9780198844396.001.0001,https://www.semanticscholar.org/paper/fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,18,0,2019.0
08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",48-196,10.2139/ssrn.3370217,https://www.semanticscholar.org/paper/08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,19,12,2019.0
e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",2318-2331,10.1109/TKDE.2017.2720168,https://www.semanticscholar.org/paper/e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,766,93,2016.0
bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",67 - 72,10.1145/3188721,https://www.semanticscholar.org/paper/bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,100,15,2018.0
ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",249-265,10.23919/JSC.2021.0029,https://www.semanticscholar.org/paper/ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,93,179,2018.0
6bec0106bebc93fc30ec47af9779d7e327639034,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",71-189,10.1088/1361-648X/aa98bd,https://www.semanticscholar.org/paper/6bec0106bebc93fc30ec47af9779d7e327639034,110,267,2018.0
0a4b3c33e830d8cde364443a52e673c2c07dcfe8,Open Data Science,,31-39,10.1007/978-3-030-01768-2_3,https://www.semanticscholar.org/paper/0a4b3c33e830d8cde364443a52e673c2c07dcfe8,85,13,2018.0
fe5e345b7f027633cff866ba9a4d4e85af7e9448,An empirical study on data warehouse systems effectiveness: the case of Jordanian banks in the business intelligence era,"PurposeDespite the increasing role of the data warehouse as a supportive decision-making tool in today's business world, academic research for measuring its effectiveness has been lacking. This paucity of academic interest stimulated us to evaluate data warehousing effectiveness in the organizational context of Jordanian banks.Design/methodology/approachThis paper develops a theoretical model specific to the data warehouse system domain that builds on the DeLone and McLean model. The model is empirically tested by means of structural equation modelling applying the partial least squares approach and using data collected in a survey questionnaire from 127 respondents at Jordanian banks.FindingsEmpirical data analysis supported that data quality, system quality, user satisfaction, individual benefits and organizational benefits have made strong contributions to data warehousing effectiveness in our organizational data context.Practical implicationsThe results provide a better understanding of the data warehouse effectiveness and its importance in enabling the Jordanian banks to be competitive.Originality/valueThis study is indeed one of the first empirical attempts to measure data warehouse system effectiveness and the first of its kind in an emerging country such as Jordan.",39-173,10.1108/emjb-01-2022-0011,https://www.semanticscholar.org/paper/fe5e345b7f027633cff866ba9a4d4e85af7e9448,31,89,2022.0
82e1e8b222aeaca19d45375b31fdc825d1a821b8,An Overview of Data Warehouse and Data Lake in Modern Enterprise Data Management,"Data is the lifeblood of any organization. In today’s world, organizations recognize the vital role of data in modern business intelligence systems for making meaningful decisions and staying competitive in the field. Efficient and optimal data analytics provides a competitive edge to its performance and services. Major organizations generate, collect and process vast amounts of data, falling under the category of big data. Managing and analyzing the sheer volume and variety of big data is a cumbersome process. At the same time, proper utilization of the vast collection of an organization’s information can generate meaningful insights into business tactics. In this regard, two of the popular data management systems in the area of big data analytics (i.e., data warehouse and data lake) act as platforms to accumulate the big data generated and used by organizations. Although seemingly similar, both of them differ in terms of their characteristics and applications. This article presents a detailed overview of the roles of data warehouses and data lakes in modern enterprise data management. We detail the definitions, characteristics and related works for the respective data management frameworks. Furthermore, we explain the architecture and design considerations of the current state of the art. Finally, we provide a perspective on the challenges and promising research directions for the future.",132,10.3390/bdcc6040132,https://www.semanticscholar.org/paper/82e1e8b222aeaca19d45375b31fdc825d1a821b8,21,48,2022.0
02c3f4c16d85825727f0cfaa7eae91547a49fbf1,Data Warehouse Systems: Design and Implementation,,3-179,10.1007/978-3-662-65167-4,https://www.semanticscholar.org/paper/02c3f4c16d85825727f0cfaa7eae91547a49fbf1,34,0,2022.0
1f5f236479e262bb5a83a73b2b7c70a7bcd04176,Data Warehouse,,25-185,10.1007/978-3-319-32010-6_300078,https://www.semanticscholar.org/paper/1f5f236479e262bb5a83a73b2b7c70a7bcd04176,0,0,2022.0
20191e1d944b1630b6d3d7b804efe4114ca39fdd,data warehouse,,97-155,10.5040/9781501365287.734,https://www.semanticscholar.org/paper/20191e1d944b1630b6d3d7b804efe4114ca39fdd,0,0,2022.0
0cc65517cbcb31beb3af6eb2a3c0fdcd60856a8b,Data Warehouse,,29-144,10.1002/9780470114735.hawley04694,https://www.semanticscholar.org/paper/0cc65517cbcb31beb3af6eb2a3c0fdcd60856a8b,125,0,2019.0
63288534c93f8854a34e5511d9171c5f71ec2886,"COVID-WAREHOUSE: A Data Warehouse of Italian COVID-19, Pollution, and Climate Data","The management of the COVID-19 pandemic presents several unprecedented challenges in different fields, from medicine to biology, from public health to social science, that may benefit from computing methods able to integrate the increasing available COVID-19 and related data (e.g., pollution, demographics, climate, etc.). With the aim to face the COVID-19 data collection, harmonization and integration problems, we present the design and development of COVID-WAREHOUSE, a data warehouse that models, integrates and stores the COVID-19 data made available daily by the Italian Protezione Civile Department and several pollution and climate data made available by the Italian Regions. After an automatic ETL (Extraction, Transformation and Loading) step, COVID-19 cases, pollution measures and climate data, are integrated and organized using the Dimensional Fact Model, using two main dimensions: time and geographical location. COVID-WAREHOUSE supports OLAP (On-Line Analytical Processing) analysis, provides a heatmap visualizer, and allows easy extraction of selected data for further analysis. The proposed tool can be used in the context of Public Health to underline how the pandemic is spreading, with respect to time and geographical location, and to correlate the pandemic to pollution and climate data in a specific region. Moreover, public decision-makers could use the tool to discover combinations of pollution and climate conditions correlated to an increase of the pandemic, and thus, they could act in a consequent manner. Case studies based on data cubes built on data from Lombardia and Puglia regions are discussed. Our preliminary findings indicate that COVID-19 pandemic is significantly spread in regions characterized by high concentration of particulate in the air and the absence of rain and wind, as even stated in other works available in literature.",97-160,10.3390/ijerph17155596,https://www.semanticscholar.org/paper/63288534c93f8854a34e5511d9171c5f71ec2886,26,25,2020.0
8dd4e1d29e21f0d7ea77f15754a1df2d1ddaf000,"Medical Big Data Warehouse: Architecture and System Design, a Case Study: Improving Healthcare Resources Distribution",,1-16,10.1007/s10916-018-0894-9,https://www.semanticscholar.org/paper/8dd4e1d29e21f0d7ea77f15754a1df2d1ddaf000,37,32,2018.0
f5dfff2fcc0782bd3e5805a6f824674353400fb9,From Star Schemas to Big Data: 20+ Years of Data Warehouse Research,,93-107,10.1007/978-3-319-61893-7_6,https://www.semanticscholar.org/paper/f5dfff2fcc0782bd3e5805a6f824674353400fb9,37,134,2018.0
00a2aeb87287d59eaf118e96f0cdbb622a7fec42,The Snowflake Elastic Data Warehouse,"We live in the golden age of distributed computing. Public cloud platforms now offer virtually unlimited compute and storage resources on demand. At the same time, the Software-as-a-Service (SaaS) model brings enterprise-class systems to users who previously could not afford such systems due to their cost and complexity. Alas, traditional data warehousing systems are struggling to fit into this new environment. For one thing, they have been designed for fixed resources and are thus unable to leverage the cloud's elasticity. For another thing, their dependence on complex ETL pipelines and physical tuning is at odds with the flexibility and freshness requirements of the cloud's new types of semi-structured data and rapidly evolving workloads. We decided a fundamental redesign was in order. Our mission was to build an enterprise-ready data warehousing solution for the cloud. The result is the Snowflake Elastic Data Warehouse, or ""Snowflake"" for short. Snowflake is a multi-tenant, transactional, secure, highly scalable and elastic system with full SQL support and built-in extensions for semi-structured and schema-less data. The system is offered as a pay-as-you-go service in the Amazon cloud. Users upload their data to the cloud and can immediately manage and query it using familiar tools and interfaces. Implementation began in late 2012 and Snowflake has been generally available since June 2015. Today, Snowflake is used in production by a growing number of small and large organizations alike. The system runs several million queries per day over multiple petabytes of data. In this paper, we describe the design of Snowflake and its novel multi-cluster, shared-data architecture. The paper highlights some of the key features of Snowflake: extreme elasticity and availability, semi-structured and schema-less data, time travel, and end-to-end security. It concludes with lessons learned and an outlook on ongoing work.",59-161,10.1145/2882903.2903741,https://www.semanticscholar.org/paper/00a2aeb87287d59eaf118e96f0cdbb622a7fec42,222,55,2016.0
d982b20b110fcbc0968bb15779f3ab9bcf19d7d0,A clinician friendly data warehouse oriented toward narrative reports: Dr. Warehouse,,"
          52-63
        ",10.1016/j.jbi.2018.02.019,https://www.semanticscholar.org/paper/d982b20b110fcbc0968bb15779f3ab9bcf19d7d0,91,44,2018.0
fd1ce8caf8b2f79d93927d62878e44d3ca05a45f,Developing a standardized healthcare cost data warehouse,,54-124,10.1186/s12913-017-2327-8,https://www.semanticscholar.org/paper/fd1ce8caf8b2f79d93927d62878e44d3ca05a45f,74,45,2017.0
6dbcc8b895dc4cfcdbeef2f40df2d359430c63de,Architecture and Implementation of a Clinical Research Data Warehouse for Prostate Cancer,"Background: Electronic health record (EHR) based research in oncology can be limited by missing data and a lack of structured data elements. Clinical research data warehouses for specific cancer types can enable the creation of more robust research cohorts. Methods: We linked data from the Stanford University EHR with the Stanford Cancer Institute Research Database (SCIRDB) and the California Cancer Registry (CCR) to create a research data warehouse for prostate cancer. The database was supplemented with information from clinical trials, natural language processing of clinical notes and surveys on patient-reported outcomes. Results: 11,898 unique prostate cancer patients were identified in the Stanford EHR, of which 3,936 were matched to the Stanford cancer registry and 6153 in the CCR. 7158 patients with EHR data and at least one of SCIRDB and CCR data were initially included in the warehouse. Conclusions: A disease-specific clinical research data warehouse combining multiple data sources can facilitate secondary data use and enhance observational research in oncology.",40-177,10.5334/egems.234,https://www.semanticscholar.org/paper/6dbcc8b895dc4cfcdbeef2f40df2d359430c63de,34,31,2018.0
59a15d402c58700a36530310940ebb55b61b5fa7,Big Data Augmentation with Data Warehouse: A Survey,"With dynamic changes in world’s technology, an increasing growth and adoption observed in the usage of social media, computer networks, internet of things, and cloud computing. Research experiments are also generating huge amount of data which are to be collected, managed and analyzed. This huge data is known as ""Big Data"". Research analysts have perceived an increase in data that contains both useful and useless entities. In extraction of useful information, data warehouse finds difficulties in enduring with increasing amount of data generated. With shifts in paradigm, big data analytics emerged as promising area of research which supports business intelligence in terms of decision making. This paper provides a comprehensive survey on BigData, BigData problems, BigData Analytics and Big Data Warehouse. In addition, it also explains how the need for augmentation of big data and data warehouse emerged in perspective of decision making, comparing methods and research problems. It also elaborates applications which support Big Data, Data Warehouse, and its challenges.",2775-2784,10.1109/BigData.2018.8622206,https://www.semanticscholar.org/paper/59a15d402c58700a36530310940ebb55b61b5fa7,13,24,2018.0
e61b4479872ed8dbbea3786a17072fdd5c31f10a,Data Warehouse and Big Data Integration,"Big Data triggered furthered an influx of research and prospective on concepts and processes pertaining previously to the Data Warehouse field. Some conclude that Data Warehouse as such will disappear; others present Big Data as the natural Data Warehouse evolution (perhaps without identifying a clear division between the two); and finally, some others pose a future of convergence, partially exploring the possible integration of both. In this paper, we revise the underlying technological features of Big Data and Data Warehouse, highlighting their differences and areas of convergence. Even when some differences exist, both technologies could (and should) be integrated because they both aim at the same purpose: data exploration and decision making support. We explore some convergence strategies, based on the common elements in both technologies. We present a revision of the state-of-the-art in integration proposals from the point of view of the purpose, methodology, architecture and underlying technology, highlighting the common elements that support both technologies that may serve as a starting point for full integration and we propose a proposal of integration between the two technologies.",1-17,10.5121/IJCSIT.2017.9201,https://www.semanticscholar.org/paper/e61b4479872ed8dbbea3786a17072fdd5c31f10a,22,33,2017.0
3dfef91427af4649632d527f92775d34cb1b95a5,Data Cleaning In Data Warehouse: A Survey of Data Pre-processing Techniques and Tools,"A Data Warehouse is a computer system designed for storing and analyzing an organization's historical data from day-to-day operations in Online Transaction Processing System (OLTP). Usually, an organization summarizes and copies information from its operational systems to the data warehouse on a regular schedule and management performs complex queries and analysis on the information without slowing down the operational systems. Data need to be pre-processed to improve quality of data, before storing into data warehouse. This survey paper presents data cleaning problems and the approaches in use currently for preprocessing. To determine which technique of preprocessing is best in what scenario to improve the performance of Data Warehouse is main goal of this paper. Many techniques have been analyzed for data cleansing, using certain evaluation attributes and tested on different kind of data sets. Data quality tools such as YALE, ALTERYX, and WEKA have been used for conclusive results to ready the data in data warehouse and ensure that only cleaned data populates the warehouse, thus enhancing usability of the warehouse. Results of paper can be useful in many future activities like cleansing, standardizing, correction, matching and transformation. This research can help in data auditing and pattern detection in the data.",50-61,10.5815/IJITCS.2017.03.06,https://www.semanticscholar.org/paper/3dfef91427af4649632d527f92775d34cb1b95a5,21,27,2017.0
3cd521b5e249ee11d6991c66167f2561c1f9dc8b,MouseMine: a new data warehouse for MGI,,325 - 330,10.1007/s00335-015-9573-z,https://www.semanticscholar.org/paper/3cd521b5e249ee11d6991c66167f2561c1f9dc8b,117,8,2015.0
f0220044ffd26da17a3bb90508fd3430b6e1d6cf,Data Warehouse with Big Data Technology for Higher Education,,93-99,10.1016/J.PROCS.2017.12.134,https://www.semanticscholar.org/paper/f0220044ffd26da17a3bb90508fd3430b6e1d6cf,86,13,2017.0
7ef882c911a59e749145f0280577715541c88f22,Data Warehouse From Architecture To Implementation,"data warehouse from architecture to implementation is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the data warehouse from architecture to implementation is universally compatible with any devices to read.",90-124,,https://www.semanticscholar.org/paper/7ef882c911a59e749145f0280577715541c88f22,42,0,2016.0
e07e835944576cba7218abf6fb8b3c02e23b6942,Olap aggregation function for textual data warehouse,"For more than a decade, OLAP and multidimensional analysis have generated methodologies, tools and resource management systems for the analysis of numeric data. With the growing availability of semistructured data there is a need for incorporating text-rich document data in a data warehouse and providing adapted multidimensional analysis. This paper presents a new aggregation function for keywords allowing the aggregation of textual data in OLAP environments as traditional arithmetic functions would do on numeric data. The AVG_KW function uses an ontology to join keywords into a more common keyword.",151-156,10.5220/0002364401510156,https://www.semanticscholar.org/paper/e07e835944576cba7218abf6fb8b3c02e23b6942,38,25,2016.0
191e7d86e995560602853aacbc08fda8ae4b6cbc,Building a Scalable Data Warehouse with Data Vault 2.0,"The Data Vault was invented by Dan Linstedt at the U.S. Department of Defense, and the standard has been successfully applied to data warehousing projects at organizations of different sizes, from small to large-size corporations. Due to its simplified design, which is adapted from nature, the Data Vault 2.0 standard helps prevent typical data warehousing failures. ""Building a Scalable Data Warehouse"" covers everything one needs to know to create a scalable data warehouse end to end, including a presentation of the Data Vault modeling technique, which provides the foundations to create a technical data warehouse layer. The book discusses how to build the data warehouse incrementally using the agile Data Vault 2.0 methodology. In addition, readers will learn how to create the input layer (the stage layer) and the presentation layer (data mart) of the Data Vault 2.0 architecture including implementation best practices. Drawing upon years of practical experience and using numerous examples and an easy to understand framework, Dan Linstedt and Michael Olschimke discuss: How to load each layer using SQL Server Integration Services (SSIS), including automation of the Data Vault loading processes. Important data warehouse technologies and practices. Data Quality Services (DQS) and Master Data Services (MDS) in the context of the Data Vault architecture. Provides a complete introduction to data warehousing, applications, and the business context so readers can get-up and running fast Explains theoretical concepts and provides hands-on instruction on how to build and implement a data warehouseDemystifies data vault modeling with beginning, intermediate, and advanced techniquesDiscusses the advantages of the data vault approach over other techniques, also including the latest updates to Data Vault 2.0 and multiple improvements to Data Vault 1.0",48-111,,https://www.semanticscholar.org/paper/191e7d86e995560602853aacbc08fda8ae4b6cbc,40,43,2015.0
62de964795b3b71f6010b284212e9c572b21b12e,Data Warehouse Tuning: The Supremacy of Bitmap Index,"data query is the only way to get information from a data warehouse, for that, the designer should consider it effectiveness while the selection of relevant indexes and their combination with materialized views, also the index selection is known as a NP-complete problem, as per the number of indexes is exponential in the total attributes in the database, This makes the choose of the suitable index type is a necessary step while the data warehouse design. In this paper we show that Bitmap index is more advantageous than the B-tree index, based on three factors, size of index, clustering factor and compression, with a real experiment.",7-10,10.5120/13751-1573,https://www.semanticscholar.org/paper/62de964795b3b71f6010b284212e9c572b21b12e,898,11,2013.0
15fdb128139282fcc0b158972c509bb00da56a7a,Datawarehouser: A data warehouse artist who have ability to understand data warehouse schema pictures,"This paper lists basic knowledge requirement to become a data warehouse artist which should have basic data warehouse knowledge in order to understand a data warehouse schema picture as a similarity when a picture or painting artist see an artwork. This paper does not discuss about data warehouse personal such as data warehouse development advisor, data warehouse consultant, data warehouse architect, data warehouse developer or any other jobs related to data warehouse. This paper only discuss how a people can be a data warehouse artist which can enjoy to see many database model design pictures, particularly for data warehouse schema pictures and enjoy to spend much time in front of those pictures. Moreover, A good datawarehouser or data warehouse artist should be able to represent their data warehouse pictures not only in usual and bored pictures but treat their data warehouse pictures as an artwork in order to increase audience's engagement. Furthermore, having knowledge and ability to build and develop data warehouse is value added for data warehouse artist. Thus, a data warehouse artist can recognize and differ each of database model picture as a database design model or data warehouse model and see them as science art.",2205-2208,10.1109/TENCON.2016.7848419,https://www.semanticscholar.org/paper/15fdb128139282fcc0b158972c509bb00da56a7a,23,5,2016.0
12dcaf154a8a72769646f9299d5c466667e1a367,Analisis Data dengan Menggunakan ERD dan Model Konseptual Data Warehouse,"Data is an important part in enterprise information system. So data can be used effectively, we have to analyse the data. There are many ways to analysing and modelling data, some of them are by using Entity Relationship Diagram (ERD) and conceptual model of data warehouse such as star schema, snowflake schema, dan fact constellations schema. This paper suggest a literature study about data analysis by using ERD and conceptual model of data warehouse and also a case study about mini market information system to support that explanation. Design of ERD, star schema, snowflake schema, dan fact constellations schema in that information system meant to manage point of sale, purchasing, and stock control. Design of ERD can be used to modelling transactional data. While data warehouse more used to support manager to make a decision in an enterprise. 
 
Keywords: Entity Relationship Design, Star schema, Snowflakes schema, Fact constellation schema.",71-85,,https://www.semanticscholar.org/paper/12dcaf154a8a72769646f9299d5c466667e1a367,38,0,2015.0
c303542ec9e91ad6e83e0644790435d12833a33e,The HMO Research Network Virtual Data Warehouse: A Public Data Model to Support Collaboration,"The HMO Research Network (HMORN) Virtual Data Warehouse (VDW) is a public, non-proprietary, research-focused data model implemented at 17 health care systems across the United States. The HMORN has created a governance structure and specified policies concerning the VDW’s content, development, implementation, and quality assurance. Data extracted from the VDW have been used by thousands of studies published in peer-reviewed journal articles. Advances in software supporting care delivery and claims processing and the availability of new data sources have greatly expanded the data available for research, but substantially increased the complexity of data management. The VDW data model incorporates software and data advances to ensure that comprehensive, up-to-date data of known quality are available for research. VDW governance works to accommodate new data and system complexities. This article highlights the HMORN VDW data model, its governance principles, data content, and quality assurance procedures. Our goal is to share the VDW data model and its operations to those wishing to implement a distributed interoperable health care data system.",1-165,10.13063/2327-9214.1049,https://www.semanticscholar.org/paper/c303542ec9e91ad6e83e0644790435d12833a33e,315,36,2014.0
01dda7d592b7346de816d50a3a6ec72645b829fc,A Systematic Approach to Creation of a Perioperative Data Warehouse,"Extraction of data from the electronic medical record is becoming increasingly important for quality improvement initiatives such as the American Society of Anesthesiologists Perioperative Surgical Home. To meet this need, the authors have built a robust and scalable data mart based on their implementation of EPIC containing data from across the perioperative period. The data mart is structured in such a way so as to first simplify the overall EPIC reporting structure into a series of Base Tables and then create several Reporting Schemas each around a specific concept (operating room cases, obstetrics, hospital admission, etc.), which contain all of the data required for reporting on various metrics. This structure allows centralized definitions with simplified reporting by a large number of individuals who access only the Reporting Schemas. In creating the database, the authors were able to significantly reduce the number of required table identifiers from >10 to 3, as well as to correct errors in linkages affecting up to 18.4% of cases. In addition, the data mart greatly simplified the code required to extract data, making the data accessible to individuals who lacked a strong coding background. Overall, this infrastructure represents a scalable way to successfully report on perioperative EPIC data while standardizing the definitions and improving access for end users.",1880–1884,10.1213/ANE.0000000000001201,https://www.semanticscholar.org/paper/01dda7d592b7346de816d50a3a6ec72645b829fc,58,18,2016.0
bc2f2a5cc55fb1c4555e31b42aaabf540c4e8451,A Survey: Data Warehouse Architecture,"Data Warehouse and Data mining are technologies that deliver optimallyvaluable information to ease effective decision making. This survey paper defines architecture of traditional data warehouse and ways in which data warehouse techniques are used to support academic decision making. This paper defines different data warehouse types and techniques used in educational environment to extract, transform and load data, and the ways to improve these techniques to have maximum benefit of data warehouse in educational environment. Further this paper have define different data warehouse framework for different situations.",349-356,10.14257/IJHIT.2015.8.5.37,https://www.semanticscholar.org/paper/bc2f2a5cc55fb1c4555e31b42aaabf540c4e8451,17,26,2015.0
0c8af5847f28cc9a267571ab45b826dc83b56cb2,Data Warehouses Federation as a Single Data Warehouse,,356-366,10.1007/978-3-319-45243-2_33,https://www.semanticscholar.org/paper/0c8af5847f28cc9a267571ab45b826dc83b56cb2,2,10,2016.0
d448a33c77562dd0bc001da91336a0160a141272,Building the data warehouse,"From the Publisher: 
The data warehouse solves the problem of getting information out of legacy systems quickly and efficiently. If designed and built right, data warehouses can provide significant freedom of access to data, thereby delivering enormous benefits to any organization. In this unique handbook, W. H. Inmon, ""the father of the data warehouse,"" provides detailed discussion and analysis of all major issues related to the design and construction of the data warehouse, including granularity of data, partitioning data, metadata, lack of creditability of decision support systems (DSS) data, the system of record, migration and more. This Second Edition of Building the Data Warehouse is revised and expanded to include new techniques and applications of data warehouse technology and update existing topics to reflect the latest thinking. It includes a useful review checklist to help evaluate the effectiveness of the design.",78-120,,https://www.semanticscholar.org/paper/d448a33c77562dd0bc001da91336a0160a141272,1964,2,1992.0
60307d0a08b59c3b3142f01a791f73442653c633,Building the data warehouse,"Y our company decides to build a data warehouse and you are designated the project manager. What are your first steps? You’ve read the books, attended the conferences, and perused the trade publications. Now you have to act. There are numerous vendors, all touting the wonders of their products, but you have specific questions that need specific answers, and building a data warehouse is an extremely complex process. Questions you have to weigh fall into the following general categories:",52-60,10.1145/285070.285080,https://www.semanticscholar.org/paper/60307d0a08b59c3b3142f01a791f73442653c633,1854,0,1998.0
92fd538d1e3a69ac3bd435623244ef8978fb4bbe,Data Warehouse Systems,,1-588,10.1007/978-3-642-54655-6,https://www.semanticscholar.org/paper/92fd538d1e3a69ac3bd435623244ef8978fb4bbe,90,193,2014.0
cb298f36b0ef7f83db27c3f1b23e7725c1274ba6,The Deep Data Warehouse: Link-Based Integration and Enrichment of Warehouse Data and Unstructured Content,"Data warehouses are at the core of enterprise IT and enable the efficient storage and analysis of structured data. Besides, unstructured content, e.g., emails and documents, constitutes more than half of the entire enterprise data and contains a lot of implicit knowledge about warehouse entities. Thus, holistic ana-lytics require the integration of structured warehouse data and unstructured content to generate novel insights. These insights can also be used to enrich the integrated data and to create a new basis for further analytics. Existing integration approaches only support a limited range of analytical applications and require the costly adaptation of the warehouse schema. In this paper, we present the Deep Data Warehouse (DeepDWH), a novel type of data warehouse based on the flexible integration and enrichment of warehouse data and unstructured content, addressing the variety challenge of Big Data. It relies on information-rich in-stance-level links between warehouse elements and content items, which are represented in a graph-oriented structure. Neither adaptations of the existing warehouse nor the design of an overall federated schema are required. We design a conceptual linking model and develop a logical schema for links based on a property graph. As a proof of concept, we present a prototypical imple-mentation of the DeepDWH including a link store based on a graph database.",210-217,10.1109/EDOC.2014.36,https://www.semanticscholar.org/paper/cb298f36b0ef7f83db27c3f1b23e7725c1274ba6,22,31,2014.0
3e46777ef526de212d9919b8fe2ea48803a80524,The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling,"Updated new edition of Ralph Kimball's groundbreaking book on dimensional modeling for data warehousing and business intelligence!The first edition of Ralph Kimball'sThe Data Warehouse Toolkitintroduced the industry to dimensional modeling, and now his books are considered the most authoritative guides in this space. This new third edition is a complete library of updated dimensional modeling techniques, the most comprehensive collection ever. It covers new and enhanced star schema dimensional modeling patterns, adds two new chapters on ETL techniques, includes new and expanded business matrices for 12 case studies, and more.Authored by Ralph Kimball and Margy Ross, known worldwide as educators, consultants, and influential thought leaders in data warehousing and business intelligenceBegins with fundamental design recommendations and progresses through increasingly complex scenariosPresents unique modeling techniques for business applications such as inventory management, procurement, invoicing, accounting, customer relationship management, big data analytics, and moreDraws real-world case studies from a variety of industries, including retail sales, financial services, telecommunications, education, health care, insurance, e-commerce, and moreDesign dimensional databases that are easy to understand and provide fast query response withThe Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd Edition.",97-135,,https://www.semanticscholar.org/paper/3e46777ef526de212d9919b8fe2ea48803a80524,309,0,2013.0
2faa1bbb65b1e324ffa8b919877e1349d0c97ca0,A Study on Big Data Integration with Data Warehouse,"The amount of data in world is exploding. Data is being collected and stored at unprecedented rates. The challenge is not only to store and manage the vast volume of data, but also to analyze and extract meaningful value from it. In the last decade Data Warehousing technology has been evolved for efficiently storing the data from different sources for business intelligence purpose. In the Age of the Big Data, it is important to remodel the existing warehouse system that will help you and your organization make the most of unstructured data with your existing Data Warehouse. As Big Data continues to revolutionize how we use data, this paper addresses how to leverage big data by effectively integrating it to your data warehouse.",188-192,10.14445/22312803/IJCTT-V9P137,https://www.semanticscholar.org/paper/2faa1bbb65b1e324ffa8b919877e1349d0c97ca0,20,15,2014.0
12bbe911851d4831f2dc533ad7fe31486586a384,"Data Architecture: A Primer for the Data Scientist: Big Data, Data Warehouse and Data Vault","Today, the world is trying to create and educate data scientists because of the phenomenon of Big Data. And everyone is looking deeply into this technology. But no one is looking at the larger architectural picture of how Big Data needs to fit within the existing systems (data warehousing systems). Taking a look at the larger picture into which Big Data fits gives the data scientist the necessary context for how pieces of the puzzle should fit together. Most references on Big Data look at only one tiny part of a much larger whole. Until data gathered can be put into an existing framework or architecture it cant be used to its full potential. Data Architecture a Primer for the Data Scientist addresses the larger architectural picture of how Big Data fits with the existing information infrastructure, an essential topic for the data scientist. Drawing upon years of practical experience and using numerous examples and an easy to understand framework. W.H. Inmon, and Daniel Linstedt define the importance of data architecture and how it can be used effectively to harness big data within existing systems. Youll be able to: Turn textual information into a form that can be analyzed by standard tools.Make the connection between analytics and Big DataUnderstand how Big Data fits within an existing systems environment Conduct analytics on repetitive and non-repetitive dataDiscusses the value in Big Data that is often overlooked, non-repetitive data, and why there is significant business value in using itShows how to turn textual information into a form that can be analyzed by standard tools.Explains how Big Data fits within an existing systems environment Presents new opportunities that are afforded by the advent of Big Data Demystifies the murky waters of repetitive and non-repetitive data in Big Data",20-195,,https://www.semanticscholar.org/paper/12bbe911851d4831f2dc533ad7fe31486586a384,58,0,2014.0
af84ff9828e212a599acc95c38bcd2149665a16b,Conceptual Data Warehouse Design,,89-119,10.1007/978-3-642-54655-6_4,https://www.semanticscholar.org/paper/af84ff9828e212a599acc95c38bcd2149665a16b,48,0,2014.0
fefe278128fadba4a63baa9b8479d2740fa0e0f3,Data mining-based algorithm for storage location assignment in a randomised warehouse,"Data mining has long been applied in information extraction for a wide range of applications such as customer relationship management in marketing. In the retailing industry, this technique is used to extract the consumers buying behaviour when customers frequently purchase similar products together; in warehousing, it is also beneficial to store these correlated products nearby so as to reduce the order picking operating time and cost. In this paper, we present a data mining-based algorithm for storage location assignment of piece picking items in a randomised picker-to-parts warehouse by extracting and analysing the association relationships between different products in customer orders. The algorithm aims at minimising the total travel distances for both put-away and order picking operations. Extensive computational experiments based on synthetic data that simulates the operations of a computer and networking products spare parts warehouse in Hong Kong have been conducted to test the effectiveness and applicability of the proposed algorithm. Results show that our proposed algorithm is more efficient than the closest open location and purely dedicated storage allocation systems in minimising the total travel distances. The proposed storage allocation algorithm is further evaluated with experiments simulating larger scale warehouse operations. Similar results on the performance comparison among the three storage approaches are observed. It supports the proposed storage allocation algorithm and is applicable to improve the warehousing operation efficiency if items have strong association among each other.",4035 - 4052,10.1080/00207543.2016.1244615,https://www.semanticscholar.org/paper/fefe278128fadba4a63baa9b8479d2740fa0e0f3,63,46,2017.0
a8bcb0fa5bd28b87943d99a8d9960d13d9c67d35,The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling,"Updated new edition of Ralph Kimball's groundbreaking book on dimensional modeling for data warehousing and business intelligence!The first edition of Ralph Kimball'sThe Data Warehouse Toolkitintroduced the industry to dimensional modeling, and now his books are considered the most authoritative guides in this space. This new third edition is a complete library of updated dimensional modeling techniques, the most comprehensive collection ever. It covers new and enhanced star schema dimensional modeling patterns, adds two new chapters on ETL techniques, includes new and expanded business matrices for 12 case studies, and more.Authored by Ralph Kimball and Margy Ross, known worldwide as educators, consultants, and influential thought leaders in data warehousing and business intelligenceBegins with fundamental design recommendations and progresses through increasingly complex scenariosPresents unique modeling techniques for business applications such as inventory management, procurement, invoicing, accounting, customer relationship management, big data analytics, and moreDraws real-world case studies from a variety of industries, including retail sales, financial services, telecommunications, education, health care, insurance, e-commerce, and moreDesign dimensional databases that are easy to understand and provide fast query response withThe Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling, 3rd Edition.",86-172,,https://www.semanticscholar.org/paper/a8bcb0fa5bd28b87943d99a8d9960d13d9c67d35,134,0,2013.0
a446bbe67eb287d9e0801fc92b7b0a0a2f19b1f9,INDIGO – INtegrated Data Warehouse of MIcrobial GenOmes with Examples from the Red Sea Extremophiles,"Background The next generation sequencing technologies substantially increased the throughput of microbial genome sequencing. To functionally annotate newly sequenced microbial genomes, a variety of experimental and computational methods are used. Integration of information from different sources is a powerful approach to enhance such annotation. Functional analysis of microbial genomes, necessary for downstream experiments, crucially depends on this annotation but it is hampered by the current lack of suitable information integration and exploration systems for microbial genomes. Results We developed a data warehouse system (INDIGO) that enables the integration of annotations for exploration and analysis of newly sequenced microbial genomes. INDIGO offers an opportunity to construct complex queries and combine annotations from multiple sources starting from genomic sequence to protein domain, gene ontology and pathway levels. This data warehouse is aimed at being populated with information from genomes of pure cultures and uncultured single cells of Red Sea bacteria and Archaea. Currently, INDIGO contains information from Salinisphaera shabanensis, Haloplasma contractile, and Halorhabdus tiamatea - extremophiles isolated from deep-sea anoxic brine lakes of the Red Sea. We provide examples of utilizing the system to gain new insights into specific aspects on the unique lifestyle and adaptations of these organisms to extreme environments. Conclusions We developed a data warehouse system, INDIGO, which enables comprehensive integration of information from various resources to be used for annotation, exploration and analysis of microbial genomes. It will be regularly updated and extended with new genomes. It is aimed to serve as a resource dedicated to the Red Sea microbes. In addition, through INDIGO, we provide our Automatic Annotation of Microbial Genomes (AAMG) pipeline. The INDIGO web server is freely available at http://www.cbrc.kaust.edu.sa/indigo.",40-189,10.1371/journal.pone.0082210,https://www.semanticscholar.org/paper/a446bbe67eb287d9e0801fc92b7b0a0a2f19b1f9,86,69,2013.0
7ea56351817ae57f93fd6a04c58539fdf9759ec7,Data warehouse testing,"During the development of the data warehouse (DW), too much data is transformed, integrated, structured, cleansed, and grouped in a single structure that is the DW. These various types of changes could lead to data corruption or data manipulation. Therefore, DW testing is a very critical stage in the DW development process.
 A number of attempts were made to describe how the testing process should take place in the DW environment. In this paper, I will state briefly these testing approaches, and then a proposed matrix will be used to evaluate and compare these approaches. Afterwards, I will highlight the weakness points that exist in the available DW testing approaches. Finally, I will describe how I will fill the gap in the DW testing in my PhD by developing a DW Testing Framework presenting briefly its architecture. Then, I will state the scope of work that I am planning to address and what type of limitations that exist in this area that I am expecting to experience. In the end, I will conclude my work and state possible future work in the field of DW testing.",1-8,10.1145/2457317.2457319,https://www.semanticscholar.org/paper/7ea56351817ae57f93fd6a04c58539fdf9759ec7,40,14,2013.0
8a9a4996ede8a04fb562b824ae978751c11c9108,Evaluating Data Reliability: An Evidential Answer with Application to a Web-Enabled Data Warehouse,"There are many available methods to integrate information source reliability in an uncertainty representation, but there are only a few works focusing on the problem of evaluating this reliability. However, data reliability and confidence are essential components of a data warehousing system, as they influence subsequent retrieval and analysis. In this paper, we propose a generic method to assess data reliability from a set of criteria using the theory of belief functions. Customizable criteria and insightful decisions are provided. The chosen illustrative example comes from real-world data issued from the Sym'Previus predictive microbiology oriented data warehouse.",92-105,10.1109/TKDE.2011.179,https://www.semanticscholar.org/paper/8a9a4996ede8a04fb562b824ae978751c11c9108,47,36,2013.0
088fa965485babd4b38f846743605ad16ed24729,Mob-Warehouse: A Semantic Approach for Mobility Analysis with a Trajectory Data Warehouse,,127-136,10.1007/978-3-319-14139-8_15,https://www.semanticscholar.org/paper/088fa965485babd4b38f846743605ad16ed24729,38,13,2013.0
4c7bfa933c11c7a802c2fa9c1dc475dba36a2bd5,Hive - a petabyte scale data warehouse using Hadoop,"The size of data sets being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hadoop [1] is a popular open-source map-reduce implementation which is being used in companies like Yahoo, Facebook etc. to store and process extremely large data sets on commodity hardware. However, the map-reduce programming model is very low level and requires developers to write custom programs which are hard to maintain and reuse. In this paper, we present Hive, an open-source data warehousing solution built on top of Hadoop. Hive supports queries expressed in a SQL-like declarative language - HiveQL, which are compiled into map-reduce jobs that are executed using Hadoop. In addition, HiveQL enables users to plug in custom map-reduce scripts into queries. The language includes a type system with support for tables containing primitive types, collections like arrays and maps, and nested compositions of the same. The underlying IO libraries can be extended to query data in custom formats. Hive also includes a system catalog - Metastore - that contains schemas and statistics, which are useful in data exploration, query optimization and query compilation. In Facebook, the Hive warehouse contains tens of thousands of tables and stores over 700TB of data and is being used extensively for both reporting and ad-hoc analyses by more than 200 users per month.",996-1005,10.1109/ICDE.2010.5447738,https://www.semanticscholar.org/paper/4c7bfa933c11c7a802c2fa9c1dc475dba36a2bd5,1004,7,2010.0
f2ec23bfa99e582f2f96d65d411e06666a5860a8,Tajo: A distributed data warehouse system on large clusters,"The increasing volumes of relational data let us find an alternative to cope with them. Recently, several hybrid approaches (e.g., HadoopDB and Hive) between parallel databases and Hadoop have been introduced to the database community. Although these hybrid approaches have gained wide popularity, they cannot avoid the choice of suboptimal execution strategies. We believe that this problem is caused by the inherent limits of their architectures. In this demo, we present Tajo, a relational, distributed data warehouse system on shared-nothing clusters. It uses Hadoop Distributed File System (HDFS) as the storage layer and has its own query execution engine that we have developed instead of the MapReduce framework. A Tajo cluster consists of one master node and a number of workers across cluster nodes. The master is mainly responsible for query planning and the coordinator for workers. The master divides a query into small tasks and disseminates them to workers. Each worker has a local query engine that executes a directed acyclic graph of physical operators. A DAG of operators can take two or more input sources and be pipelined within the local query engine. In addition, Tajo can control distributed data flow more flexible than that of MapReduce and supports indexing techniques. By combining these features, Tajo can employ more optimized and efficient query processing, including the existing methods that have been studied in the traditional database research areas. To give a deep understanding of the Tajo architecture and behavior during query processing, the demonstration will allow users to submit TPC-H queries to 32 Tajo cluster nodes. The web-based user interface will show (1) how the submitted queries are planned, (2) how the query are distributed across nodes, (3) the cluster and node status, and (4) the detail of relations and their physical information. Also, we provide the performance evaluation of Tajo compared with Hive.",1320-1323,10.1109/ICDE.2013.6544934,https://www.semanticscholar.org/paper/f2ec23bfa99e582f2f96d65d411e06666a5860a8,32,6,2013.0
ef41b8874c936133c687b7e9f6309dbbf45d804a,Use data warehouse and data mining to predict student academic performance in schools: A case study (perspective application and benefits),"The real facts in the education institute is the significant growth of the educational data. Basically the main goal of this paper is to propose a model that can be applied in data warehouse and data mining techniques to predict student performance (academic) in schools. Data mining techniques was used to extract the essential information from the data warehouse and to explore the relationships between variables stored in the data warehouse. In this study we will discuss how data mining and data warehouse model can help the low achiever students, evaluate the course or module suitability, and tailor the interventions to increase student academic performance in schools.",98-103,10.1109/TALE.2013.6654408,https://www.semanticscholar.org/paper/ef41b8874c936133c687b7e9f6309dbbf45d804a,30,16,2013.0
7c1bb3f136310d95e4732da60d6130d7eb4a7fc5,The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling,"From the Publisher: 
The latest edition of the single most authoritative guide on dimensional modeling for data warehousing! 
Dimensional modeling has become the most widely accepted approach for data warehouse design. Here is a complete library of dimensional modeling techniques the most comprehensive collection ever written. Greatly expanded to cover both basic and advanced techniques for optimizing data warehouse design, this second edition to Ralph Kimballs classic guide is more than sixty percent updated. 
The authors begin with fundamental design recommendations and gradually progress step-by-step through increasingly complex scenarios. Clear-cut guidelines for designing dimensional models are illustrated using real-world data warehouse case studies drawn from a variety of business application areas and industries, including: 
 
Retail sales and e-commerce 
Inventory management 
Procurement 
Order management 
Customer relationship management (CRM) 
Human resources management 
Accounting 
Financial services 
Telecommunications and utilities 
Education 
Transportation 
Health care and insurance 
 
 
By the end of the book, you will have mastered the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query response. You will also learn how to create an architected framework that integrates the distributed data warehouse using standardized dimensions and facts. 
Author Biography: Ralph KimbalL, PhD, has been a leading visionary in the data warehouse industry since 1982 and is one of todays most well- known speakers, consultants, and teachers. He writes the ""Data Warehouse Designer"" column for Intelligent Enterprise magazine and is also the author of the bestselling books The Data Warehouse Lifecycle Toolkit and The Data Webhouse Toolkit (both from Wiley). 
Margy Ross is President of DecisionWorks Consulting and a Ralph Kimball Associate. She has focused exclusively on decision support and data warehousing since 1982. Margy coauthored the highly acclaimed The Data Warehouse Lifecycle Toolkit, consults on data warehouse projects worldwide, and teaches data warehouse design for Kimball University.",51-188,,https://www.semanticscholar.org/paper/7c1bb3f136310d95e4732da60d6130d7eb4a7fc5,1296,0,1996.0
cca2b641ab7433209e7eb1e924e4ea31292a7477,Data Warehouse Governance Programs in Healthcare Settings: A Literature Review and a Call to Action,"Purpose: Given the extensive data stored in healthcare data warehouses, data warehouse governance policies are needed to ensure data integrity and privacy. This review examines the current state of the data warehouse governance literature as it applies to healthcare data warehouses, identifies knowledge gaps, provides recommendations, and suggests approaches for further research. Methods: A comprehensive literature search using five data bases, journal article title-search, and citation searches was conducted between 1997 and 2012. Data warehouse governance documents from two healthcare systems in the USA were also reviewed. A modified version of nine components from the Data Governance Institute Framework for data warehouse governance guided the qualitative analysis. Results: Fifteen articles were retrieved. Only three were related to healthcare settings, each of which addressed only one of the nine framework components. Of the remaining 12 articles, 10 addressed between one and seven framework components and the remainder addressed none. Each of the two data warehouse governance plans obtained from healthcare systems in the USA addressed a subset of the framework components, and between them they covered all nine. Conclusions: While published data warehouse governance policies are rare, the 15 articles and two healthcare organizational documents reviewed in this study may provide guidance to creating such policies. Additional research is needed in this area to ensure that data warehouse governance polices are feasible and effective. The gap between the development of data warehouses in healthcare settings and formal governance policies is substantial, as evidenced by the sparse literature in this domain.",77-182,10.13063/2327-9214.1010,https://www.semanticscholar.org/paper/cca2b641ab7433209e7eb1e924e4ea31292a7477,23,28,2013.0
4d77cbb83ff2eb85e52a455e95e7a09c440e5ccd,A business-oriented approach to data warehouse development,"Several surveys have indicated that many data warehouses fail to meet business objectives or are outright failures. One reason for this is that requirement engineering is typically overlooked in real projects. This paper addresses data warehouse design from a business perspective by highlighting business strategy analysis, alignment between data warehouse objectives and a firm's strategy, goal-oriented information requirements' modelling and how an underlying multidimensional data warehouse model may be derived. A set of guidelines is provided allowing developers to design a data warehouse aligned with a prevailing business strategy. A classic case study is presented.",68-155,10.15446/ing.investig.v33n1.37668,https://www.semanticscholar.org/paper/4d77cbb83ff2eb85e52a455e95e7a09c440e5ccd,22,25,2013.0
96fce12309b0b3a9a4d3f7200f16ff704bf38284,Graph Data Warehouse: Steps to Integrating Graph Databases Into the Traditional Conceptual Structure of a Data Warehouse,"As an important kind of NOSQL databases, graph database management systems (GDBMSs) have improved greatly in recent years. However, the development of related linked queries is still limited. This paper introduces an approach for matching frequently-used management tasks of GDBMS with concepts in Structured Query Language (SQL). An application programming interface (API) based on the Neo4j GDBMS was developed to enable functionalities needed with graph databases. Besides the API, a user-friendly graphical user interface (GUI) was developed enabling the management and demonstration of graph datasets. Further, the concept ""graph cube"" is proposed as a design for integrating graphs with tables. The prototype combining these components constitutes the fundamental elements of a graph data warehouse. The proposed research initiates an effort to develop the potential of graph information systems (GRIS) and to define associated technical aspects.",433-434,10.1109/BigData.Congress.2013.72,https://www.semanticscholar.org/paper/96fce12309b0b3a9a4d3f7200f16ff704bf38284,23,9,2013.0
4b190f29fdfd9d9c33491dccc9de7c24af9b74f7,A Data Warehouse Solution for Analyzing RFID-Based Baggage Tracking Data,"Today, airport baggage handling is far from perfect. Baggage goes on the wrong flights, is left behind, or gets lost, which costs a lot of money for the airlines, as well as frustration for the passengers. To remedy the situation, we present a data warehouse (DW) solution for storing and analyzing spatio-temporal Radio Frequency Identification (RFID) baggage tracking data. Analysis of this data can yield interesting results on baggage flow, the causes of baggage mishandling, and the parties responsible for the mishandling(airline, airport, handler,...), which can ultimately lead to improved baggage handling quality. The paper presents a carefully designed data warehouse (DW), with a relational schema sitting underneath a multidimensional data cube, that can handle the many complexities in the data. The paper also discusses the Extract-Transform-Load (ETL) flow that loads the data warehouse with the appropriate tracking data from the data sources. The presented concepts are generalizable to other types of multi-site indoor tracking systems based on Bluetooth and RFID. The system has been tested with large amount of real-world RFID-based baggage tracking data from a major industry initiative. The developed solution is shown to both reveal interesting insights as well as being several orders of magnitude faster than computing the results directly on the data sources.",283-292,10.1109/MDM.2013.42,https://www.semanticscholar.org/paper/4b190f29fdfd9d9c33491dccc9de7c24af9b74f7,20,14,2013.0
8c24601d94997f4254d44026df77e861b4231eac,Cyclotron : Juggling data and queries for a data warehouse audience,"Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the material inaccessible and/or remove it from the website. Please Ask the Library: http://uba.uva.nl/en/contact, or a letter to: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. You will be contacted as soon as possible.",73-134,,https://www.semanticscholar.org/paper/8c24601d94997f4254d44026df77e861b4231eac,44,123,2013.0
6bf743f12612eefe748f0ee66c71057f0ce8b2bb,YeastMine—an integrated data warehouse for Saccharomyces cerevisiae data as a multipurpose tool-kit,"The Saccharomyces Genome Database (SGD; http://www.yeastgenome.org/) provides high-quality curated genomic, genetic, and molecular information on the genes and their products of the budding yeast Saccharomyces cerevisiae. To accommodate the increasingly complex, diverse needs of researchers for searching and comparing data, SGD has implemented InterMine (http://www.InterMine.org), an open source data warehouse system with a sophisticated querying interface, to create YeastMine (http://yeastmine.yeastgenome.org). YeastMine is a multifaceted search and retrieval environment that provides access to diverse data types. Searches can be initiated with a list of genes, a list of Gene Ontology terms, or lists of many other data types. The results from queries can be combined for further analysis and saved or downloaded in customizable file formats. Queries themselves can be customized by modifying predefined templates or by creating a new template to access a combination of specific data types. YeastMine offers multiple scenarios in which it can be used such as a powerful search interface, a discovery tool, a curation aid and also a complex database presentation format. Database URL: http://yeastmine.yeastgenome.org",55-177,10.1093/database/bar062,https://www.semanticscholar.org/paper/6bf743f12612eefe748f0ee66c71057f0ce8b2bb,267,9,2012.0
2b4ec96f0c04ea205d3d9ce25c77986d6766ee4c,InterMine: a flexible data warehouse system for the integration and analysis of heterogeneous biological data,"Summary: InterMine is an open-source data warehouse system that facilitates the building of databases with complex data integration requirements and a need for a fast customizable query facility. Using InterMine, large biological databases can be created from a range of heterogeneous data sources, and the extensible data model allows for easy integration of new data types. The analysis tools include a flexible query builder, genomic region search and a library of ‘widgets’ performing various statistical analyses. The results can be exported in many commonly used formats. InterMine is a fully extensible framework where developers can add new tools and functionality. Additionally, there is a comprehensive set of web services, for which client libraries are provided in five commonly used programming languages. Availability: Freely available from http://www.intermine.org under the LGPL license. Contact: g.micklem@gen.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",3163 - 3165,10.1093/bioinformatics/bts577,https://www.semanticscholar.org/paper/2b4ec96f0c04ea205d3d9ce25c77986d6766ee4c,239,19,2012.0
0099389bdab89755e33780a15d9206e11cfbddd2,Clinical Use of an Enterprise Data Warehouse,"The enormous amount of data being collected by electronic medical records (EMR) has found additional value when integrated and stored in data warehouses. The enterprise data warehouse (EDW) allows all data from an organization with numerous inpatient and outpatient facilities to be integrated and analyzed. We have found the EDW at Intermountain Healthcare to not only be an essential tool for management and strategic decision making, but also for patient specific clinical decision support. This paper presents the structure and two case studies of a framework that has provided us the ability to create a number of decision support applications that are dependent on the integration of previous enterprise-wide data in addition to a patient's current information in the EMR.","
          189-98
        ",,https://www.semanticscholar.org/paper/0099389bdab89755e33780a15d9206e11cfbddd2,80,63,2012.0
d73ff43391d6dc05a907bf6bbb7ecae04e240efc,COMPARATIVE STUDY OF DATA WAREHOUSE DESIGN APPROACHES : A SURVEY,"The process of developing a data warehouse starts with identifying and gathering requirements, designing the dimensional model followed by testing and maintenance. The design phase is the most important activity in the successful building of a data warehouse. In this paper, we surveyed and evaluated the literature related to the various data warehouse design approaches on the basis of design criteria and propose a generalized object oriented conceptual design framework based on UML that meets all types of user needs.",33-45,10.5121/IJDMS.2012.4104,https://www.semanticscholar.org/paper/d73ff43391d6dc05a907bf6bbb7ecae04e240efc,34,40,2012.0
de7cff83dfd2e7ffe6286f52adc13a76dbe16c67,A Data Warehouse Design for a Typical University Information System,"Presently, large enterprises rely on database systems to manage their data and information. These databases are useful for conducting daily business transactions. However, the tight competition in the marketplace has led to the concept of data mining in which data are analyzed to derive effective business strategies and discover better ways in carrying out business. In order to perform data mining, regular databases must be converted into what so called informational databases also known as data warehouse. This paper presents a design model for building data warehouse for a typical university information system. It is based on transforming an operational database into an informational warehouse useful for decision makers to conduct data analysis, predication, and forecasting. The proposed model is based on four stages of data migration: Data extraction, data cleansing, data transforming, and data indexing and loading. The complete system is implemented under MS Access 2010 and is meant to serve as a repository of data for data mining operations.",83-132,,https://www.semanticscholar.org/paper/de7cff83dfd2e7ffe6286f52adc13a76dbe16c67,29,11,2012.0
c18a42a27ec5e2f7b9579267031d326a27fa6cb3,Materialised view construction in data warehouse for decision making,"A data warehouse contains historical and summarised data that grows, almost, exponentially with time. It provides a uniform platform for posing decision support queries. These queries are usually analytical and complex in nature and, when processed against a large data warehouse, consume a lot of processing time resulting in an increased query response time. This time can be reduced by using materialised views, which pre-compute the most frequently accessed information and stores them in a data warehouse. In this paper, an algorithm to construct materialised views using previously posed, optimal, user queries on the data warehouse, is proposed. This algorithm defines a heuristic that maximally merges the optimal queries to construct a single materialised view. These materialised views are capable of providing meaningful information for a given future query. Further, experiments are performed to evaluate the effectiveness of the materialised views with respect to the query response time. The experimental results show that materialised views so constructed are capable of answering future user queries in a reduced response time. This would enable effective and efficient decision-making.",379-396,10.1504/IJBIS.2012.050172,https://www.semanticscholar.org/paper/c18a42a27ec5e2f7b9579267031d326a27fa6cb3,29,52,2012.0
05bbc3079659d6990aa90d24564c4656202755c6,TRUNCATULIX – a data warehouse for the legume community,,19 - 19,10.1186/1471-2229-9-19,https://www.semanticscholar.org/paper/05bbc3079659d6990aa90d24564c4656202755c6,842,54,2009.0
c3ba2a6c5f5c429406f8b7fd7e732e60ccbdb6f8,"Reproduction for academic, not-for profit purposes permitted provided this text is included. A Data Warehouse Architecture for Clinical Data Warehousing",,2-160,,https://www.semanticscholar.org/paper/c3ba2a6c5f5c429406f8b7fd7e732e60ccbdb6f8,90,0,
5e63e47cb3386b032ec43a92ce5980466228c761,A Solution to the Network Challenges of Data Recovery in Erasure-coded Distributed Storage Systems: A Study on the Facebook Warehouse Cluster,"Erasure codes, such as Reed-Solomon (RS) codes, are being increasingly employed in data centers to combat the cost of reliably storing large amounts of data. Although these codes provide optimal storage efficiency, they require significantly high network and disk usage during recovery of missing data. In this paper, we first present a study on the impact of recovery operations of erasure-coded data on the data-center network, based on measurements from Facebook's warehouse cluster in production. To the best of our knowledge, this is the first study of its kind available in the literature. Our study reveals that recovery of RS-coded data results in a significant increase in network traffic, more than a hundred terabytes per day, in a cluster storing multiple petabytes of RS-coded data. 
To address this issue, we present a new storage code using our recently proposed ""Piggybacking"" framework, that reduces the network and disk usage during recovery by 30% in theory, while also being storage optimal and supporting arbitrary design parameters. The implementation of the proposed code in the Hadoop Distributed File System (HDFS) is underway. We use the measurements from the warehouse cluster to show that the proposed code would lead to a reduction of close to fifty terabytes of cross-rack traffic per day.",14-186,,https://www.semanticscholar.org/paper/5e63e47cb3386b032ec43a92ce5980466228c761,282,19,2013.0
527c2be12d95c8a95e9eb9ed8714df59cd4eb4c5,Design and application of Internet of things-based warehouse management system for smart logistics,"Warehouse operations need to change due to the increasing complexity and variety of customer orders. The demand for real-time data and contextual information is requried because of the highly customised orders, which tend to be of small batch size but with high variety. Since the orders frequently change according to customer requirements, the synchronisation of purchase orders to support production to ensure on-time order fulfilment is of high importance. However, the inefficient and inaccurate order picking process has adverse effects on the order fulfilment. The objective of this paper is to propose an Internet of things (IoT)-based warehouse management system with an advanced data analytical approach using computational intelligence techniques to enable smart logistics for Industry 4.0. Based on the data collected from a case company, the proposed IoT-based WMS shows that the warehouse productivity, picking accuracy and efficiency can be improved and it is robust to order variability.",2753 - 2768,10.1080/00207543.2017.1394592,https://www.semanticscholar.org/paper/527c2be12d95c8a95e9eb9ed8714df59cd4eb4c5,272,34,2018.0
984572ffe791b5340bf0b3a7e2e049cbfdb808e2,"TargetMine, an Integrated Data Warehouse for Candidate Gene Prioritisation and Target Discovery","Prioritising candidate genes for further experimental characterisation is a non-trivial challenge in drug discovery and biomedical research in general. An integrated approach that combines results from multiple data types is best suited for optimal target selection. We developed TargetMine, a data warehouse for efficient target prioritisation. TargetMine utilises the InterMine framework, with new data models such as protein-DNA interactions integrated in a novel way. It enables complicated searches that are difficult to perform with existing tools and it also offers integration of custom annotations and in-house experimental data. We proposed an objective protocol for target prioritisation using TargetMine and set up a benchmarking procedure to evaluate its performance. The results show that the protocol can identify known disease-associated genes with high precision and coverage. A demonstration version of TargetMine is available at http://targetmine.nibio.go.jp/.",19-153,10.1371/journal.pone.0017844,https://www.semanticscholar.org/paper/984572ffe791b5340bf0b3a7e2e049cbfdb808e2,117,60,2011.0
d50eb299d0ce9024cab0483556572d3eaf466c8e,MitoMiner: a data warehouse for mitochondrial proteomics data,"MitoMiner (http://mitominer.mrc-mbu.cam.ac.uk/) is a data warehouse for the storage and analysis of mitochondrial proteomics data gathered from publications of mass spectrometry and green fluorescent protein tagging studies. In MitoMiner, these data are integrated with data from UniProt, Gene Ontology, Online Mendelian Inheritance in Man, HomoloGene, Kyoto Encyclopaedia of Genes and Genomes and PubMed. The latest release of MitoMiner stores proteomics data sets from 46 studies covering 11 different species from eumetazoa, viridiplantae, fungi and protista. MitoMiner is implemented by using the open source InterMine data warehouse system, which provides a user interface allowing users to upload data for analysis, personal accounts to store queries and results and enables queries of any data in the data model. MitoMiner also provides lists of proteins for use in analyses, including the new MitoMiner mitochondrial proteome reference sets that specify proteins with substantial experimental evidence for mitochondrial localization. As further mitochondrial proteomics data sets from normal and diseased tissue are published, MitoMiner can be used to characterize the variability of the mitochondrial proteome between tissues and investigate how changes in the proteome may contribute to mitochondrial dysfunction and mitochondrial-associated diseases such as cancer, neurodegenerative diseases, obesity, diabetes, heart failure and the ageing process.",D1160 - D1167,10.1093/nar/gkr1101,https://www.semanticscholar.org/paper/d50eb299d0ce9024cab0483556572d3eaf466c8e,94,31,2011.0
d873ca587904ee6c1823a78995f77cee849e4c9f,Modern Software Engineering Methodologies Meet Data Warehouse Design: 4WD,,66-79,10.1007/978-3-642-23544-3_6,https://www.semanticscholar.org/paper/d873ca587904ee6c1823a78995f77cee849e4c9f,38,26,2011.0
a01a325c6f0e63146e7161d55f9b8e4a43df760e,An Enhanced Technique to Clean Data in the Data Warehouse,"Data quality is a critical factor for the success of data warehousing projects. Improving the quality of data is important in data warehouse, because it is used in the process of decision support, which requires accurate data. There are many errors and inconsistencies that occur in the data sets when brought in from several sources. Data cleaning is the process of identifying and removing or correcting errors in the data. There are some methods to deal with data cleaning, but they are generally inefficient in cleaning the data because they suffer from variety of errors. In this paper we present an enhanced technique to clean data in the data warehouse by using a new algorithm that detects and corrects most of the error types and expected problems, such as lexical errors, domain format errors, irregularities, integrity constraint violation, and duplicates.",306-311,10.1109/DeSE.2011.32,https://www.semanticscholar.org/paper/a01a325c6f0e63146e7161d55f9b8e4a43df760e,26,11,2011.0
65d8b3c080ccc3d3f0e354a4a307d778932cb441,Schema Evolution for Data Warehouse: A Survey,"warehouse is considered as the core component of the modern decision support systems. Due to the major support of data warehouse in the daily transaction of an enterprise, the requirements for the design and the implementation of DW are dynamic and subjective. This dynamic nature of the data warehouse may reflect the evolution in the data warehouse. Data warehouse evolution may be focused on three approaches namely schema evolution, schema versioning and view maintenance. Evolution of the data warehouse may often change their data and structure (schema changes). These schema changes may be consider according to the change in structure, software and users"" requirement. Schema evolution in data warehouse consists of various level namely structural level, conceptual level and behavioural level. This paper mainly focuses on schema evolution and proposes the operators to handle the creation and evolution of aggregated fact table. Our work is to do comparative study for various approaches of schema evolution.",6-14,10.5120/2590-3588,https://www.semanticscholar.org/paper/65d8b3c080ccc3d3f0e354a4a307d778932cb441,17,29,2011.0
e4a5541d7dfb7cb11ca0ff86f840da540a2140b9,Towards Data Quality into the Data Warehouse Development,"Commonly, DW development methodologies, paying little attention to the problem of data quality and completeness. One of the common mistakes made during the planning of a data warehousing project is to assume that data quality will be addressed during testing. In addition to the data warehouse development methodologies, we will introduce in this paper a new approach to data warehouse development. This proposal will be based on integration data quality into the whole data warehouse development phase, denoted by: integrated requirement analysis for designing data warehouse (IRADAH). This paper shows that data quality is not only an integrated part of data warehouse project, but will remain a sustained and ongoing activity.",1199-1206,10.1109/DASC.2011.194,https://www.semanticscholar.org/paper/e4a5541d7dfb7cb11ca0ff86f840da540a2140b9,16,48,2011.0
28a697d97e51bed8afd01f89d28c25e8e5c65360,The Data Mining of the Human Resources Data Warehouse in University Based on Association Rule,"Based on an actual dataset of college human resources, we analyzed the data warehouse technologies and combined them with the pracical work.  The snowflake structure has been described about a data warehouse of university human resource and the data warehouse has been constructed. This paper reduces and categorizes features by explorative data analysis. When the data warehouse had built and applied on human resources management in university, we studied mining technologies and processes based on association rule. Association rules show the relationship among teaching, research and social practices. The results of this study can be well explained and have some management suggestions on human resource management in university.",139-146,10.4304/jcp.6.1.139-146,https://www.semanticscholar.org/paper/28a697d97e51bed8afd01f89d28c25e8e5c65360,20,7,2011.0
9d6df0c771cd88153705f7822e5b70765bd75321,A proposed model for data warehouse ETL processes,,91-104,10.1016/J.JKSUCI.2011.05.005,https://www.semanticscholar.org/paper/9d6df0c771cd88153705f7822e5b70765bd75321,145,39,2011.0
55d5dcd9f042ef5ea40b6d0f7eff75df254b99d6,DW4TR: A Data Warehouse for Translational Research,,"
          1004-19
        ",10.1016/j.jbi.2011.08.003,https://www.semanticscholar.org/paper/55d5dcd9f042ef5ea40b6d0f7eff75df254b99d6,53,79,2011.0
8df07e1dc23ea017ad417784895363aa598752f4,Data Warehouse Testing,,26-43,10.1016/j.infsof.2011.04.002,https://www.semanticscholar.org/paper/8df07e1dc23ea017ad417784895363aa598752f4,36,48,2011.0
0c4b25139e57bdd1ea86e872906deadcf9053764,The Data Warehouse Lifecycle Toolkit,"The world of data warehousing has changed remarkably since the first edition of The Data Warehouse Lifecycle Toolkit was published in 1998. With this new edition, Ralph Kimball and his colleagues have refined the original set of Lifecycle methods and techniques based on their consulting and training experience. They walk you through the detailed steps of designing, developing, and deploying a data warehousing/business intelligence system. With substantial new and updated content, this second edition again sets the standard in data warehousing for the next decade.",7-198,,https://www.semanticscholar.org/paper/0c4b25139e57bdd1ea86e872906deadcf9053764,662,0,2009.0
473a4d94774b4f46219348d9b3c14dc6fd7c2f21,Advanced Data Warehouse Design: From Conventional to Spatial and Temporal Applications,"A data warehouse stores large volumes of historical data required for analytical purposes. This data is extracted from operational databases; transformed into a coherent whole using a multidimensional model that includes measures, dimensions, and hierarchies; and loaded into a data warehouse during the extraction-transformation-loading (ETL) process. Malinowski and Zimnyi explain in detail conventional data warehouse design, covering in particular complex hierarchy modeling. Additionally, they address two innovative domains recently introduced to extend the capabilities of data warehouse systems, namely the management of spatial and temporal information. Their presentation covers different phases of the design process, such as requirements specification, conceptual, logical, and physical design. They include three different approaches for requirements specification depending on whether users, operational data sources, or both are the driving force in the requirements gathering process, and they show how each approach leads to the creation of a conceptual multidimensional model. Throughout the book the concepts are illustrated using many real-world examples and completed by sample implementations for Microsoft's Analysis Services 2005 and Oracle 10g with the OLAP and the Spatial extensions. For researchers this book serves as an introduction to the state of the art on data warehouse design, with many references to more detailed sources. Providing a clear and a concise presentation of the major concepts and results of data warehouse design, it can also be used as the basis of a graduate or advanced undergraduate course. The book may help experienced data warehouse designers to enlarge their analysis possibilities by incorporating spatial and temporal information. Finally, experts in spatial databases or in geographical information systems could benefit from the data warehouse vision for building innovative spatial analytical applications.",10-179,,https://www.semanticscholar.org/paper/473a4d94774b4f46219348d9b3c14dc6fd7c2f21,235,0,2010.0
2a0dbd60f1f3f76f16b73cd14dcfb9a33e298a2c,Digital twin-driven joint optimisation of packing and storage assignment in large-scale automated high-rise warehouse product-service system,"ABSTRACT Current mass individualisation and service-oriented paradigm calls for high flexibility and agility in the warehouse system to adapt changes in products. This paper proposes a novel digital twin-driven joint optimisation approach for warehousing in large-scale automated high-rise warehouse product-service system. A Digital Twin System is developed to aggregate real-time data from physical warehouse product-service system and then to map it to the cyber model. A joint optimisation model on how to timely optimise stacked packing and storage assignment of warehouse product-service system is integrated to the Digital Twin System. Through perceiving online data from the physical warehouse product-service system, periodical optimal decisions can be obtained via the joint optimisation model and then fed back to the semi-physical simulation engine in the Digital Twin System for verifying the implementation result. A demonstrative prototype is developed and verified with a case study of a tobacco warehouse product-service system. The proposed approach can maximise the utilisation and efficiency of the large-scale automated high-rise warehouse product-service system.",783 - 800,10.1080/0951192X.2019.1667032,https://www.semanticscholar.org/paper/2a0dbd60f1f3f76f16b73cd14dcfb9a33e298a2c,104,50,2019.0
f431475390a0a19b2fde1eb6e6b360c438b44968,VHA Corporate Data Warehouse height and weight data: opportunities and challenges for health services research.,"Within the Veterans Health Administration (VHA), anthropometric measurements entered into the electronic medical record are stored in local information systems, the national Corporate Data Warehouse (CDW), and in some regional data warehouses. This article describes efforts to examine the quality of weight and height data within the CDW and to compare CDW data with data from warehouses maintained by several of VHA's regional groupings of healthcare facilities (Veterans Integrated Service Networks [VISNs]). We found significantly fewer recorded heights than weights in both the CDW and VISN data sources. In spite of occasional anomalies, the concordance in the number and value of records in the CDW and the VISN warehouses was generally 97% to 99% or greater. Implausible variation in same-day and same-year heights and weights was noted, suggesting measurement or data-entry errors. Our work suggests that the CDW, over time and through validation, has become a generally reliable source of anthropometric data. Researchers should assess the reliability of data contained within any source and apply strategies to minimize the impact of data errors appropriate to their study population.","
          739-50
        ",10.1682/JRRD.2009.08.0110,https://www.semanticscholar.org/paper/f431475390a0a19b2fde1eb6e6b360c438b44968,64,28,2010.0
ff1934a103c32b8214e346047459f50039f78017,Analysis of Data Quality Aspects in Data Warehouse Systems,"Abstract: Data quality is a critical factor for the success of data warehousing projects. If data is of inadequate quality, then the knowledge workers who query the data warehouse and the decision makers who receive the information cannot trust the results. In order to obtain clean and reliable data, it is imperative to focus on data quality. While many data warehouse projects do take data quality into consideration, it is often given a delayed afterthought. Even QA after ETL is not good enough the Quality process needs to be incorporated in the ETL process itself. Data quality has to be maintained for individual records or even small bits of information to ensure accuracy of complete database. Data quality is an increasingly serious issue for organizations large and small. It is central to all data integration initiatives. Before data can be used effectively in a data warehouse, or in customer relationship management, enterprise resource planning or business analytics applications, it needs to be analyzed and cleansed. To ensure high quality data is sustained, organizations need to apply ongoing data cleansing processes and procedures, and to monitor and track data quality levels over time. Otherwise poor data quality will lead to increased costs, breakdowns in the supply chain and inferior customer relationship management. Defective data also hampers business decision making and efforts to meet regulatory compliance responsibilities. The key to successfully addressing data quality is to get business professionals centrally involved in the process. We have analyzed possible set of causes of data quality issues from exhaustive survey and discussions with data warehouse groups working in distinguishes organizations in India and abroad. We expect this paper will help modelers, designers of warehouse to analyze and implement quality warehouse and business intelligence applications.",94-144,,https://www.semanticscholar.org/paper/ff1934a103c32b8214e346047459f50039f78017,36,23,2010.0
76b85069c84056ef5b412e0c540250a4ecaa8ef3,"Improve Performance of Extract, Transform and Load (ETL) in Data Warehouse","Extract, transform and load (ETL) is the core process of data integration and is typically associated with data warehousing. ETL tools extract data from a chosen source, transform it into new formats according to business rules, and then load it into target data structure. Managing rules and processes for the increasing diversity of data sources and high volumes of data processed that ETL must accommodate, make management, performance and cost the primary and challenges for users. ETL is a key process to bring all the data together in a standard, homogenous environment. ETL functions reshape the relevant data from the source systems into useful information to be stored in the data warehouse. Without these functions, there would be no strategic information in the data warehouse. If source data taken from various sources is not cleanse, extracted properly, transformed and integrated in the proper way, query process which is the backbone of the data warehouse could not happened In this paper we purpose an ultimate advance approach which will increase the speed of Extract, transform and load in data ware house with the support of query cache. Because the query process is the backbone of the data warehouse It will reduce response time and improve the performance of data ware house.",20-160,,https://www.semanticscholar.org/paper/76b85069c84056ef5b412e0c540250a4ecaa8ef3,36,9,2010.0
f2f51a5f1e2edface850ff423536091f5d344ab0,Key organizational factors in data warehouse architecture selection,,200-212,10.1016/j.dss.2010.02.006,https://www.semanticscholar.org/paper/f2f51a5f1e2edface850ff423536091f5d344ab0,73,97,2010.0
b5f95887c3f0998906841b84e03fe1e99422bbab,Data Warehouse Design: Modern Principles and Methodologies,Chapter 1. Introduction to Data Warehousing Chapter 2. Data Warehouse System Lifecycle Chapter 3. Analysis and Reconciliation of Data Sources Chapter 4. User Requirement Analysis Chapter 5. Conceptual Modeling Chapter 6. Conceptual Design Chapter 7. Workload and Data Volume Chapter 8. Logical Modeling Chapter 9. Logical Design Chapter 10. Data-staging Design Chapter 11. Indexes for the Data Warehouse Chapter 12. Physical Design Chapter 13. Data Warehouse Project Documentation Chapter 14. A Case Study Chapter 15. Business Intelligence: Beyond the Data Warehouse Glossary Bibliography Index,58-125,,https://www.semanticscholar.org/paper/b5f95887c3f0998906841b84e03fe1e99422bbab,305,0,2009.0
f99f099427d81978706373c3b5022726b03d2607,Flexible automated warehouse: a literature review and an innovative framework,,533 - 558,10.1007/s00170-019-04588-z,https://www.semanticscholar.org/paper/f99f099427d81978706373c3b5022726b03d2607,66,143,2019.0
c5ee3dde6eca0eafa53919b423dd94abae9e6573,Data ethics,"CUSP aims to utilize Big Data to help study and understand urban environments. As a part of this effort, we are planning to build an inclusive data warehouse at CUSP. Our vision for this data warehouse is to hold large quantities of data from multiple sources, including personal (most likely anonymized) data about individuals. But, obtaining, housing, and protecting these data come with many challenges and questions. We hope to answer some of these questions in this working session and converge on a set of principles that will guide our data practices moving forward. Personal data is a new asset class touching all aspects of society. It is potentially as valuable a resource in the 21st century as heavily traded physical goods like oil have been in the past hundred years. However, throughout history, economic value creation has been linked to the ability to move and trade physical goods. Similarly, "" data needs to move to create value. Data sitting alone on a server is like money hidden under a mattress. It is safe and secure, but largely stagnant and underutilized. "" But, personal data lacks the trading rules and policy frameworks that exist for widely traded physical assets. As a result, there is little trust among the key stakeholders,-individuals, governments and the private sector,-which could undermine its long-term potential. In response to surveys, individuals generally say that they want enhanced control over their personal data, increased transparency on how it is used, and some kind of fair value in return. However, their actions are often quite different. While many say they care deeply about privacy, they share information quite widely online. They often sign up for services not knowing how their data will be protected or whether it will be shared. They rarely read the privacy policies of the organizations providing these services, which are usually written in hard-to-comprehend legal language. Companies, on the other hand, view the data they have captured or created about individuals as theirs. Data is an asset on which they have invested significant resources. They want to leverage the data to create business value, better understand the behavior of their customers and help themselves become more productive. They struggle with how to best protect all the data they now have access to, as well as trying to figure out the different regulations pertaining to its use. Governments are trying to leverage all this data …",67-149,10.4324/9781003201182-27,https://www.semanticscholar.org/paper/c5ee3dde6eca0eafa53919b423dd94abae9e6573,212,23,2021.0
b4c06d1040185de46805041f8c5977bee7ac6e5c,Conceptual Modeling Solutions for the Data Warehouse,"In the context of data warehouse design, a basic role is played by conceptual modeling, that provides a higher level of abstraction in describing the warehousing process and architecture in all its aspects, aimed at achieving independence of implementation issues. This chapter focuses on a conceptual model called the DFM that suits the variety of modeling situations that may be encountered in real projects of small to large complexity. The aim of the chapter is to propose a comprehensive set of solutions for conceptual modeling according to the DFM and to give the designer a practical guide for applying them in the context of a design methodology. Besides the basic concepts of multidimensional modeling, the other issues discussed are descriptive and cross-dimension attributes; convergences; shared, incomplete, recursive, and dynamic hierarchies; multiple and optional arcs; and additivity.",86-104,10.4018/978-1-59904-951-9.CH016,https://www.semanticscholar.org/paper/b4c06d1040185de46805041f8c5977bee7ac6e5c,160,81,2009.0
b987db72959e565266dab809dd6965a29f72846f,Warehouse management system customization and information availability in 3pl companies,"PurposeThe purpose of this paper is to illustrate an original decision-support tool (DST) that aids 3PL managers to decide on the proper warehouse management system (WMS) customization. The aim of this tool is to address to the three main issues affecting such decision: the cost of the information sharing, the scarce visibility of the client’s data and the uncertainty of quantifying the return from investing into a WMS feature.Design/methodology/approachThe tool behaves as a digital twin of a WMS. In addition, it incorporates a set of WMS’s features based both on heuristics and optimization techniques and uses simulation to perform what-if multi-scenario analyses of alternative management scenarios. In order to validate the effectiveness of the tool, its application to a real-world 3PL warehouse operating in the sector of biomedical products is illustrated.FindingsThe results of a simulation campaign along an observation horizon of ten months demonstrate how the tool supports the comparison of alternative scenarios with theas-is, thereby suggesting the most suitable WMS customization to adopt.Practical implicationsThe tool supports 3PL managers in enhancing the efficiency of the operations and the fulfilling of the required service level, which is increasingly challenging given the large inventory mix and the variable clients portfolio that 3PLs have to manage. Particularly, the choice of the WMS customization that better perform with each business can be problematic, given the scarce information visibility of the provider on the client’s processes.Originality/valueTo the author’s knowledge, this paper is among the first to address a still uncovered gap of the warehousing literature by illustrating a DST that exploits optimization and simulation techniques to quantify the impacts of the information availability on the warehousing operations performance. As a second novel contribution, this tool enables to create a digital twin of a WMS and foresee the evolution of the warehouse’s performance over time.",251-273,10.1108/IMDS-01-2018-0033,https://www.semanticscholar.org/paper/b987db72959e565266dab809dd6965a29f72846f,52,65,2019.0
e51fe5e9f39538a4b936a99ffa11a12f9c27c967,A comprehensive approach to data warehouse testing,"Testing is an essential part of the design life-cycle of any software product. Nevertheless, while most phases of data warehouse design have received considerable attention in the literature, not much has been said about data warehouse testing. In this paper we introduce a number of data mart-specific testing activities, we classify them in terms of what is tested and how it is tested, and we discuss how they can be framed within a reference design methodology.",17-24,10.1145/1651291.1651295,https://www.semanticscholar.org/paper/e51fe5e9f39538a4b936a99ffa11a12f9c27c967,47,21,2009.0
df0926379fa3d34d99a12be72148dbc9cabcd529,Optimizing data warehouse loading procedures for enabling useful-time data warehousing,"The purpose of a data warehouse is to aid decision making. As the real-time enterprise evolves, synchronism between transactional data and data warehouses is redefined. To cope with real-time requirements, the data warehouses must be able to enable continuous data integration, in order to deal with the most recent business data. Traditional data warehouses are unable to support any dynamics in structure and content while they are available for OLAP. Their data is periodically updated because they are unprepared for continuous data integration. For real-time enterprises with needs in decision support while the transactions are occurring, (near) real-time data warehousing seem very promising. In this paper we present a survey on testing today's most used loading techniques and analyze which are the best data loading methods, presenting a methodology for efficiently supporting continuous data integration for data warehouses. To accomplish this, we use techniques such as table structure replication with minimum content and query predicate restrictions for selecting data, to enable loading data in the data warehouse continuously, with minimum impact in query execution time. We demonstrate the efficiency of the method using benchmark TPC-H and executing query workloads while simultaneously performing continuous data integration.",292-299,10.1145/1620432.1620464,https://www.semanticscholar.org/paper/df0926379fa3d34d99a12be72148dbc9cabcd529,34,46,2009.0
8e974bfd6b38bdda9946c6be59bdc8415a895873,"Kimball's Data Warehouse Toolkit Classics: The Data Warehouse Toolkit, 2nd Edition; The Data Warehouse Lifecycle, 2nd Edition; The Data Warehouse ETL Toolk","Three books that set the standard with their groundbreaking methods and techniques for data warehousing. Great value! Save $20, value of the set is $165. This set includes the following books from the Kimball Group: The Data Warehouse Toolkit, Second Edition This authoritative guide presents clear-cut guidelines for designing dimensional models through the use of real-world data warehouse case studies drawn from a variety of business application areas and industries. Ralph Kimball and Margy Ross help you master the full range of powerful techniques for designing dimensional databases that are easy to understand and provide fast query responses. The Data Warehouse Lifecycle Toolkit, Second Edition With this second edition, Ralph Kimball and his Kimball Group colleagues refined the original set of Lifecycle methods and techniques based on their consulting and training experiences. They walk you through detailed steps of designing, developing, and deploying a data warehousing/business intelligence system, from business requirements gathering through delivery, with the steadfast goal of enabling users to make better, more informed business decisions. The Data Warehouse ETL Toolkit Serving as a road map for planning, designing, building, and running the backroom of a data warehouse, this book provides complete coverage of proven, time-saving ETL techniques. You'll discover how a properly designed ETL system extracts data from the source systems, enforces data quality and consistency standards, conforms the data so that separate sources can be used together, and finally delivers the data in a presentation-ready format",58-173,,https://www.semanticscholar.org/paper/8e974bfd6b38bdda9946c6be59bdc8415a895873,41,0,2009.0
059f5771d19fe395f3b6d69dc4bafa22152ff1d7,Alliance Rules for Data Warehouse Cleansing,"Data Cleansing is an activity performed on the data sets of data warehouse to enhance and maintain the quality and consistency of the data. This paper addresses the problems related with dirty data, entrance of dirty data and detection of dirty data in the data warehouse. The paper perceives the procedure of data cleansing from a different perspective. It provides an algorithm for the detection of errors and dirty data in the data sets of an already existing data warehouse. The paper characterizes the alliance rules based on the concept of mathematical association rules to determine the dirty and faulty data in data warehouse. The research marks the use of q-grams [1] to determine the errors in a prominent way.",743-747,10.1109/ICSPS.2009.133,https://www.semanticscholar.org/paper/059f5771d19fe395f3b6d69dc4bafa22152ff1d7,24,14,2009.0
6add25af24f567d9508f4b1f4fd7620d0b752e69,A Fuzzy Data Warehouse Approach for Web Analytics,,276-285,10.1007/978-3-642-04754-1_29,https://www.semanticscholar.org/paper/6add25af24f567d9508f4b1f4fd7620d0b752e69,34,21,2009.0
cd461d873b39f434a5171622166eb719f1d6e5d1,Data Warehouse Refreshment,,675,10.1007/978-0-387-39940-9_2412,https://www.semanticscholar.org/paper/cd461d873b39f434a5171622166eb719f1d6e5d1,21,80,2009.0
c0985494b16273b1c18847675340001fbcd9f7e7,Data Warehouse Performance,"A data warehouse is a large electronic repository of information that is generated and updated in a structured manner by an enterprise over time to aid business intelligence and to support decision making. Data stored in a data warehouse is non-volatile and time variant and is organized by subjects in a manner to support decision making (Inmon et al., 2001). Data warehousing has been increasingly adopted by enterprises as the backbone technology for business intelligence reporting and query performance has become the key to the successful implementation of data warehouses. According to a survey of 358 businesses on reporting and end-user query tools, conducted by Appfluent Technology, data warehouse performance significantly affects the Return on Investment (ROI) on Business Intelligence (BI) systems and directly impacts the bottom line of the systems (Appfluent Technology, 2002). Even though in some circumstances it is very difficult to measure the benefits of BI projects in terms of ROI or dollar figures, management teams are still eager to have a “single version of the truth,” better information for strategic and tactical decision making, and more efficient business processes by using BI solutions (Eckerson, 2003). Dramatic increases in data volumes over time and the mixed quality of data can adversely affect the performance of a data warehouse. Some data may become outdated over time and can be mixed with data that are still valid for decision making. In addition, data are often collected to meet potential requirements, but may never be used. Data warehouses also contain external data (e.g. demographic, psychographic, etc.) to support a variety of predictive data mining activities. All these factors contribute to the massive growth of data volume. As a result, even a simple query may become burdensome to process and cause overflowing system indices (Inmon et al., 1998). Thus, exploring the techniques of performance tuning becomes an important subject in data warehouse management.",580-585,10.4018/978-1-59140-557-3.CH061,https://www.semanticscholar.org/paper/c0985494b16273b1c18847675340001fbcd9f7e7,23,16,2009.0
74316800076e912c776d4a4aa870cfe8c5fa2063,Data modelling for effective data warehouse architecture and design,"A data warehouse is attractive as the main repository of an organisation's historical data and is optimised for reporting and analysis. In this paper, we present a data warehouse and the process of data warehouse architecture development and design. We highlight the different aspects to be considered in building a data warehouse. These range from data store characteristics to data modelling and the principles to be considered for effective data warehouse architecture.",282-300,10.1504/IJIDS.2009.027656,https://www.semanticscholar.org/paper/74316800076e912c776d4a4aa870cfe8c5fa2063,18,15,2009.0
b58e98029d53d69ccc7089dca7b01bf050b5ad2b,IMG/M v.5.0: an integrated data management and comparative analysis system for microbial genomes and microbiomes,"Abstract The Integrated Microbial Genomes & Microbiomes system v.5.0 (IMG/M: https://img.jgi.doe.gov/m/) contains annotated datasets categorized into: archaea, bacteria, eukarya, plasmids, viruses, genome fragments, metagenomes, cell enrichments, single particle sorts, and metatranscriptomes. Source datasets include those generated by the DOE’s Joint Genome Institute (JGI), submitted by external scientists, or collected from public sequence data archives such as NCBI. All submissions are typically processed through the IMG annotation pipeline and then loaded into the IMG data warehouse. IMG’s web user interface provides a variety of analytical and visualization tools for comparative analysis of isolate genomes and metagenomes in IMG. IMG/M allows open access to all public genomes in the IMG data warehouse, while its expert review (ER) system (IMG/MER: https://img.jgi.doe.gov/mer/) allows registered users to access their private genomes and to store their private datasets in workspace for sharing and for further analysis. IMG/M data content has grown by 60% since the last report published in the 2017 NAR Database Issue. IMG/M v.5.0 has a new and more powerful genome search feature, new statistical tools, and supports metagenome binning.",D666 - D677,10.1093/nar/gky901,https://www.semanticscholar.org/paper/b58e98029d53d69ccc7089dca7b01bf050b5ad2b,649,46,2018.0
2d34c2d4ab26d5b9f901dd9c57f9acf0dad079c6,RCFile: A fast and space-efficient data placement structure in MapReduce-based warehouse systems,"MapReduce-based data warehouse systems are playing important roles of supporting big data analytics to understand quickly the dynamics of user behavior trends and their needs in typical Web service providers and social network sites (e.g., Facebook). In such a system, the data placement structure is a critical factor that can affect the warehouse performance in a fundamental way. Based on our observations and analysis of Facebook production systems, we have characterized four requirements for the data placement structure: (1) fast data loading, (2) fast query processing, (3) highly efficient storage space utilization, and (4) strong adaptivity to highly dynamic workload patterns. We have examined three commonly accepted data placement structures in conventional databases, namely row-stores, column-stores, and hybrid-stores in the context of large data analysis using MapReduce. We show that they are not very suitable for big data processing in distributed systems. In this paper, we present a big data placement structure called RCFile (Record Columnar File) and its implementation in the Hadoop system. With intensive experiments, we show the effectiveness of RCFile in satisfying the four requirements. RCFile has been chosen in Facebook data warehouse system as the default option. It has also been adopted by Hive and Pig, the two most widely used data analysis systems developed in Facebook and Yahoo!",1199-1208,10.1109/ICDE.2011.5767933,https://www.semanticscholar.org/paper/2d34c2d4ab26d5b9f901dd9c57f9acf0dad079c6,288,24,2011.0
5b6ba7a04d1245d7233af9874206f8b9e8fe1c44,Enhancing XML data warehouse query performance by fragmentation,"XML data warehouses form an interesting basis for decision-support applications that exploit heterogeneous data from multiple sources. However, XML-native database systems currently suffer from limited performances in terms of manageable data volume and response time for complex analytical queries. Fragmenting and distributing XML data warehouses (e.g., on data grids) allow to address both these issues. In this paper, we work on XML warehouse fragmentation. In relational data warehouses, several studies recommend the use of derived horizontal fragmentation. Hence, we propose to adapt it to the XML context. We particularly focus on the initial horizontal fragmentation of dimensions' XML documents and exploit two alternative algorithms. We experimentally validate our proposal and compare these alternatives with respect to a unified XML warehouse model we advocate for.",54-185,10.1145/1529282.1529630,https://www.semanticscholar.org/paper/5b6ba7a04d1245d7233af9874206f8b9e8fe1c44,32,27,2009.0
553aecdf18ab01ab456d59fd5252ac93294033ed,The Data Warehouse Toolkit: Practical Techniques for Building Dimensional Data Warehouses,"This definitive guide succinctly explains how to build a data warehouse by using actual case studies of existing data warehouses developed for specific types of business applications such as retail, manufacturing, banking, insurance, subcriptions and airline reservations. Describes a powerful new model of data warehouse design, the dimensional data warehouse, that provides readers with the ability to quickly analyze complex information in order to make sound decisions. The accompanying CD-ROM includes a toolkit for building dimensional data warehouses and examples of all the databases discussed in the text.",48-140,,https://www.semanticscholar.org/paper/553aecdf18ab01ab456d59fd5252ac93294033ed,760,0,1996.0
7378ece4303e7741d98f5cdde4aef931b771da60,Data warehouse technology by infobright,We discuss Infobright technology with respect to its main features and architectural differentiators. We introduce the upcoming research and development projects that may be of special interest to the academic and industry communities.,3-184,10.1145/1559845.1559933,https://www.semanticscholar.org/paper/7378ece4303e7741d98f5cdde4aef931b771da60,47,34,2009.0
7841e5e365a69475a108317dabfcead9719cde26,Data Driven vs. Metric Driven Data Warehouse Design,"Although data warehousing theory and technology have been around for well over a decade, they may well be the next hot technologies. How can it be that a technology sleeps for so long and then begins to move rapidly to the foreground? This question can have several answers. Perhaps the technology had not yet caught up to the theory or that computer technology 10 years ago did not have the capacity to delivery what the theory promised. Perhaps the ideas and the products were just ahead of their time. All these answers are true to some extent. But the real answer, I believe, is that data warehousing is in the process of undergoing a radical theoretical and paradigmatic shift, and that shift will reposition data warehousing to meet future demands.",382-387,10.4018/978-1-60566-010-3.CH060,https://www.semanticscholar.org/paper/7841e5e365a69475a108317dabfcead9719cde26,13,10,2009.0
e73917930fa390944c2bc5fe5695299b2dc9db72,Empirical studies to assess the understandability of data warehouse schemas using structural metrics,,79-106,10.1007/s11219-007-9030-7,https://www.semanticscholar.org/paper/e73917930fa390944c2bc5fe5695299b2dc9db72,60,30,2008.0
81c5d725581ead15ac80698b0e472ec93f7edbfc,From Federated Databases to a Federated Data Warehouse System,"Although Data Warehousing is regarded as a mature technology now, the definition of a federated architecture for Data Warehouse (DW) integration remains an open research question. This paper identifies requirements on a Federated DW System and proposes an architecture supporting the tightly coupled integration of heterogeneous data marts into a global, logical schema. In order to enable the processing of queries in the federation, our approach provides a Dimension Algebra (DA) and Fact Algebra (FA) to define the mappings between the global and local schemas. Moreover, we demonstrate how to apply DA and FA expressions for dimension and fact integration and explain the benefits of such an approach.",394-394,10.1109/HICSS.2008.178,https://www.semanticscholar.org/paper/81c5d725581ead15ac80698b0e472ec93f7edbfc,42,29,2008.0
4c01ae3022cfeeb4ab0f679d708ecd67fe156c74,Building a Data Warehouse: With Examples in SQL Server,"Building a Data Warehouse: With Examples in SQL Server describes how to build a data warehouse completely from scratch and shows practical examples on how to do it. Author Vincent Rainardi also describes some practical issues he has experienced that developers are likely to encounter in their first data warehousing project, along with solutions and advice. The RDBMS used in the examples is SQL Server; the version will not be an issue as long as the user has SQL Server 2005 or later. 
 
The book is organized as follows. In the beginning of this book (Chapters 1 through 6), you learn how to build a data warehouse, for example, defining the architecture, understanding the methodology, gathering the requirements, designing the data models, and creating the databases. Then in Chapters 7 through 10, you learn how to populate the data warehouse, for example, extracting from source systems, loading the data stores, maintaining data quality, and utilizing the metadata. After you populate the data warehouse, in Chapters 11 through 15, you explore how to present data to users using reports and multidimensional databases and how to use the data in the data warehouse for business intelligence, customer relationship management, and other purposes. Chapters 16 and 17 wrap up the book: After you have built your data warehouse, before it can be released to production, you need to test it thoroughly. After your application is in production, you need to understand how to administer data warehouse operation. 
 
What youll learn 
A detailed understanding of what it takes to build a data warehouse 
The implementation code in SQL Server to build the data warehouse 
Dimensional modeling, data extraction methods, data warehouse loading, populating dimension and fact tables, data quality, data warehouse architecture, and database design 
Practical data warehousing applications such as business intelligence reports, analytics applications, and customer relationship management 
 
Who is this book for? 
 
There are three audiences for the book. The first are the people who implement the data warehouse. This could be considered a field guide for them. The second is database users/admins who want to get a good understanding of what it would take to build a data warehouse. Finally, the third audience is managers who must make decisions about aspects of the data warehousing task before them and use the book to learn about these issues. 
 
Related Titles 
Beginning Relational Data Modeling, Second Edition 
Data Mining and Statistical Analysis Using SQL",67-118,,https://www.semanticscholar.org/paper/4c01ae3022cfeeb4ab0f679d708ecd67fe156c74,122,2,2008.0
f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",75-121,,https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,6833,6,2017.0
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",74-161,,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,10353,60,2016.0
f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",802-810,,https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,6405,26,2015.0
bc00ff34ec7772080c7039b17f7069a2f7df0889,Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,,206 - 215,10.1038/s42256-019-0048-x,https://www.semanticscholar.org/paper/bc00ff34ec7772080c7039b17f7069a2f7df0889,4116,87,2018.0
d422df8bff4e677a3077635db116679d25142bfc,"Machine learning: Trends, perspectives, and prospects","Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",255 - 260,10.1126/science.aaa8415,https://www.semanticscholar.org/paper/d422df8bff4e677a3077635db116679d25142bfc,4906,71,2015.0
9f5b82d9915d0752957602224c5056be7e749c83,Foundations of Machine Learning,VisionPlaying,83-179,10.2139/ssrn.3399990,https://www.semanticscholar.org/paper/9f5b82d9915d0752957602224c5056be7e749c83,2699,5,2021.0
794b3ffd28d28606230efc975eeec9f0522fb139,An Introduction to Machine Learning,,1-348,10.1007/978-3-319-63913-0,https://www.semanticscholar.org/paper/794b3ffd28d28606230efc975eeec9f0522fb139,3791,87,2017.0
0090023afc66cd2741568599057f4e82b566137c,A Survey on Bias and Fairness in Machine Learning,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",1 - 35,10.1145/3457607,https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c,2675,188,2019.0
730ca170962a58607e092035beb2afc4b5fa6242,Data Mining Practical Machine Learning Tools and Techniques,,92-96,,https://www.semanticscholar.org/paper/730ca170962a58607e092035beb2afc4b5fa6242,16942,0,2014.0
360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5,Machine learning - a probabilistic perspective,"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. Machine learning : a probabilistic perspective / Kevin P. Murphy. p. cm. — (Adaptive computation and machine learning series) Includes bibliographical references and index. Contents Preface xxvii 1 Introduction 1 1.1 Machine learning: what and why? 1 1.1.1 Types of machine learning 2 1.2 Supervised learning 3 1.2.1 Classification 3 1.2.2 Regression 8 1.3 Unsupervised learning 9 1.3.1 Discovering clusters 10 1.3.2 Discovering latent factors 11 1.3.3 Discovering graph structure 13 1.3.4 Matrix completion 14 1.4 Some basic concepts in machine learning 16 1.4.1 Parametric vs non-parametric models 16 1.4.2 A simple non-parametric classifier: K-nearest neighbors 16 1.4.3 The curse of dimensionality 18 1.4.4 Parametric models for classification and regression 19 1.4.5","I-XXIX, 1-1067",,https://www.semanticscholar.org/paper/360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5,8755,0,2012.0
168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74,Scikit-learn: Machine Learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",57-187,10.5555/1953048.2078195,https://www.semanticscholar.org/paper/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74,63316,18,2011.0
554894f70b28dba58b396c2d84080ac01051261b,Gaussian Processes For Machine Learning,"Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other ""kernel machines"" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.","
          69-106
        ",10.1142/S0129065704001899,https://www.semanticscholar.org/paper/554894f70b28dba58b396c2d84080ac01051261b,20793,87,2004.0
807c1f19047f96083e13614f7ce20f2ac98c239a,C4.5: Programs for Machine Learning,"From the Publisher: 
Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. 
 
C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. 
 
This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.",96-148,,https://www.semanticscholar.org/paper/807c1f19047f96083e13614f7ce20f2ac98c239a,23730,0,1992.0
597bd2e45427563cdf025e53a3239006aa364cfc,Open Graph Benchmark: Datasets for Machine Learning on Graphs,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .",74-136,,https://www.semanticscholar.org/paper/597bd2e45427563cdf025e53a3239006aa364cfc,1787,110,2020.0
2e2089ae76fe914706e6fa90081a79c8fe01611e,Practical Bayesian Optimization of Machine Learning Algorithms,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ""black art"" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",2960-2968,,https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e,6691,28,2012.0
b0c34618ffd1154f35863e2ce7250ac6b6f2c424,Interpretable Machine Learning,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.",77-131,10.1201/9780367816377-16,https://www.semanticscholar.org/paper/b0c34618ffd1154f35863e2ce7250ac6b6f2c424,2088,219,2019.0
5c39e37022661f81f79e481240ed9b175dec6513,Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",34-173,,https://www.semanticscholar.org/paper/5c39e37022661f81f79e481240ed9b175dec6513,2888,57,2017.0
668b1277fbece28c4841eeab1c97e4ebd0079700,Pattern Recognition and Machine Learning,,366 - 366,10.1007/978-0-387-45528-0,https://www.semanticscholar.org/paper/668b1277fbece28c4841eeab1c97e4ebd0079700,35325,360,2006.0
2e62d1345b340d5fda3b092c460264b9543bc4b5,Genetic Algorithms in Search Optimization and Machine Learning,"From the Publisher: 
This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. 
 
Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.",96-121,10.5860/choice.27-0936,https://www.semanticscholar.org/paper/2e62d1345b340d5fda3b092c460264b9543bc4b5,59950,1,1988.0
f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d,Membership Inference Attacks Against Machine Learning Models,"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",3-18,10.1109/SP.2017.41,https://www.semanticscholar.org/paper/f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d,3012,41,2016.0
4954fa180728932959997a4768411ff9136aac81,This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "" parameter server "" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production , we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.",38-146,,https://www.semanticscholar.org/paper/4954fa180728932959997a4768411ff9136aac81,16805,77,
53b047e503f4c24602f376a774d653f7ed56c024,Practical Black-Box Attacks against Machine Learning,"Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",38-188,10.1145/3052973.3053009,https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024,3154,42,2016.0
7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea,"Machine Learning: Algorithms, Real-World Applications and Research Directions",,6-117,10.1007/s42979-021-00592-x,https://www.semanticscholar.org/paper/7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea,1302,144,2021.0
53c9f3c34d8481adaf24df3b25581ccf1bc53f5c,Physics-informed machine learning,,422 - 440,10.1038/s42254-021-00314-5,https://www.semanticscholar.org/paper/53c9f3c34d8481adaf24df3b25581ccf1bc53f5c,1356,226,2021.0
12d0353ce8b41b7e5409e5a4a611110aee33c7bc,Thumbs up? Sentiment Classification using Machine Learning Techniques,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",54-177,10.3115/1118693.1118704,https://www.semanticscholar.org/paper/12d0353ce8b41b7e5409e5a4a611110aee33c7bc,9023,39,2002.0
6a6ad9eb495739f4c80e7c09598720c3d5c5dff7,"Federated Learning: Collaborative Machine Learning without
Centralized Training Data","Federated learning (also known as collaborative learning) is a machine learning technique that trains
an algorithm without transferring data samples across numerous decentralized edge devices or
servers. This strategy differs from standard centralized machine learning techniques in which all local
datasets are uploaded to a single server, as well as more traditional decentralized alternatives, which
frequently presume that local data samples are uniformly distributed.
Federated learning allows several actors to collaborate on the development of a single, robust
machine learning model without sharing data, allowing crucial issues such as data privacy, data
security, data access rights, and access to heterogeneous data to be addressed. Defence,
telecommunications, internet of things, and pharmaceutical industries are just a few of the sectors
where it has applications.",90-184,10.46647/ijetms.2022.v06i05.052,https://www.semanticscholar.org/paper/6a6ad9eb495739f4c80e7c09598720c3d5c5dff7,546,6,2022.0
2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c,"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems","Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details",6-196,,https://www.semanticscholar.org/paper/2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c,2490,0,2017.0
6b20af22b0734757d9ead382b201a65f9dd637cc,Machine learning in automated text categorization,"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",11-195,10.1145/505282.505283,https://www.semanticscholar.org/paper/6b20af22b0734757d9ead382b201a65f9dd637cc,8699,197,2001.0
eb9e0da8b7170e3ca4364f2f9010599c2d2556f1,Machine Learning With Python,,41-160,,https://www.semanticscholar.org/paper/eb9e0da8b7170e3ca4364f2f9010599c2d2556f1,2077,0,2019.0
e068be31ded63600aea068eacd12931efd2a1029,UCI Repository of machine learning databases,,5-158,,https://www.semanticscholar.org/paper/e068be31ded63600aea068eacd12931efd2a1029,14185,0,1998.0
e2a85a6766b982ff7c8980e57ca6342d22493827,Adversarial Machine Learning at Scale,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",35-158,,https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827,2678,22,2016.0
fbc6562814e08e416e28a268ce7beeaa3d0708c8,Large-Scale Machine Learning with Stochastic Gradient Descent,,177-186,10.1007/978-3-7908-2604-3_16,https://www.semanticscholar.org/paper/fbc6562814e08e416e28a268ce7beeaa3d0708c8,5351,25,2010.0
fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",22-174,,https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,24641,33,2014.0
0b544dfe355a5070b60986319a3f51fb45d1348e,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,"In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",1724-1734,10.3115/v1/D14-1179,https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e,20200,33,2014.0
7feb0fc888cd55360949554db032d7d1cba9e947,Programs for Machine Learning,"Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.",99-131,,https://www.semanticscholar.org/paper/7feb0fc888cd55360949554db032d7d1cba9e947,9051,3,1994.0
ab06951251e0abfdb866694f9a23a79c72784317,Challenges and opportunities in quantum machine learning,,567 - 576,10.1038/s43588-022-00311-3,https://www.semanticscholar.org/paper/ab06951251e0abfdb866694f9a23a79c72784317,173,135,2022.0
d21703674ae562bae4a849a75847cdd9ead417df,Optimization Methods for Large-Scale Machine Learning,"This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",32-184,10.1137/16M1080173,https://www.semanticscholar.org/paper/d21703674ae562bae4a849a75847cdd9ead417df,2671,183,2016.0
9e27190f2d9b2167d4a66b88696def4585072fd5,SoilGrids250m: Global gridded soil information based on machine learning,"This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",90-179,10.1371/journal.pone.0169748,https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5,2339,109,2017.0
2097ff87df3cb9427c7388bc7b997ed56907d45b,Optimization and Machine Learning,"Problem 1 (20%) Consider the function f(x1, x2) = (x1 + x 2 2) 2 At the point xk = [1, 0] T , find (a) the gradient descent direction (b) xk+1 by exact line search on the gradient descent direction (c) the Newton direction (d) xk+1 by exact line search on the Newton direction Problem 2 (40%) Consider the following quadratic function: f(x) = 1 2 xQx− bx Assume Q is symmetric and positive definite. (a) What’s the gradient of f(x)?",31-115,10.1002/9781119902881,https://www.semanticscholar.org/paper/2097ff87df3cb9427c7388bc7b997ed56907d45b,358,0,2022.0
5d433da6d0f143f20936379910104d2bb139d4ae,ilastik: interactive machine learning for (bio)image analysis,,1226 - 1232,10.1038/s41592-019-0582-9,https://www.semanticscholar.org/paper/5d433da6d0f143f20936379910104d2bb139d4ae,1593,45,2019.0
e0408181bccb7e3754dd5e6785ec47d8beb8b6bd,Machine Learning for High-Speed Corner Detection,,430-443,10.1007/11744023_34,https://www.semanticscholar.org/paper/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd,4554,38,2006.0
c292e473b3825eeb9db03c70b2e1c033aea190d5,Machine learning for molecular and materials science,,547 - 555,10.1038/s41586-018-0337-2,https://www.semanticscholar.org/paper/c292e473b3825eeb9db03c70b2e1c033aea190d5,2256,119,2018.0
61a1565016477b2092a212e7af0e789a250bc552,Pattern Recognition and Machine Learning (Information Science and Statistics),,7-131,,https://www.semanticscholar.org/paper/61a1565016477b2092a212e7af0e789a250bc552,6919,0,2006.0
db0cc2f21b20cbc0ab8946090967399c25709614,Practical Secure Aggregation for Privacy-Preserving Machine Learning,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.",14-134,10.1145/3133956.3133982,https://www.semanticscholar.org/paper/db0cc2f21b20cbc0ab8946090967399c25709614,2183,64,2017.0
e24b8a9531573d284647239affc6c855505b0de4,Adversarial machine learning,"In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.",43-58,10.1145/2046684.2046692,https://www.semanticscholar.org/paper/e24b8a9531573d284647239affc6c855505b0de4,1293,197,2019.0
a8d76d84408c1fe6b1543084e6cec3dfc4ede429,Stable learning establishes some common ground between causal inference and machine learning,,110 - 115,10.1038/s42256-022-00445-z,https://www.semanticscholar.org/paper/a8d76d84408c1fe6b1543084e6cec3dfc4ede429,78,44,2022.0
2afa490dde7a8c582d889530c7f8b042fef6a8b7,Machine learning–accelerated computational fluid dynamics,"Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.",19-194,10.1073/pnas.2101784118,https://www.semanticscholar.org/paper/2afa490dde7a8c582d889530c7f8b042fef6a8b7,514,64,2021.0
62ccd99a65bfc7c735ae1f33b75b107665de95df,Federated Machine Learning,"Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.",1 - 19,10.1145/3298981,https://www.semanticscholar.org/paper/62ccd99a65bfc7c735ae1f33b75b107665de95df,1765,80,2019.0
f75b70c9d7078724b592ec3e21de705e7b6ff73f,Double/Debiased Machine Learning for Treatment and Structural Parameters,"We revisit the classic semiparametric problem of inference on a low dimensional parameter θ_0 in the presence of high-dimensional nuisance parameters η_0. We depart from the classical setting by allowing for η_0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate η_0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η_0 cause a heavy bias in estimators of θ_0 that are obtained by naively plugging ML estimators of η_0 into estimating equations for θ_0. This bias results in the naive estimator failing to be N^(-1/2) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ_0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ_0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N^(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.",11-125,10.1111/ectj.12097,https://www.semanticscholar.org/paper/f75b70c9d7078724b592ec3e21de705e7b6ff73f,1612,100,2017.0
7ae2783a9196fb4bc2a610ae812d19722daddce5,Applications of machine learning to machine fault diagnosis: A review and roadmap,,106587,10.1016/j.ymssp.2019.106587,https://www.semanticscholar.org/paper/7ae2783a9196fb4bc2a610ae812d19722daddce5,1261,440,2020.0
bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098,Understanding Machine Learning,,75-135,10.1007/978-3-030-91585-8_2,https://www.semanticscholar.org/paper/bb0cec8f2d34cfb9dbf8bffd1a5de499311ae098,125,7,2022.0
62df84d6a4d26f95e4714796c2337c9848cc13b5,MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",81-120,,https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5,2139,13,2015.0
c5c4142a01981787a71bf6ebcb791520c458ab5d,FedML: A Research Library and Benchmark for Federated Machine Learning,"Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at this https URL.",49-181,,https://www.semanticscholar.org/paper/c5c4142a01981787a71bf6ebcb791520c458ab5d,408,160,2020.0
6bc43977fb11cceed0b9aa55b23c6dd29dd9a132,Correlation-based Feature Selection for Machine Learning,"A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. On natural domains, CFS typically eliminated well over half the features. In most cases, classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set. Feature selection degraded machine learning performance in cases where some features were eliminated which were highly predictive of very small areas of the instance space. Further experiments compared CFS with a wrapper—a well known approach to feature selection that employs the target learning algorithm to evaluate feature sets. In many cases CFS gave comparable results to the wrapper, and in general, outperformed the wrapper on small datasets. CFS executes many times faster than the wrapper, which allows it to scale to larger datasets. Two methods of extending CFS to handle feature interaction are presented and experimentally evaluated. The first considers pairs of features and the second incorporates iii feature weights calculated by the RELIEF algorithm. Experiments on artificial domains showed that both methods were able to identify interacting features. On natural domains, the pairwise method gave more reliable results than using weights provided by RELIEF.",96-128,,https://www.semanticscholar.org/paper/6bc43977fb11cceed0b9aa55b23c6dd29dd9a132,3812,108,2003.0
57e6cca1479a4642f867e69b4dee93d14259dc3d,Power of data in quantum machine learning,,17-146,10.1038/s41467-021-22539-9,https://www.semanticscholar.org/paper/57e6cca1479a4642f867e69b4dee93d14259dc3d,394,65,2020.0
f1b962fb4070fedd46758e334db3ba4f00ddc3ec,Supervised Machine Learning: A Review of Classification Techniques,"The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.",3-24,,https://www.semanticscholar.org/paper/f1b962fb4070fedd46758e334db3ba4f00ddc3ec,4524,163,2007.0
a9cbbef8f4426329d0687025b34287c35bdd8b38,Machine learning and the physical sciences,"Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.",16-123,10.1103/RevModPhys.91.045002,https://www.semanticscholar.org/paper/a9cbbef8f4426329d0687025b34287c35bdd8b38,1195,427,2019.0
6068d39e92aef1bb0e1291e9931894c35692a85e,Counterfactual Explanations for Machine Learning: A Review,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.",4-122,,https://www.semanticscholar.org/paper/6068d39e92aef1bb0e1291e9931894c35692a85e,337,109,2020.0
f156ecbbb9243522275490d698c6825f4d2e01af,Explainable AI: A Review of Machine Learning Interpretability Methods,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",83-119,10.3390/e23010018,https://www.semanticscholar.org/paper/f156ecbbb9243522275490d698c6825f4d2e01af,1087,166,2020.0
4087e84fc695bb6433d0104ee94f9d7e9f4b7da5,Machine Learning for Fluid Mechanics,"The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.",23-107,10.1146/annurev-fluid-010719-060214,https://www.semanticscholar.org/paper/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5,1488,171,2019.0
0273507eb05f1135f3a05f9c7adc9a56f12c7c5c,Recent advances and applications of machine learning in solid-state materials science,,1-36,10.1038/s41524-019-0221-0,https://www.semanticscholar.org/paper/0273507eb05f1135f3a05f9c7adc9a56f12c7c5c,1284,507,2019.0
8ca86e941da7254613a5d03dd7a6c36886fadc1d,Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning),,1-108,,https://www.semanticscholar.org/paper/8ca86e941da7254613a5d03dd7a6c36886fadc1d,4065,0,2005.0
48ddd9101a90fe65e3061de69626741b843ff5e4,The use of the area under the ROC curve in the evaluation of machine learning algorithms,,1145-1159,10.1016/S0031-3203(96)00142-2,https://www.semanticscholar.org/paper/48ddd9101a90fe65e3061de69626741b843ff5e4,5910,52,1997.0
46f74231b9afeb0c290d6d550043c55045284e5f,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",141-142,10.1109/MSP.2012.2211477,https://www.semanticscholar.org/paper/46f74231b9afeb0c290d6d550043c55045284e5f,3002,7,2012.0
6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,Multimodal Machine Learning: A Survey and Taxonomy,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",423-443,10.1109/TPAMI.2018.2798607,https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,2111,273,2017.0
1800404340d43ba185f2b0e2b4f94201c2eeadaa,Machine Learning,"We are pleased to inform you that your research paper, entitled "" Machine Learning "", has been successfully published in the International Scientific Journal of Engineering and Management (ISJEM) on Volume 02 Issue 04 April 2023. We would like to congratulate you on this achievement, as it is a testament to the hard work and dedication that you have put into your research. With this email, we enclose e-certificates & Published Paper for all the authors as a token from us.",69-163,10.55041/isjem00268,https://www.semanticscholar.org/paper/1800404340d43ba185f2b0e2b4f94201c2eeadaa,0,0,2023.0
a0f303b6e22ef52943355993f57d65938997066a,Machine learning and deep learning,,685 - 695,10.1007/s12525-021-00475-2,https://www.semanticscholar.org/paper/a0f303b6e22ef52943355993f57d65938997066a,520,64,2021.0
24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe,Swarm Learning for decentralized and confidential clinical machine learning,,265 - 270,10.1038/s41586-021-03583-3,https://www.semanticscholar.org/paper/24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe,400,47,2021.0
256db9dba1978f004a67c86ffc321563b1aee79a,Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges,"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the""Rashomon set""of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.",71-156,10.1214/21-ss133,https://www.semanticscholar.org/paper/256db9dba1978f004a67c86ffc321563b1aee79a,383,346,2021.0
80d9f0eb47b712988d19cbe29a7bfa63f2a175d0,A guide to machine learning for biologists,,40 - 55,10.1038/s41580-021-00407-0,https://www.semanticscholar.org/paper/80d9f0eb47b712988d19cbe29a7bfa63f2a175d0,554,174,2021.0
2b7f9117eb6608a58be4c078ca3d69c0e5ccb875,SecureML: A System for Scalable Privacy-Preserving Machine Learning,"Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.",19-38,10.1109/SP.2017.12,https://www.semanticscholar.org/paper/2b7f9117eb6608a58be4c078ca3d69c0e5ccb875,1449,41,2017.0
b5904cd5dbf73b8d5ff13517de490c292d877ee0,Applications of machine learning in drug discovery and development,,463 - 477,10.1038/s41573-019-0024-5,https://www.semanticscholar.org/paper/b5904cd5dbf73b8d5ff13517de490c292d877ee0,1228,121,2019.0
287ba5bf00d96af1596aaf80c178392a9c4fcc28,Machine Learning Basics,"coined in 1959 by Arthur Samuel [Samuel 1959], Tom Mitchell [Mitchell 1997] provided a more formal definition: “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” ML has be applied to many real-world problems or tasks, like medical diagno­ sis, robotics, recommendation systems, facial recognition, stock prices prediction, and sentiment analysis, with great success. We can divide ML algorithms into three main categories (see Figure 4.1): Machine Learning Basics",15-106,10.1145/3447404.3447414,https://www.semanticscholar.org/paper/287ba5bf00d96af1596aaf80c178392a9c4fcc28,229,51,2021.0
f381c53aeb7742e4047d06d84f9e0c4f523231a3,Coronavirus disease (COVID-19) cases analysis using machine-learning applications,,2013 - 2025,10.1007/s13204-021-01868-7,https://www.semanticscholar.org/paper/f381c53aeb7742e4047d06d84f9e0c4f523231a3,245,40,2021.0
4d8f0ae904779a50b2e18fec49e51a5661a98d8a,MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers,"Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.",11-118,10.3390/s21062222,https://www.semanticscholar.org/paper/4d8f0ae904779a50b2e18fec49e51a5661a98d8a,217,77,2021.0
f1664bbaddedea8c250873e7610ab07e53fa7132,Machine learning pipeline for battery state-of-health estimation,,447 - 456,10.1038/s42256-021-00312-3,https://www.semanticscholar.org/paper/f1664bbaddedea8c250873e7610ab07e53fa7132,205,89,2021.0
0d6ef817813d04a3b3ec6c3ce008e104fb3e587a,Classification Based on Decision Tree Algorithm for Machine Learning,"Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.",63-196,10.38094/JASTT20165,https://www.semanticscholar.org/paper/0d6ef817813d04a3b3ec6c3ce008e104fb3e587a,577,79,2021.0
2e5d2f2dc01b150dffc163a9f457848e9b5b5c38,On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice,,67-176,10.1016/j.neucom.2020.07.061,https://www.semanticscholar.org/paper/2e5d2f2dc01b150dffc163a9f457848e9b5b5c38,1023,131,2020.0
69d49a06f09cf934310ccbf3bb2a360fa719272d,Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans,,199 - 217,10.1038/s42256-021-00307-0,https://www.semanticscholar.org/paper/69d49a06f09cf934310ccbf3bb2a360fa719272d,626,115,2020.0
fee8f63972906214b77f16cfeca0b93ee8f36ba2,Fairness in Machine Learning: A Survey,"When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.",27-127,10.1145/3616865,https://www.semanticscholar.org/paper/fee8f63972906214b77f16cfeca0b93ee8f36ba2,359,353,2020.0
97ac11e5a6440eccb70ae7146392ac138c36fa6c,Fairness in Machine Learning,,3-148,10.1007/978-3-030-43883-8_7,https://www.semanticscholar.org/paper/97ac11e5a6440eccb70ae7146392ac138c36fa6c,423,369,2020.0
74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",31-110,,https://www.semanticscholar.org/paper/74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,307,122,2020.0
20f63033e8775cbab0692aed92d38da7e725d64e,Understanding Machine Learning - From Theory to Algorithms,"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability ; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning ; and emerging theoretical concepts such as the PACBayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and nonexpert readers in statistics, computer science, mathematics, and engineering.","I-XVI, 1-397",,https://www.semanticscholar.org/paper/20f63033e8775cbab0692aed92d38da7e725d64e,2293,0,2014.0
d1e701665e73faa648cb15473952576f40e8e122,The Machine‐Learning Approach,,91-133,10.1002/9781119602927.ch2,https://www.semanticscholar.org/paper/d1e701665e73faa648cb15473952576f40e8e122,533,0,2020.0
b9518627db25f05930e931f56497602363a75491,"Definitions, methods, and applications in interpretable machine learning","Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.",22071 - 22080,10.1073/pnas.1900654116,https://www.semanticscholar.org/paper/b9518627db25f05930e931f56497602363a75491,1043,113,2019.0
739769f4862753fc80057194456d758d2a148ee3,Extreme Learning Machine for Regression and Multiclass Classification,"Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.",513-529,10.1109/TSMCB.2011.2168604,https://www.semanticscholar.org/paper/739769f4862753fc80057194456d758d2a148ee3,4820,64,2012.0
f86f1748d1b6d22870f4347fd5d65314ba800583,Reconciling modern machine-learning practice and the classical bias–variance trade-off,"Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.",15849 - 15854,10.1073/pnas.1903070116,https://www.semanticscholar.org/paper/f86f1748d1b6d22870f4347fd5d65314ba800583,1251,47,2018.0
609471b915f5764f63148c1195590b1085a5067b,"The Changing Landscape of Machine Learning: A Comparative Analysis of Centralized Machine Learning, Distributed Machine Learning and Federated Machine Learning",,18-28,10.1007/978-3-031-47508-5_2,https://www.semanticscholar.org/paper/609471b915f5764f63148c1195590b1085a5067b,0,0,2023.0
643da4c4de1954daeac571a82367241db012a8bf,Automatic differentiation in machine learning: a survey,"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply “auto-diff”, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until 
very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other’s results. Despite its 
relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names “dynamic computational 
graphs” and “differentiable programming”. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main imple- 
mentation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms “autodiff”, “automatic differentiation”, and “symbolic differentiation” as these are encountered more and more in machine learning settings.",38-120,,https://www.semanticscholar.org/paper/643da4c4de1954daeac571a82367241db012a8bf,2114,243,2015.0
46c266b3d1274dacd7fce27ee8cb4d587f087a58,Machine Learning Interpretability: A Survey on Methods and Metrics,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.",45-145,10.3390/ELECTRONICS8080832,https://www.semanticscholar.org/paper/46c266b3d1274dacd7fce27ee8cb4d587f087a58,907,141,2019.0
f70b2f20be241f445a61f33c4b8e76e554760340,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",291-300,10.1109/ICSE-SEIP.2019.00042,https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340,605,44,2019.0
b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Data Shapley: Equitable Valuation of Data for Machine Learning,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",23-104,,https://www.semanticscholar.org/paper/b7a717233ec3ff37385ab1b06816d0ca375f5bb3,495,50,2019.0
b55e490637babd50dab3cdaaa3a60a2be6eb1cbb,"Automated Machine Learning - Methods, Systems, Challenges",,75-123,,https://www.semanticscholar.org/paper/b55e490637babd50dab3cdaaa3a60a2be6eb1cbb,979,0,2019.0
71a85e735a3686bef8cce3725ae5ba82e2cabb1b,Underspecification Presents Challenges for Credibility in Modern Machine Learning,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",226:1-226:61,,https://www.semanticscholar.org/paper/71a85e735a3686bef8cce3725ae5ba82e2cabb1b,536,149,2020.0
2bc3644ce4de7fce5812c1455e056649a47c1bbf,Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques,"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).",81542-81554,10.1109/ACCESS.2019.2923707,https://www.semanticscholar.org/paper/2bc3644ce4de7fce5812c1455e056649a47c1bbf,760,46,2019.0
4a7eea3ec3080ecb277bfe466afce4822a1071d7,Quantum embeddings for machine learning,"Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.",99-192,,https://www.semanticscholar.org/paper/4a7eea3ec3080ecb277bfe466afce4822a1071d7,255,22,2020.0
d3f9a39e49abfdf084da558e305be5473c8740e5,Machine learning for alloys,,730-755,10.1038/s41578-021-00340-w,https://www.semanticscholar.org/paper/d3f9a39e49abfdf084da558e305be5473c8740e5,194,334,2021.0
218062f45c15f39bc8f4fb2c930ddf20b5809b11,"Machine Learning Testing: Survey, Landscapes and Horizons","This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.",1-36,10.1109/tse.2019.2962027,https://www.semanticscholar.org/paper/218062f45c15f39bc8f4fb2c930ddf20b5809b11,573,279,2019.0
d0ab11de3077490c80a08abd0fb8827bac84c454,MoleculeNet: a benchmark for molecular machine learning,"A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.",513 - 530,10.1039/c7sc02664a,https://www.semanticscholar.org/paper/d0ab11de3077490c80a08abd0fb8827bac84c454,1258,121,2017.0
6e23398447a022fb9495c44fa80e9de593a574bc,Machine Learning in Agriculture: A Review,"Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.",89-116,10.3390/s18082674,https://www.semanticscholar.org/paper/6e23398447a022fb9495c44fa80e9de593a574bc,1362,120,2018.0
42b1cb0030e174ba4395d987df77cfa6d112d221,Joint 2D-3D-Semantic Data for Indoor Scene Understanding,"We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\deg} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: this http URL",4-180,,https://www.semanticscholar.org/paper/42b1cb0030e174ba4395d987df77cfa6d112d221,716,21,2017.0
c8c494ee5488fe20e0aa01bddf3fc4632086d654,The Cityscapes Dataset for Semantic Urban Scene Understanding,"Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.",3213-3223,10.1109/CVPR.2016.350,https://www.semanticscholar.org/paper/c8c494ee5488fe20e0aa01bddf3fc4632086d654,9416,86,2016.0
92893716be3b7fed89e989dfd4ab4af5e5d8f214,Regularizing Deep Networks With Semantic Data Augmentation,"Data augmentation is widely known as a simple yet surprisingly effective technique for regularizing deep networks. Conventional data augmentation schemes, e.g., flipping, translation or rotation, are low-level, data-independent and class-agnostic operations, leading to limited diversity for augmented samples. To this end, we propose a novel semantic data augmentation algorithm to complement traditional approaches. The proposed method is inspired by the intriguing property that deep networks are effective in learning linearized features, i.e., certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., changing the background or view angle of an object. Based on this observation, translating training samples along many such directions in the feature space can effectively augment the dataset for more diversity. To implement this idea, we first introduce a sampling based method to obtain semantically meaningful directions efficiently. Then, an upper bound of the expected cross-entropy (CE) loss on the augmented training set is derived by assuming the number of augmented samples goes to infinity, yielding a highly efficient algorithm. In fact, we show that the proposed implicit semantic data augmentation (ISDA) algorithm amounts to minimizing a novel robust CE loss, which adds minimal extra computational cost to a normal training procedure. In addition to supervised learning, ISDA can be applied to semi-supervised learning tasks under the consistency regularization framework, where ISDA amounts to minimizing the upper bound of the expected KL-divergence between the augmented features and the original features. Although being simple, ISDA consistently improves the generalization performance of popular deep models (e.g., ResNets and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN, ImageNet, and Cityscapes. Code for reproducing our results is available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.",3733-3748,10.1109/TPAMI.2021.3052951,https://www.semanticscholar.org/paper/92893716be3b7fed89e989dfd4ab4af5e5d8f214,93,85,2020.0
10ffcae001fa724f9fd49fe0726c32166df9209b,Semantic data mining in the information age: A systematic review,"Data mining is the discovery of meaningful information or unrevealed patterns in data. Traditional data‐mining approaches, using statistical calculations, machine learning, artificial intelligence, and database technology, cannot interpret data on a conceptual or semantic level and fail to reveal the meanings within the data. This results in a user not being analyzed and determines its signification and implications. Several semantic data‐mining approaches have been proposed in the past decade that overcome these limitations by using a domain ontology as background knowledge to enable and enhance data‐mining performance. The main contributions of this literature survey include organizing the surveyed articles in a new way that provides ease of understanding for interested researchers, and the provision of a critical analysis and summary of the surveyed articles, identifying the contribution of these papers to the field, and the limitations of the analysis methods and approaches discussed in this corpus, with the intention of informing researchers in this growing field in their innovative approaches to new research. Finally, we identify the future trends and challenges in this study track that will be of concern to future researchers, such as dynamic knowledge‐based methods or big‐data tool collaboration. This survey article provides a comprehensive overview of the literature on domain ontologies as used in the various semantic data‐mining tasks, such as preprocessing, modeling, and postprocessing. We investigated the role of semantic data mining in the field of data science and the processes and methods of applying semantic data mining to a data resource description framework.",3880 - 3916,10.1002/int.22443,https://www.semanticscholar.org/paper/10ffcae001fa724f9fd49fe0726c32166df9209b,15,123,2021.0
11babff42b5bf9841ebb87781bfc21f74acb3d28,Implicit Semantic Data Augmentation for Deep Networks,"In this paper, we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping, translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds. As a consequence, translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently, we first perform an online estimate of the covariance matrix of deep features for each class, which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly, instead of augmenting the samples explicitly, we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set, leading to a highly efficient algorithm. In fact, we show that the proposed ISDA amounts to minimizing a novel robust CE loss, which adds negligible extra computational cost to a normal training procedure. Although being simple, ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet. Code for reproducing our results are available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.",12614-12623,,https://www.semanticscholar.org/paper/11babff42b5bf9841ebb87781bfc21f74acb3d28,139,43,2019.0
cf5e97a98460a8c4766dffb62b33cfb819b22c73,Adversarial Semantic Data Augmentation for Human Pose Estimation,,606-622,10.1007/978-3-030-58529-7_36,https://www.semanticscholar.org/paper/cf5e97a98460a8c4766dffb62b33cfb819b22c73,48,33,2020.0
b03e4702c8427b2458234ff8f37358c68177ccbb,Sherlock: A Deep Learning Approach to Semantic Data Type Detection,"Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery. Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types. However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types. We introduce Sherlock, a multi-input deep neural network for detecting semantic types. We train Sherlock on $686,765$ data columns retrieved from the VizNet corpus by matching $78$ semantic types from DBpedia to column headers. We characterize each matched column with $1,588$ features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values. Sherlock achieves a support-weighted F$_1$ score of $0.89$, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations.",62-156,10.1145/3292500.3330993,https://www.semanticscholar.org/paper/b03e4702c8427b2458234ff8f37358c68177ccbb,131,40,2019.0
6c96408fc1e34effc99ec4cc35b5561ef0c1fed3,PARCIV: Recognizing physical activities having complex interclass variations using semantic data of smartphone,"Smartphones are equipped with precise hardware sensors including accelerometer, gyroscope, and magnetometer. These devices provide real‐time semantic data that can be used to recognize daily life physical activities for personalized smart health assessment. Existing studies focus on the recognition of simple physical activities but they lacked in providing accurate recognition of physical activities having complex interclass variations. Therefore, this research focuses on the accurate recognition of physical activities having complex interclass variations. We propose a two‐layered approach called PARCIVthat first clusters similar activities based on semantic data and then recognize them using a machine learning classifier. Our two‐layered approach first bounds the highly indistinguishable activities in clusters to avoid misclassification with other distinguishable activities and thereafter recognize them on a fine‐grained level within each cluster. To evaluate our approach, we make an android application that collects labeled data by using smartphone sensors from 10 participants, while performing activities. PARCIV recognizes distinguishable as well as indistinguishable activities with high accuracy of 99% on the self‐collected dataset. Furthermore, PARCIV achieve 95% accuracy on the publicly available dataset used by state‐of‐the‐art studies. PARCIVoutperforms various state‐of‐the‐art studies by 8%‐17% for simple activities as well as complex activities.",532 - 549,10.1002/spe.2846,https://www.semanticscholar.org/paper/6c96408fc1e34effc99ec4cc35b5561ef0c1fed3,31,42,2020.0
9cf22346cf0219f32480259848c8ac7ed5519291,The Semantic Data Dictionary – An Approach for Describing and Annotating Data,"It is common practice for data providers to include text descriptions for each column when publishing data sets in the form of data dictionaries. While these documents are useful in helping an end-user properly interpret the meaning of a column in a data set, existing data dictionaries typically are not machine-readable and do not follow a common specification standard. We introduce the Semantic Data Dictionary, a specification that formalizes the assignment of a semantic representation of data, enabling standardization and harmonization across diverse data sets. In this paper, we present our Semantic Data Dictionary work in the context of our work with biomedical data; however, the approach can and has been used in a wide range of domains. The rendition of data in this form helps promote improved discovery, interoperability, reuse, traceability, and reproducibility. We present the associated research and describe how the Semantic Data Dictionary can help address existing limitations in the related literature. We discuss our approach, present an example by annotating portions of the publicly available National Health and Nutrition Examination Survey data set, present modeling challenges, and describe the use of this approach in sponsored research, including our work on a large National Institutes of Health (NIH)-funded exposure and health data portal and in the RPI-IBM collaborative Health Empowerment by Analytics, Learning, and Semantics project. We evaluate this work in comparison with traditional data dictionaries, mapping languages, and data integration tools.",443-486,10.1162/dint_a_00058,https://www.semanticscholar.org/paper/9cf22346cf0219f32480259848c8ac7ed5519291,23,70,2020.0
5d62ffc9a5527d1fd52342ff3359c2bb5e7cc4de,"Semantic data interoperability, digital medicine, and e-health in infectious disease management: a review",,1023 - 1034,10.1007/s10096-019-03501-6,https://www.semanticscholar.org/paper/5d62ffc9a5527d1fd52342ff3359c2bb5e7cc4de,32,87,2019.0
5dfde01d761d97c3a6c609007531973eb1229d09,A Decentralized Architecture for Sharing and Querying Semantic Data,,3-18,10.1007/978-3-030-21348-0_1,https://www.semanticscholar.org/paper/5dfde01d761d97c3a6c609007531973eb1229d09,15,26,2019.0
95543515c290a9d0c6c3f7b94e8087936950b40c,Using Ontologies for Semantic Data Integration,,187-202,10.1007/978-3-319-61893-7_11,https://www.semanticscholar.org/paper/95543515c290a9d0c6c3f7b94e8087936950b40c,87,51,2018.0
fa2d3edeb7c3ad333028e6089458d1175d16cae2,"SemEHR: A general-purpose semantic search system to surface semantic data from clinical notes for tailored care, trial recruitment, and clinical research","Objective Unlocking the data contained within both structured and unstructured components of Electronic Health Records (EHRs) has the potential to provide a step change in data available forsecondary research use, generation of actionable medical insights, hospital management and trial recruitment. To achieve this, we implemented SemEHR - a semantic search and analytics, open source tool for EHRs. Methods SemEHR implements a generic information extraction (IE) and retrieval infrastructure by identifying contextualised mentions of a wide range of biomedical concepts within EHRs. Natural Language Processing (NLP) annotations are further assembled at patient level and extended with EHR-specific knowledge to generate a timeline for each patient. The semantic data is serviced via ontology-based search and analytics interfaces. Results SemEHR has been deployed to a number of UK hospitals including the Clinical Record Interactive Search (CRIS), an anonymised replica of the EHR of the UK South London and Maudsley (SLaM) NHS Foundation Trust, one of Europes largest providers of mental health services. In two CRIS-based studies, SemEHR achieved 93% (Hepatitis C case) and 99% (HIV case) F-Measure results in identifying true positive patients. At King’s College Hospital in London, as part of the CogStack programme (github.com/cogstack), SemEHR is being used to recruit patients into the UK Dept of Health 100k Genome Project (genomicsengland.co.uk). The validation study suggests that the tool can validate previously recruited cases and is very fast in searching phenotypes - time for recruitment criteria checking reduced from days to minutes. Validated on an open intensive care EHR data - MIMICIII, the vital signs extracted by SemEHR can achieve around 97% accuracy. Conclusion Results from the multiple case studies demonstrate SemEHR’s efficiency - weeks or months of work can be done within hours or minutes in some cases. SemEHR provides a more comprehensive view of a patient, bringing in more and unexpected insight compared to study-oriented bespoke information extraction systems. SemEHR is open source available at https://github.com/CogStack/SemEHR.",530 - 537,10.1093/jamia/ocx160,https://www.semanticscholar.org/paper/fa2d3edeb7c3ad333028e6089458d1175d16cae2,88,26,2017.0
be86963ce9b0580f7778b797c2076726371b13ec,Visualizing Semantic Data,,41-154,10.1007/978-3-319-63962-8_230-1,https://www.semanticscholar.org/paper/be86963ce9b0580f7778b797c2076726371b13ec,0,12,2019.0
e7af4716dd7c0b6601c0929d54a6404af6df5a33,Knowledge Reasoning with Semantic Data for Real-Time Data Processing in Smart Factory,"The application of high-bandwidth networks and cloud computing in manufacturing systems will be followed by mass data. Industrial data analysis plays important roles in condition monitoring, performance optimization, flexibility, and transparency of the manufacturing system. However, the currently existing architectures are mainly for offline data analysis, not suitable for real-time data processing. In this paper, we first define the smart factory as a cloud-assisted and self-organized manufacturing system in which physical entities such as machines, conveyors, and products organize production through intelligent negotiation and the cloud supervises this self-organized process for fault detection and troubleshooting based on data analysis. Then, we propose a scheme to integrate knowledge reasoning and semantic data where the reasoning engine processes the ontology model with real time semantic data coming from the production process. Based on these ideas, we build a benchmarking system for smart candy packing application that supports direct consumer customization and flexible hybrid production, and the data are collected and processed in real time for fault diagnosis and statistical analysis.",28-193,10.3390/s18020471,https://www.semanticscholar.org/paper/e7af4716dd7c0b6601c0929d54a6404af6df5a33,38,14,2018.0
0856f7c043b812e684593ed989151e2300e789de,Building an active semantic data warehouse for precision dairy farming,"ABSTRACT Digitalization of agricultural technology has led to the emergence of precision dairy farming, which strives for the simultaneous improvement of productivity as well as animal well-being in dairy farming through advanced use of technology such as movement sensors and milking parlors to monitor, control, and improve dairy production processes. The data warehouse serves as the appropriate technology for effective and efficient data management, which is paramount to the success of precision dairy farming. This paper presents a joint effort between industry and academia on the experimental development of an active semantic data warehouse to support business intelligence and business analytics in precision dairy farming. The research follows an action research approach, deriving lessons for theory and practice from a set of actions taken in the course of the project. Among these actions are the development of a loading stage to facilitate data integration, the definition of an analysis view as well as the introduction of semantic OLAP patterns to facilitate analysis, and analysis rules to automate periodic analyses. The large volumes of generated sensor data in precision dairy farming required careful decision-making concerning the appropriate level of detail of the data stored in the data warehouse. Semantic technologies played a key role in rendering analysis accessible to end users.",122 - 141,10.1080/10919392.2018.1444344,https://www.semanticscholar.org/paper/0856f7c043b812e684593ed989151e2300e789de,29,36,2018.0
d5ab4aab4d30bc9f76151a50ca509030e5c1bcdc,AnchorViz: Facilitating Classifier Error Discovery through Interactive Semantic Data Exploration,"When building a classifier in interactive machine learning, human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This paper presents AnchorViz, an interactive visualization that facilitates error discovery through semantic data exploration. By creating example-based anchors, users create a topology to spread data based on their similarity to the anchors and examine the inconsistencies between data points that are semantically related. The results from our user study show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods.",10-192,10.1145/3172944.3172950,https://www.semanticscholar.org/paper/d5ab4aab4d30bc9f76151a50ca509030e5c1bcdc,34,34,2018.0
30c2433a934ead817308aa05ae42728f6b743af3,Synthesizing Type-Detection Logic for Rich Semantic Data Types using Open-source Code,"Given a table of data, existing systems can often detect basic atomic types (e.g., strings vs. numbers) for each column. A new generation of data-analytics and data-preparation systems are starting to automatically recognize rich semantic types such as date-time, email address, etc., for such metadata can bring an array of benefits including better table understanding, improved search relevance, precise data validation, and semantic data transformation. However, existing approaches only detect a limited number of types using regular-expression-like patterns, which are often inaccurate, and cannot handle rich semantic types such as credit card and ISBN numbers that encode semantic validations (e.g., checksum). We developed AUTOTYPE from open-source repositories like GitHub. Users only need to provide a set of positive examples for a target data type and a search keyword, our system will automatically identify relevant code, and synthesize type-detection functions using execution traces. We compiled a benchmark with 112 semantic types, out of which the proposed system can synthesize code to detect 84 such types at a high precision. Applying the synthesized type-detection logic on web table columns have also resulted in a significant increase in data types discovered compared to alternative approaches.",20-107,10.1145/3183713.3196888,https://www.semanticscholar.org/paper/30c2433a934ead817308aa05ae42728f6b743af3,28,49,2018.0
3f4297e1acabb25b9dd96462d8e9adfdca8e2dd7,An ontology-guided semantic data integration framework to support integrative data analysis of cancer survival,,83-105,10.1186/s12911-018-0636-4,https://www.semanticscholar.org/paper/3f4297e1acabb25b9dd96462d8e9adfdca8e2dd7,50,70,2018.0
d7ed878c08c90186e3bf607c20ff943834ad0d68,Semantic Data Integration,,263-305,10.1007/978-3-319-49340-4_8,https://www.semanticscholar.org/paper/d7ed878c08c90186e3bf607c20ff943834ad0d68,30,114,2017.0
0accf351494c1586fc9694100ec3d99444f1b4e7,In Search of Sustainable Design Patterns: Combining Data Mining and Semantic Data Modelling on Disparate Building Data,,43-185,10.1007/978-3-030-00220-6_3,https://www.semanticscholar.org/paper/0accf351494c1586fc9694100ec3d99444f1b4e7,32,36,2018.0
e7d9c306138b3a583c48d9a3d46a5597221deaae,Semantic data mining: A survey of ontology-based approaches,"Semantic Data Mining refers to the data mining tasks that systematically incorporate domain knowledge, especially formal semantics, into the process. In the past, many research efforts have attested the benefits of incorporating domain knowledge in data mining. At the same time, the proliferation of knowledge engineering has enriched the family of domain knowledge, especially formal semantics and Semantic Web ontologies. Ontology is an explicit specification of conceptualization and a formal way to define the semantics of knowledge and data. The formal structure of ontology makes it a nature way to encode domain knowledge for the data mining use. In this survey paper, we introduce general concepts of semantic data mining. We investigate why ontology has the potential to help semantic data mining and how formal semantics in ontologies can be incorporated into the data mining process. We provide detail discussions for the advances and state of art of ontology-based approaches and an introduction of approaches that are based on other form of knowledge representations.",244-251,10.1109/ICOSC.2015.7050814,https://www.semanticscholar.org/paper/e7d9c306138b3a583c48d9a3d46a5597221deaae,158,83,2015.0
1a8e864093212caf1ec98c207c55cc21d4dc5775,Semantic Data Integration for Knowledge Graph Construction at Query Time,"The evolution of the Web of documents into a Web of services and data has resulted in an increased availability of data from almost any domain. For example, general domain knowledge bases such as DBpedia or Wikidata, or domain specific Web sources like the Oxford Art archive, allow for accessing knowledge about a wide variety of entities including people, organizations, or art paintings. However, these data sources publish data in different ways, and they may be equipped with different search capabilities, e.g., SPARQL endpoints or REST services, thus requiring data integration techniques that provide a unified view of the published data. We devise a semantic data integration approach named FuhSen that exploits keyword and structured search capabilities of Web data sources and generates on-demand knowledge graphs merging data collected from available Web sources. Resulting knowledge graphs model semantics or meaning of merged data in terms of entities that satisfy keyword queries, and relationships among those entities. FuhSen relies on both RDF to semantically describe the collected entities, and on semantic similarity measures to decide on relatedness among entities that should be merged. We empirically evaluate the results of FuhSen data integration techniques on data from the DBpedia knowledge base. The experimental results suggest that FuhSen data integration techniques accurately integrate similar entities semantically into knowledge graphs.",109-116,10.1109/ICSC.2017.85,https://www.semanticscholar.org/paper/1a8e864093212caf1ec98c207c55cc21d4dc5775,19,23,2017.0
292e6cf64909bea4a84eef2c7ca7b21efe80b51b,The Essence of Functional Programming on Semantic Data,,750-776,10.1007/978-3-662-54434-1_28,https://www.semanticscholar.org/paper/292e6cf64909bea4a84eef2c7ca7b21efe80b51b,13,47,2017.0
55e022fb7581bb9e1fce678d21fb25ffbb3fbb88,Deep visual-semantic alignments for generating image descriptions,"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",3128-3137,10.1109/CVPR.2015.7298932,https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88,5128,65,2014.0
469f509688c53285af68c708e5c317b6df2ad6a2,Semantic data mapping technology to solve semantic data problem on heterogeneity aspect,"The diversity of applications developed with different programming languages, application/data architectures, database systems and representation of data/information leads to heterogeneity issues. One of the problem challenges in the problem of heterogeneity is about heterogeneity data in term of semantic aspect. The semantic aspect is about data that has the same name with different meaning or data that has a different name with the same meaning. The semantic data mapping process is the best solution in the current days to solve semantic data problem. There are many semantic data mapping technologies that have been used in recent years. This research aims to compare and analyze existing semantic data mapping technology using five criteria’s. After comparative and analytical process, this research provides recommendations of appropriate semantic data mapping technology based on several criteria’s. Furthermore, at the end of this research we apply the recommended semantic data mapping technology to be implemented with the real data in the specific application. The result of this research is the semantic data mapping file that contains all data structures in the application data source. This semantic data mapping file can be used to map, share and integrate with other semantic data mapping from other applications and can also be used to integrate with the ontology language.",161-172,10.26555/IJAIN.V3I3.131,https://www.semanticscholar.org/paper/469f509688c53285af68c708e5c317b6df2ad6a2,8,52,2017.0
eefa2a53c635ebb011eb4b2410b6c7c65f4e51a5,Engineering IoT Healthcare Applications: Towards a Semantic Data Driven Sustainable Architecture,,407-418,10.1007/978-3-319-49655-9_49,https://www.semanticscholar.org/paper/eefa2a53c635ebb011eb4b2410b6c7c65f4e51a5,32,21,2016.0
2f4df08d9072fc2ac181b7fced6a245315ce05c8,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",580-587,10.1109/CVPR.2014.81,https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8,23016,55,2013.0
8823f1bc6c008ed0081a42d7ab9b5f05ea2f605f,PepeSearch: Semantic Data for the Masses,"With the emergence of the Web of Data, there is a need of tools for searching and exploring the growing amount of semantic data. Unfortunately, such tools are scarce and typically require knowledge of SPARQL/RDF. We propose here PepeSearch, a portable tool for searching semantic datasets devised for mainstream users. PepeSearch offers a multi-class search form automatically constructed from a SPARQL endpoint. We have tested PepeSearch with 15 participants searching a Linked Open Data version of the Norwegian Register of Business Enterprises for non-trivial challenges. Retrieval performance was encouragingly high and usability ratings were also very positive, thus suggesting that PepeSearch is effective for searching semantic datasets by mainstream users. We also assessed its portability by configuring PepeSearch to query other SPARQL endpoints.",49-200,10.1371/journal.pone.0151573,https://www.semanticscholar.org/paper/8823f1bc6c008ed0081a42d7ab9b5f05ea2f605f,26,30,2016.0
014722151ee576e0361910b31f30902942f6a15a,Towards a Programmable Semantic Extract-Transform-Load Framework for Semantic Data Warehouses,"In order to create better decisions for business analytics, organizations increasingly use external data, structured, semi-structured and unstructured, in addition to the (mostly structured) internal data. Current Extract-Transform-Load (ETL) tools are not suitable for this ""open world scenario"" because they do not consider semantic issues in the integration process. Also, current ETL tools neither support processing semantic-aware data nor create a Semantic Data Warehouse (DW) as a semantic repository of semantically integrated data. This paper describes SETL: a (Python-based) programmable Semantic ETL framework. SETL builds on Semantic Web (SW) standards and tools and supports developers by offering a number of powerful modules, classes and methods for (dimensional and semantic) DW constructs and tasks. Thus it supports semantic-aware data sources, semantic integration, and creating a semantic DW, composed of an ontology and its instances. A comprehensive experimental evaluation comparing SETL to a solution made with traditional tools (requiring much more hand-coding) on a concrete use case, shows that SETL provides better performance, knowledge base quality and programmer productivity.",77-159,10.1145/2811222.2811229,https://www.semanticscholar.org/paper/014722151ee576e0361910b31f30902942f6a15a,44,25,2015.0
487b1794b8d7025f6fbf5e1bcf32b14b0618d8fd,Advanced Real-Time Indoor Tracking Based on the Viterbi Algorithm and Semantic Data,"A real-time indoor tracking system based on the Viterbi algorithm is developed. This Viterbi principle is used in combination with semantic data to improve the accuracy, that is, the environment of the object that is being tracked and a motion model. The starting point is a fingerprinting technique for which an advanced network planner is used to automatically construct the radio map, avoiding a time consuming measurement campaign. The developed algorithm was verified with simulations and with experiments in a building-wide testbed for sensor experiments, where a median accuracy below 2 m was obtained. Compared to a reference algorithm without Viterbi or semantic data, the results indicated a significant improvement: the mean accuracy and standard deviation improved by, respectively, 26.1% and 65.3%. Thereafter a sensitivity analysis was conducted to estimate the influence of node density, grid size, memory usage, and semantic data on the performance.",66-168,10.1155/2015/271818,https://www.semanticscholar.org/paper/487b1794b8d7025f6fbf5e1bcf32b14b0618d8fd,43,27,2015.0
863b9432378270b1ba79f7f0532970cd1a214740,Semantic data extraction over MQTT for IoTcentric wireless sensor networks,"The emergence of the paradigm of Internet of Things (IoT) has necessitated the development of machine-to-machine (M2M) protocols geared towards wireless sensor network interfacing to the Internet and implementing machine learning algorithms over the cloud. This paper discusses the viability of the MQ Telemetry Transport (MQTT) protocol for such applications. This paper introduces MQTT along with its merits and demerits and suitability towards IoT applications. Then it outlines an implementation of a typical IoT application involving ubiquitous sensing, M2M communication, cloud computing and semantic data extraction. The results of this experiment are then analyzed. Finally, the paper looks at future improvements in the proposed architecture for widespread use.",227-232,10.1109/IOTA.2016.7562727,https://www.semanticscholar.org/paper/863b9432378270b1ba79f7f0532970cd1a214740,20,4,2016.0
1f9ebb1040f948b4b1caff16ad7a7e0ae7bb42d3,A Platform for Urban Analytics and Semantic Data Integration in City Planning,,21-36,10.1007/978-3-662-47386-3_2,https://www.semanticscholar.org/paper/1f9ebb1040f948b4b1caff16ad7a7e0ae7bb42d3,70,22,2015.0
b461d4f3c0da5bcb5a9fd6b09ba2aaddf6be2324,Review on web search personalization through semantic data,"Information generated within the World Wide Web is increasing at huge rate and user's access for their own interested work. Ambiguous queries and lower ability of user to precisely express what they need have been one of the challenging obstacles in improving search results. It is obvious that the current search engines retrieve results are sometimes not of user relevance due to keyword based search, so to fill the gap between the user interest and retrieved search results, personalized web search needs to be evolved. For example, a biologist and a programmer may use the same query “mouse” with different search context, but the search systems would return same results. Again still there is need to customize the search results by re-ranking the retrieved results incorporating the user interests. In this paper we have presents various approaches to personalize web search through user modelling by analyzing semantic data.",1-6,10.1109/ICECCT.2015.7226083,https://www.semanticscholar.org/paper/b461d4f3c0da5bcb5a9fd6b09ba2aaddf6be2324,16,29,2015.0
2bc472478247173b25aaf5e12f2604a74a8d2063,"The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes","The morphometry of a kidney tumor revealed by contrast-enhanced Computed Tomography (CT) imaging is an important factor in clinical decision making surrounding the lesion's diagnosis and treatment. Quantitative study of the relationship between kidney tumor morphology and clinical outcomes is difficult due to data scarcity and the laborious nature of manually quantifying imaging predictors. Automatic semantic segmentation of kidneys and kidney tumors is a promising tool towards automatically quantifying a wide array of morphometric features, but no sizeable annotated dataset is currently available to train models for this task. We present the KiTS19 challenge dataset: A collection of multi-phase CT imaging, segmentation masks, and comprehensive clinical outcomes for 300 patients who underwent nephrectomy for kidney tumors at our center between 2010 and 2018. 210 (70%) of these patients were selected at random as the training set for the 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge and have been released publicly. With the presence of clinical context and surgical outcomes, this data can serve not only for benchmarking semantic segmentation models, but also for developing and studying biomarkers which make use of the imaging and semantic segmentation masks.",15-105,,https://www.semanticscholar.org/paper/2bc472478247173b25aaf5e12f2604a74a8d2063,323,24,2019.0
1f3652b98b825f2ec4fd4a5c2bd2416377eef8b0,GraphScale: Adding Expressive Reasoning to Semantic Data Stores,"We present GraphScale, a technology that empowers semantic data stores with OWL reasoning. It connects a given data store with any state of the art OWL reasoner. The underlying abstraction approach allows to efficiently perform a full materialization of the store based on sound and complete OWL 2 RL reasoning for high-performance querying.",73-163,,https://www.semanticscholar.org/paper/1f3652b98b825f2ec4fd4a5c2bd2416377eef8b0,11,4,2015.0
805cf2ec243c7b085e172b3d5ada11fd275dd4df,Spatiotemporal query processing for semantic data stream,"In this paper, we propose a method for processing spatiotemporal queries on semantic data streams generated from diverse sensors. On the Internet of Things (IoT) environment, the number of mobile sensors greatly increases and their locations are becoming more important. IoT services may not be fully supported when only considering the temporal feature of streaming data. Accordingly, stream processing should be performed with consideration into both temporal and spatial factors. However, existing researches have a limitation of processing spatial queries since they focus on the temporal processing of streaming data. To support spatiotemporal query processing on semantic data streams, we propose a query language, which integrates temporal and geospatial properties. Specifically, we construct a spatiotemporal index to process the proposed spatiotemporal query language efficiently. The experimental results with a prototype implementation show that the proposed method processes spatiotemporal queries in an acceptable time.",290-297,10.1109/ICOSC.2015.7050822,https://www.semanticscholar.org/paper/805cf2ec243c7b085e172b3d5ada11fd275dd4df,11,17,2015.0
9c6eca31f311eae935e84efa6966c0165bc4e14a,ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data,,96-129,10.1016/j.isprsjprs.2020.01.013,https://www.semanticscholar.org/paper/9c6eca31f311eae935e84efa6966c0165bc4e14a,744,92,2019.0
cd773d9b82f8331f5b2910ec2f52388aafb6dbfd,Rich feature hierarchies for accurate object detection and semantic segmentation,"Formulation of the problem. Over the past few years, there has been little progress in object detection techniques. The most efficient are complex computational ensemble methods, which usually combine several low-level image properties with high-level properties. However, every day interest in artificial intelligence is growing, and the idea of using neural networks on board a spacecraft, with the possibility of making decisions and issuing one-time commands, is very promising, since it makes it possible to analyze a large data stream in real time without resorting to ground station, thereby not losing information when transmitting a packet. The purpose of the work is to conduct research on the possibility of effective use of modern models of neural networks, to develop a methodology for their use in the problem of object detection and analysis of the element base for hardware implementation with the possibility of using convolutional neural networks for thermovideotelemetry on board a spacecraft. Results of work. An approach has been formulated that combines two key ideas: 1) application of high-throughput convolutional neural networks for downward processing of image regions in order to localize and segment objects; 2) preliminary training for the auxiliary task, followed by fine tuning of the domain, which gives a significant increase in performance in the case when the training data is insufficient. The analysis of the element base for the possibility of hardware implementation of neural networks on board a spacecraft using electrical radio products of domestic and foreign production is carried out. Practical significance. The efficiency of preliminary network training for an auxiliary task is shown, followed by fine tuning of the subject area. A technique is described that makes it possible to increase the average accuracy of detecting objects in an image by more than 30%. The analysis of the existing element base, the possibility of hardware implementation of neural networks on board the spacecraft using electrical radio products of domestic and foreign production, as well as the criteria for selecting key elements.",11-140,10.18127/j00338486-202109-11,https://www.semanticscholar.org/paper/cd773d9b82f8331f5b2910ec2f52388aafb6dbfd,1102,0,2021.0
ff6af0d04b36bfcf16a70e22f4815013ef2f59f0,Semantic Foggy Scene Understanding with Synthetic Data,,973 - 992,10.1007/s11263-018-1072-8,https://www.semanticscholar.org/paper/ff6af0d04b36bfcf16a70e22f4815013ef2f59f0,776,80,2017.0
ac65e6ccf8a54ec4f296808c0b10774f2c2a4aaa,Semantic data provisioning and reasoning for the Internet of Things,"Semantic technologies could facilitate realizing features like interoperability and reasoning for Internet of Things (IoT). However, the dynamic and heterogeneous nature of IoT data, constrained resources, and real-time requirements set challenges for applying these technologies. In this paper, we study approaches for delivering semantic data from IoT nodes to distributed reasoning engines and reasoning over such data. We perform experiments to evaluate the scalability of these approaches and also study how reasoning is affected by different data aggregation strategies.",67-72,10.1109/IOT.2014.7030117,https://www.semanticscholar.org/paper/ac65e6ccf8a54ec4f296808c0b10774f2c2a4aaa,25,22,2014.0
25cd63c0728876505cac2c09af072198563f8df0,"Towards GeoSpatial semantic data management: strengths, weaknesses, and challenges ahead","An immense wealth of data is already accessible through the Semantic Web and an increasing part of it also has geospatial context or relevance. Although existing technology is mature enough to integrate a variety of information from heterogeneous sources into interlinked features, it still falls behind when it comes to representation and reasoning on spatial characteristics. It is only lately that several RDF stores have begun to accommodate geospatial entities and to enable some kind of processing on them. To address interoperability, the OGC has recently adopted the GeoSPARQL standard, which defines a vocabulary for representing geometric types in RDF and an extension to the SPARQL language for formulating queries. In this paper, we provide a comprehensive review of the current state-of-the-art in geospatially-enabled semantic data management. Apart from an insightful analysis of the available architectures in industry and academia, we conduct an evaluation study on prominent RDF stores with geospatial support. We also compare their performance and attested capabilities to renowned DBMSs widely used in geospatial applications. We introduce a methodology suitable to assess RDF stores for robustness against large geospatial datasets, and also for expressiveness on a variety of queries involving both spatial and thematic criteria. As our findings demonstrate, the potential for query optimization, advanced indexing schemes, and spatio-semantic extensions is significant. Towards this goal, we point out several challenging issues for joint research by the GIS and Semantic Web communities.",85-120,10.1145/2666310.2666410,https://www.semanticscholar.org/paper/25cd63c0728876505cac2c09af072198563f8df0,31,51,2014.0
9108230681fb2a789a47aa43b3b3a2e42b800d3d,Pattern Based Feature Construction in Semantic Data Mining,"The authors propose a new method for mining sets of patterns for classification, where patterns are represented as SPARQL queries over RDFS. The method contributes to so-called semantic data mining, a data mining approach where domain ontologies are used as background knowledge, and where the new challenge is to mine knowledge encoded in domain ontologies, rather than only purely empirical data. The authors have developed a tool that implements this approach. Using this the authors have conducted an experimental evaluation including comparison of our method to state-of-the-art approaches to classification of semantic data and an experimental study within emerging subfield of meta-learning called semantic meta-mining. The most important research contributions of the paper to the state-of-art are as follows. For pattern mining research or relational learning in general, the paper contributes a new algorithm for discovery of new type of patterns. For Semantic Web research, it theoretically and empirically illustrates how semantic, structured data can be used in traditional machine learning methods through a pattern-based approach for constructing semantic features.",27-65,10.4018/ijswis.2014010102,https://www.semanticscholar.org/paper/9108230681fb2a789a47aa43b3b3a2e42b800d3d,32,90,2014.0
4c5e6d601575d9d88afc405af821ddd79788a5d6,Semantic Data Extraction,,827-834,10.1016/J.PROTCY.2015.02.119,https://www.semanticscholar.org/paper/4c5e6d601575d9d88afc405af821ddd79788a5d6,4,11,2015.0
07fdff9dae9ea9ba32c251669b3b7a66269930f9,Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision,"In this paper, we study the semi-supervised semantic segmentation problem via exploring both labeled data and extra unlabeled data. We propose a novel consistency regularization approach, called cross pseudo supervision (CPS). Our approach imposes the consistency on two segmentation networks perturbed with different initialization for the same input image. The pseudo one-hot label map, output from one perturbed segmentation network, is used to supervise the other segmentation network with the standard cross-entropy loss, and vice versa. The CPS consistency has two roles: encourage high similarity between the predictions of two perturbed networks for the same input image, and expand training data by using the unlabeled data with pseudo labels. Experiment results show that our approach achieves the state-of-the-art semi-supervised segmentation performance on Cityscapes and PASCAL VOC 2012.",2613-2622,10.1109/CVPR46437.2021.00264,https://www.semanticscholar.org/paper/07fdff9dae9ea9ba32c251669b3b7a66269930f9,452,42,2021.0
a368b002477d8a57dff40fd41bf79c4d9041fe64,From Business Intelligence to Semantic Data Stream Management,,85-93,10.1007/978-3-319-12256-4_9,https://www.semanticscholar.org/paper/a368b002477d8a57dff40fd41bf79c4d9041fe64,21,24,2014.0
7d1a9c693af4f099c3c7b8e2a8c92c5525105dd2,Semantic Communications in Networked Systems: A Data Significance Perspective,"We present our vision for a departure from the established way of architecting and assessing communication networks, by incorporating the semantics of information, defined not necessarily as the meaning of the messages, but as their significance, possibly within a real-time constraint, relative to the purpose of the data exchange. We argue that research efforts must focus on laying the theoretical foundations of a redesign of the entire process of information generation, transmission, and usage for networked systems in unison by developing advanced semantic metrics for communications and control systems; an optimal sampling theory combining signal sparsity and timeliness, for real-time prediction/reconstruction/control under communication constraints and delays; temporally effective compressed sensing techniques for decision making and inference directly in the compressed domain; and semantic-aware data generation, channel coding, packetization, feedback, and multiple and random access schemes that reduce the volume of data and the energy consumption, increasing the number of supportable devices. This paradigm shift targets jointly optimal information gathering, information dissemination, and decision making policies in networked systems.",233-240,10.1109/MNET.106.2100636,https://www.semanticscholar.org/paper/7d1a9c693af4f099c3c7b8e2a8c92c5525105dd2,79,28,2021.0
fdb813d8b927bdd21ae1858cafa6c34b66a36268,Learning deep structured semantic models for web search using clickthrough data,"Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.",35-164,10.1145/2505515.2505665,https://www.semanticscholar.org/paper/fdb813d8b927bdd21ae1858cafa6c34b66a36268,1811,25,2013.0
72e82166511d953a49ac97f8d4cef1da26dddf21,A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation*,"Recently, significant progress has been made on semantic segmentation. However, the success of supervised semantic segmentation typically relies on a large amount of labeled data, which is time-consuming and costly to obtain. Inspired by the success of semi-supervised learning methods for image classification, here we propose a simple yet effective semi-supervised learning framework for semantic segmentation. We demonstrate that the devil is in the details: a set of simple designs and training techniques can collectively improve the performance of semi-supervised semantic segmentation significantly. Previous works [3], [25] fail to effectively employ strong augmentation in pseudo-label learning, as the large distribution disparity caused by strong augmentation harms the batch nor-malization statistics. We design a new batch normalization, namely distribution-specific batch normalization (DSBN) to address this problem and show the importance of strong augmentation for semantic segmentation. Moreover, we design a self-correction loss, which is effective in terms of noise resistance. We conduct a series of ablation studies to show the effectiveness of each component. Our method achieves state-of-the-art results in the semi-supervised settings on the Cityscapes and Pascal VOC datasets.",8209-8218,10.1109/ICCV48922.2021.00812,https://www.semanticscholar.org/paper/72e82166511d953a49ac97f8d4cef1da26dddf21,66,45,2021.0
e3ad973cbaaae8e28aa237c13fc5328ed4eb752d,"Avogadro: an advanced semantic chemical editor, visualization, and analysis platform",,17 - 17,10.1186/1758-2946-4-17,https://www.semanticscholar.org/paper/e3ad973cbaaae8e28aa237c13fc5328ed4eb752d,6271,77,2012.0
64f16d43358b08cf9522dac2d0bc2683f8d4a1ab,Exploring Cross-Image Pixel Contrast for Semantic Segmentation,"Current semantic segmentation methods focus only on mining ""local"" context, i.e., dependencies between pixels within individual images, by context-aggregation modules (e.g., dilated convolution, neural attention) or structure-aware optimization criteria (e.g., IoU-like loss). However, they ignore ""global"" context of the training data, i.e., rich semantic relations between pixels across different images. Inspired by recent advance in unsupervised contrastive representation learning, we propose a pixel-wise contrastive algorithm for semantic segmentation in the fully supervised setting. The core idea is to enforce pixel embeddings belonging to a same semantic class to be more similar than embeddings from different classes. It raises a pixel-wise metric learning paradigm for semantic segmentation, by explicitly exploring the structures of labeled pixels, which were rarely explored before. Our method can be effortlessly incorporated into existing segmentation frameworks without extra overhead during testing. We experimentally show that, with famous segmentation models (i.e., DeepLabV3, HRNet, OCR) and backbones (i.e., ResNet, HRNet), our method brings performance improvements across diverse datasets (i.e., Cityscapes, PASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our community to rethink the current de facto training paradigm in semantic segmentation.",7283-7293,10.1109/iccv48922.2021.00721,https://www.semanticscholar.org/paper/64f16d43358b08cf9522dac2d0bc2683f8d4a1ab,340,93,2021.0
07a34b86a4394d532dcaa6db38701297893bcb4e,Self-Ensembling With GAN-Based Data Augmentation for Domain Adaptation in Semantic Segmentation,"Deep learning-based semantic segmentation methods have an intrinsic limitation that training a model requires a large amount of data with pixel-level annotations. To address this challenging issue, many researchers give attention to unsupervised domain adaptation for semantic segmentation. Unsupervised domain adaptation seeks to adapt the model trained on the source domain to the target domain. In this paper, we introduce a self-ensembling technique, one of the successful methods for domain adaptation in classification. However, applying self-ensembling to semantic segmentation is very difficult because heavily-tuned manual data augmentation used in self-ensembling is not useful to reduce the large domain gap in the semantic segmentation. To overcome this limitation, we propose a novel framework consisting of two components, which are complementary to each other. First, we present a data augmentation method based on Generative Adversarial Networks (GANs), which is computationally efficient and effective to facilitate domain alignment. Given those augmented images, we apply self-ensembling to enhance the performance of the segmentation network on the target domain. The proposed method outperforms state-of-the-art semantic segmentation methods on unsupervised domain adaptation benchmarks.",6829-6839,10.1109/ICCV.2019.00693,https://www.semanticscholar.org/paper/07a34b86a4394d532dcaa6db38701297893bcb4e,205,59,2019.0
7a6ae9d856c2850f8175e211122b375c5428c1f0,FuseSeg: Semantic Segmentation of Urban Scenes Based on RGB and Thermal Data Fusion,"Semantic segmentation of urban scenes is an essential component in various applications of autonomous driving. It makes great progress with the rise of deep learning technologies. Most of the current semantic segmentation networks use single-modal sensory data, which are usually the RGB images produced by visible cameras. However, the segmentation performance of these networks is prone to be degraded when lighting conditions are not satisfied, such as dim light or darkness. We find that thermal images produced by thermal imaging cameras are robust to challenging lighting conditions. Therefore, in this article, we propose a novel RGB and thermal data fusion network named FuseSeg to achieve superior performance of semantic segmentation in urban scenes. The experimental results demonstrate that our network outperforms the state-of-the-art networks. Note to Practitioners—This article investigates the problem of semantic segmentation of urban scenes when lighting conditions are not satisfied. We provide a solution to this problem via information fusion with RGB and thermal data. We build an end-to-end deep neural network, which takes as input a pair of RGB and thermal images and outputs pixel-wise semantic labels. Our network could be used for urban scene understanding, which serves as a fundamental component of many autonomous driving tasks, such as environment modeling, obstacle avoidance, motion prediction, and planning. Moreover, the simple design of our network allows it to be easily implemented using various deep learning frameworks, which facilitates the applications on different hardware or software platforms.",1000-1011,10.1109/tase.2020.2993143,https://www.semanticscholar.org/paper/7a6ae9d856c2850f8175e211122b375c5428c1f0,91,60,2020.0
dfd72b994765a1979c6872fc8948657885a31752,Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation,"Visual Domain Adaptation is a problem of immense importance in computer vision. Previous approaches showcase the inability of even deep neural networks to learn informative representations across domain shift. This problem is more severe for tasks where acquiring hand labeled data is extremely hard and tedious. In this work, we focus on adapting the representations learned by segmentation networks across synthetic and real domains. Contrary to previous approaches that use a simple adversarial objective or superpixel information to aid the process, we propose an approach based on Generative Adversarial Networks (GANs) that brings the embeddings closer in the learned feature space. To showcase the generality and scalability of our approach, we show that we can achieve state of the art results on two challenging scenarios of synthetic to real domain adaptation. Additional exploratory experiments show that our approach: (1) generalizes to unseen domains and (2) results in improved alignment of source and target distributions.",3752-3761,10.1109/CVPR.2018.00395,https://www.semanticscholar.org/paper/dfd72b994765a1979c6872fc8948657885a31752,427,34,2017.0
fd9aef292d9f8e092f4627bbc4fc3377b55ab6b5,Towards an Architecture for Managing Big Semantic Data in Real-Time,,45-53,10.1007/978-3-642-39031-9_5,https://www.semanticscholar.org/paper/fd9aef292d9f8e092f4627bbc4fc3377b55ab6b5,29,17,2013.0
f15aecec2a5672beaae77853ea0eea560505df8e,FDA: Fourier Domain Adaptation for Semantic Segmentation,"We describe a simple method for unsupervised domain adaptation, whereby the discrepancy between the source and target distributions is reduced by swapping the low-frequency spectrum of one with the other. We illustrate the method in semantic segmentation, where densely annotated images are aplenty in one domain (synthetic data), but difficult to obtain in another (real images). Current state-of-the-art methods are complex, some requiring adversarial optimization to render the backbone of a neural network invariant to the discrete domain selection variable. Our method does not require any training to perform the domain alignment, just a simple Fourier Transform and its inverse. Despite its simplicity, it achieves state-of-the-art performance in the current benchmarks, when integrated into a relatively standard semantic segmentation model. Our results indicate that even simple procedures can discount nuisance variability in the data that more sophisticated methods struggle to learn away.",4084-4094,10.1109/cvpr42600.2020.00414,https://www.semanticscholar.org/paper/f15aecec2a5672beaae77853ea0eea560505df8e,628,56,2020.0
f3fc543fb0ebc7cb51a22823d298e14cc2b222a6,Enabling Semantic Search Based on Conceptual Graphs over Encrypted Outsourced Data,"Currently, searchable encryption is a hot topic in the field of cloud computing. The existing achievements are mainly focused on keyword-based search schemes, and almost all of them depend on predefined keywords extracted in the phases of index construction and query. However, keyword-based search schemes ignore the semantic representation information of users’ retrieval and cannot completely match users’ search intention. Therefore, how to design a content-based search scheme and make semantic search more effective and context-aware is a difficult challenge. In this paper, for the first time, we define and solve the problems of semantic search based on conceptual graphs (CGs) over encrypted outsourced data in clouding computing (SSCG). We first employ the efficient measure of “sentence scoring” in text summarization and Tregex to extract the most important and simplified topic sentences from documents. We then convert these simplified sentences into CGs. To perform quantitative calculation of CGs, we design a new method that can map CGs to vectors. Next, we rank the returned results based on “text summarization score”. Furthermore, we propose a basic idea for SSCG and give a significantly improved scheme to satisfy the security guarantee of searchable symmetric encryption (SSE). Finally, we choose a real-world dataset, i.e., the CNN dataset to test our scheme. The results obtained from the experiment show the effectiveness of our proposed scheme.",813-823,10.1109/TSC.2016.2622697,https://www.semanticscholar.org/paper/f3fc543fb0ebc7cb51a22823d298e14cc2b222a6,160,39,2019.0
a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096,SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,"Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",1-14,10.18653/v1/S17-2001,https://www.semanticscholar.org/paper/a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096,1543,91,2017.0
e65f148eb5a60ab701dd54931363afbb1cf30eae,Exploring exploratory search: a user study with linked semantic data,"The maturation of semantic technologies and the growing popularity of the Linked Open Data (LOD) cloud make it possible to expose linked semantic data sets to end users in order to empower a range of analytical tasks taking advantage of knowledge integration and semantic linking. Linked semantic data appears to offer a great potential for exploratory search, which is open-ended, multi-faceted, and iterative in nature. However, there is limited insight into how browsing through linked semantic data sets can support exploratory search. This paper presents a user study with a uni-focal semantic browsing interface for exploratory search through several data sets linked via domain ontologies. The study, which is qualitative and exploratory in nature and uses music as an illustrative domain, examines (i) obstacles and challenges related to user exploratory search in LOD and (ii) the serendipitous learning effect and the role semantics plays in that. The approach and lessons learnt can benefit future human factor studies to evaluate interactive exploration of linked semantic data, as well as technology developers to become aware of issues that have to be addressed in to facilitate exploratory search with LOD.",2,10.1145/2462197.2462199,https://www.semanticscholar.org/paper/e65f148eb5a60ab701dd54931363afbb1cf30eae,21,31,2013.0
88e8a250f8e28544fe7b74db72ec18a7627ea456,Semi-Supervised Semantic Segmentation With Cross-Consistency Training,"In this paper, we present a novel cross-consistency based semi-supervised approach for semantic segmentation. Consistency training has proven to be a powerful semi-supervised learning framework for leveraging unlabeled data under the cluster assumption, in which the decision boundary should lie in low-density regions. In this work, we first observe that for semantic segmentation, the low-density regions are more apparent within the hidden representations than within the inputs. We thus propose cross-consistency training, where an invariance of the predictions is enforced over different perturbations applied to the outputs of the encoder. Concretely, a shared encoder and a main decoder are trained in a supervised manner using the available labeled examples. To leverage the unlabeled examples, we enforce a consistency between the main decoder predictions and those of the auxiliary decoders, taking as inputs different perturbed versions of the encoder's output, and consequently, improving the encoder's representations. The proposed method is simple and can easily be extended to use additional training signal, such as image-level labels or pixel-level labels across different domains. We perform an ablation study to tease apart the effectiveness of each component, and conduct extensive experiments to demonstrate that our method achieves state-of-the-art results in several datasets.",12671-12681,10.1109/cvpr42600.2020.01269,https://www.semanticscholar.org/paper/88e8a250f8e28544fe7b74db72ec18a7627ea456,488,72,2020.0
77725617432f5fa9c37f152918694ff39c7b1957,Semantic Data Mining of Financial News Articles,,294-307,10.1007/978-3-642-40897-7_20,https://www.semanticscholar.org/paper/77725617432f5fa9c37f152918694ff39c7b1957,17,21,2013.0
90ce37d2df4b89cc5454e421d4b2dce5f27def74,Evaluation of semantic data storages for integrating heterogenous disciplines in automation systems engineering,"Automation systems development projects typically require the integration of heterogeneous local tool data models that come from various disciplines and sources. Semantic data integration provides solutions for bridging semantic gaps between common project-level concepts and the local tool concepts used by each discipline. The following use cases represent the foundation for efficient data integration: (a) data insertion in the local tool models, (b) transformation of data between the local models and a common model, and (c) querying across concepts from different local models by using the common model. The selection of a proper semantic data storage for storing the data has a strong impact on efficiently executing these use cases. Three different important types of semantic storages have been identified: ontology file storages, triple storages, and relational databases storages. In this paper, we evaluate them, and identify their drawbacks and advantages in the context of the presented integration use cases' requirements.",6858-6865,10.1109/IECON.2013.6700268,https://www.semanticscholar.org/paper/90ce37d2df4b89cc5454e421d4b2dce5f27def74,13,21,2013.0
f397d5e75e8799cebf32a9e095b4cc4f02fe7cac,Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding,,1182 - 1204,10.1007/s11263-019-01182-4,https://www.semanticscholar.org/paper/f397d5e75e8799cebf32a9e095b4cc4f02fe7cac,104,85,2019.0
86eeb603370514857780464c902d992020a0c94c,Semantic Data Warehouse Design: From ETL to Deployment à la Carte,,64-83,10.1007/978-3-642-37450-0_5,https://www.semanticscholar.org/paper/86eeb603370514857780464c902d992020a0c94c,38,35,2013.0
c577c6835cf29b109c8c647abb61a1245ddcb6d8,Towards Vagueness-Aware Semantic Data,"The emergence in recent years of initiatives like the Linked Open Data (LOD) has led to a significant increase in the amount of structured semantic data on the Web. In this paper we argue that the shareability and wider reuse of such data can very often be hampered by the existence of vagueness within it, as this makes the data's meaning less explicit. Moreover, as a way to reduce this problem, we propose a vagueness metaontology that may represent in an explicit way the nature and characteristics of vague elements within semantic data.",40-45,,https://www.semanticscholar.org/paper/c577c6835cf29b109c8c647abb61a1245ddcb6d8,7,7,2013.0
b7eac64a8410976759445cce235469163d23ee65,Data Recombination for Neural Semantic Parsing,"Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.",66-106,10.18653/v1/P16-1002,https://www.semanticscholar.org/paper/b7eac64a8410976759445cce235469163d23ee65,448,40,2016.0
b8601c86905b0184b9387b042400609febb93d10,A survey of loss functions for semantic segmentation,"Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.",1-7,10.1109/CIBCB48159.2020.9277638,https://www.semanticscholar.org/paper/b8601c86905b0184b9387b042400609febb93d10,559,25,2020.0
5c1e86ad3071d5ebd963fa5f84033b836266337d,Using the Semantic Web in digital humanities: Shift from data publishing to data-analysis and serendipitous knowledge discovery,"The present study examines how perceived insecurity in the neighborhood affects adolescent civic engagement: directly or indirectly via subjective well-being; and whether these relationships are the same for boys and girls. Participants included 2651 adolescents (1488 girls and 1163 boys), aged 15 to 18. The participants filled out a questionnaire, which consisted of the Civic Commitment Scale, adopted from Flanagan et al. (2007), the Future Civic Activity Scale, adopted from Paltie and Seydo (2003), the Fear of Crime Scale (Tolano, Gorman-Smith, Henry, 2001), and the Satisfaction with Life Scale (Pavit, Diener, 1993). Path analysis indicated that an unsafe neighborhood leads to girls’ and boys’ lower level of subjective well-being and when they are less satisfied with their life, their civic engagement decreases. Only for boys, but not for girls, the unsafe neighborhood could directly encourage boys to actively participate in community life.",187-193,10.3233/sw-190386,https://www.semanticscholar.org/paper/5c1e86ad3071d5ebd963fa5f84033b836266337d,78,36,2020.0
a6cfe02d90260640a151a2cd172b2bf3b000b738,Enriching Texture Analysis with Semantic Data,"We argue for the importance of explicit semantic modelling in human-centred texture analysis tasks such as retrieval, annotation, synthesis, and zero-shot learning. To this end, low-level attributes are selected and used to define a semantic space for texture. 319 texture classes varying in illumination and rotation are positioned within this semantic space using a pair wise relative comparison procedure. Low-level visual features used by existing texture descriptors are then assessed in terms of their correspondence to the semantic space. Textures with strong presence of attributes connoting randomness and complexity are shown to be poorly modelled by existing descriptors. In a retrieval experiment semantic descriptors are shown to outperform visual descriptors. Semantic modelling of texture is thus shown to provide considerable value in both feature selection and in analysis tasks.",1248-1255,10.1109/CVPR.2013.165,https://www.semanticscholar.org/paper/a6cfe02d90260640a151a2cd172b2bf3b000b738,30,31,2013.0
fb4915611ad787c74d44ebb53f886e5802204593,Probabilistic data association for semantic SLAM,"Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.",1722-1729,10.1109/ICRA.2017.7989203,https://www.semanticscholar.org/paper/fb4915611ad787c74d44ebb53f886e5802204593,382,46,2017.0
9358d2ae944cfbdcb4b48e2e0c5f7ad97118b74e,The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes,"Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images, thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation - in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.",3234-3243,10.1109/CVPR.2016.352,https://www.semanticscholar.org/paper/9358d2ae944cfbdcb4b48e2e0c5f7ad97118b74e,1897,43,2016.0
f58b61d70caca32c265c39f539c81730b842a3df,Unsupervised Intra-Domain Adaptation for Semantic Segmentation Through Self-Supervision,"Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model, from this adaptation, we separate target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard subdomain. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.",3763-3772,10.1109/cvpr42600.2020.00382,https://www.semanticscholar.org/paper/f58b61d70caca32c265c39f539c81730b842a3df,308,40,2020.0
d8af6a45eaea68adda8597ae65f91ece152f7b21,Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation,"Convolutional neural networks are designed for dense data, but vision data is often sparse (stereo depth, point clouds, pen stroke, etc.). We present a method to handle sparse depth data with optional dense RGB, and accomplish depth completion and semantic segmentation changing only the last layer. Our proposal efficiently learns sparse features without the need of an additional validity mask. We show how to ensure network robustness to varying input sparsities. Our method even works with densities as low as 0.8% (8 layer lidar), and outperforms all published state-of-the-art on the Kitti depth completion benchmark.",52-60,10.1109/3DV.2018.00017,https://www.semanticscholar.org/paper/d8af6a45eaea68adda8597ae65f91ece152f7b21,230,27,2018.0
d9acae36f15e4b6fb4a9dc31777321f2526fd43b,Querying Semantic Data on the Web?,"The Semantic Web is the initiative of the W3C to make information on the Web readable not only by humans but also by machines. RDF is the data model for Semantic Web data, and SPARQL is the standard query language for this data model. In recent years, we have witnessed a constant growth in the amount of RDF data available on the Web, which has motivated the theoretical study of fundamental aspects of RDF and SPARQL. The goal of this paper is two-fold: to introduce SPARQL, which is a fundamental technology for the development of the Semantic Web, and to present some interesting and non-trivial problems on RDF data management at a Web scale, that we think the database community should address.",6-17,10.1145/2430456.2430458,https://www.semanticscholar.org/paper/d9acae36f15e4b6fb4a9dc31777321f2526fd43b,18,52,2013.0
07c704709448988b6247032277b04e9190d16008,Learning Semantic Segmentation From Synthetic Data: A Geometrically Guided Input-Output Adaptation Approach,"As an alternative to manual pixel-wise annotation, synthetic data has been increasingly used for training semantic segmentation models. Such synthetic images and semantic labels can be easily generated from virtual 3D environments. In this work, we propose an approach to cross-domain semantic segmentation with the auxiliary geometric information, which can also be easily obtained from virtual environments. The geometric information is utilized on two levels for reducing domain shift: on the input level, we augment the standard image translation network with the geometric information to translate synthetic images into realistic style; on the output level, we build a task network which simultaneously performs semantic segmentation and depth estimation. Meanwhile, adversarial training is applied on the joint output space to preserve the correlation between semantics and depth. The proposed approach is validated on two pairs of synthetic to real dataset: from Virtual KITTI to KITTI, and from SYNTHIA to Cityscapes, where we achieve a clear performance gain compared to the baselines and various competing methods, demonstrating the effectiveness of the geometric information for cross-domain semantic segmentation.",1841-1850,10.1109/CVPR.2019.00194,https://www.semanticscholar.org/paper/07c704709448988b6247032277b04e9190d16008,204,63,2018.0
1365b4a286e607a4902ef11c84a1f309719d946c,ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation,"Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real-world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging “synthetic-2-real” set-ups and show that the approach can also be used for detection.",2512-2521,10.1109/CVPR.2019.00262,https://www.semanticscholar.org/paper/1365b4a286e607a4902ef11c84a1f309719d946c,999,51,2018.0
21b58c8aba44c173493e418a797a1f36c6dae8a9,ERFNet: Efficient Residual Factorized ConvNet for Real-Time Semantic Segmentation,"Semantic segmentation is a challenging task that addresses most of the perception needs of intelligent vehicles (IVs) in an unified way. Deep neural networks excel at this task, as they can be trained end-to-end to accurately classify multiple object categories in an image at pixel level. However, a good tradeoff between high quality and computational resources is yet not present in the state-of-the-art semantic segmentation approaches, limiting their application in real vehicles. In this paper, we propose a deep architecture that is able to run in real time while providing accurate semantic segmentation. The core of our architecture is a novel layer that uses residual connections and factorized convolutions in order to remain efficient while retaining remarkable accuracy. Our approach is able to run at over 83 FPS in a single Titan X, and 7 FPS in a Jetson TX1 (embedded device). A comprehensive set of experiments on the publicly available Cityscapes data set demonstrates that our system achieves an accuracy that is similar to the state of the art, while being orders of magnitude faster to compute than other architectures that achieve top precision. The resulting tradeoff makes our model an ideal approach for scene understanding in IV applications. The code is publicly available at: https://github.com/Eromera/erfnet",263-272,10.1109/TITS.2017.2750080,https://www.semanticscholar.org/paper/21b58c8aba44c173493e418a797a1f36c6dae8a9,1016,38,2018.0
9d67af2158807aa815b5a4485b076f7a18ce6ab4,Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy Scene Understanding,,28-145,10.1007/978-3-030-01261-8_42,https://www.semanticscholar.org/paper/9d67af2158807aa815b5a4485b076f7a18ce6ab4,177,68,2018.0
f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8,VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,"We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-of-the-art methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1).",12,,https://www.semanticscholar.org/paper/f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8,939,38,2017.0
1d6a5d0299ed8458191e4e0407d4d513e6a7dd7e,3D Semantic Segmentation with Submanifold Sparse Convolutional Networks,"Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard ""dense"" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SS-CNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.",9224-9232,10.1109/CVPR.2018.00961,https://www.semanticscholar.org/paper/1d6a5d0299ed8458191e4e0407d4d513e6a7dd7e,1167,26,2017.0
dc3f8c8513441915408ab0549e9ac5f2f2f31eec,3D Semantic Parsing of Large-Scale Indoor Spaces,"In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for discovering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geometric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6, 000m2 and over 215 million points, demonstrating robust results readily useful for practical applications.",1534-1543,10.1109/CVPR.2016.170,https://www.semanticscholar.org/paper/dc3f8c8513441915408ab0549e9ac5f2f2f31eec,1282,41,2016.0
7e92c50e680819486365949ee7ef2adce7d0704b,Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation,,1106 - 1120,10.1007/s11263-020-01395-y,https://www.semanticscholar.org/paper/7e92c50e680819486365949ee7ef2adce7d0704b,365,60,2020.0
64e23f006fda19bbbbc324b1353851ebca95aa02,RangeNet ++: Fast and Accurate LiDAR Semantic Segmentation,"Perception in autonomous vehicles is often carried out through a suite of different sensing modalities. Given the massive amount of openly available labeled RGB data and the advent of high-quality deep learning algorithms for image-based recognition, high-level semantic perception tasks are pre-dominantly solved using high-resolution cameras. As a result of that, other sensor modalities potentially useful for this task are often ignored. In this paper, we push the state of the art in LiDAR-only semantic segmentation forward in order to provide another independent source of semantic information to the vehicle. Our approach can accurately perform full semantic segmentation of LiDAR point clouds at sensor frame rate. We exploit range images as an intermediate representation in combination with a Convolutional Neural Network (CNN) exploiting the rotating LiDAR sensor model. To obtain accurate results, we propose a novel post-processing algorithm that deals with problems arising from this intermediate representation such as discretization errors and blurry CNN outputs. We implemented and thoroughly evaluated our approach including several comparisons to the state of the art. Our experiments show that our approach outperforms state-of-the-art approaches, while still running online on a single embedded GPU. The code can be accessed at https://github.com/PRBonn/lidar-bonnetal.",4213-4220,10.1109/IROS40897.2019.8967762,https://www.semanticscholar.org/paper/64e23f006fda19bbbbc324b1353851ebca95aa02,729,23,2019.0
9b15362b9a025071aa170f7ed81a761bc057c859,Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training,,297-313,10.1007/978-3-030-01219-9_18,https://www.semanticscholar.org/paper/9b15362b9a025071aa170f7ed81a761bc057c859,1100,47,2018.0
bc7d49fcd26042c755f1ab663f8b93351e6768fa,Effective Use of Synthetic Data for Urban Scene Semantic Segmentation,,42-161,10.1007/978-3-030-01216-8_6,https://www.semanticscholar.org/paper/bc7d49fcd26042c755f1ab663f8b93351e6768fa,139,55,2018.0
68dd4b89ce1407372a29d05ca9e4e1a2e0513617,"A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.","How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched.",211-240,10.1037/0033-295X.104.2.211,https://www.semanticscholar.org/paper/68dd4b89ce1407372a29d05ca9e4e1a2e0513617,6289,139,1997.0
9f93e2de4c24acd3ffb10efae1c849a63830148d,Domain Generalization via Model-Agnostic Learning of Semantic Features,"Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift. Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge of inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component. The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.",6447-6458,,https://www.semanticscholar.org/paper/9f93e2de4c24acd3ffb10efae1c849a63830148d,552,53,2019.0
88512be44744615f4baa8e14f600f036db4c2433,Semantic Understanding of Scenes Through the ADE20K Dataset,,302 - 321,10.1007/s11263-018-1140-0,https://www.semanticscholar.org/paper/88512be44744615f4baa8e14f600f036db4c2433,1302,44,2016.0
e9de7cc307b6c2a39383ef1a9acbca1ea3dfe27c,Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks,,23-194,10.1007/978-3-319-54181-5_12,https://www.semanticscholar.org/paper/e9de7cc307b6c2a39383ef1a9acbca1ea3dfe27c,353,41,2016.0
d0e25741c5f840a5783e3536cf732a8415697112,Geospatial data ontology: the semantic foundation of geospatial data integration and sharing,"ABSTRACT Effective integration and wide sharing of geospatial data is an important and basic premise to facilitate the research and applications of geographic information science. However, the semantic heterogeneity of geospatial data is a major problem that significantly hinders geospatial data integration and sharing. Ontologies are regarded as a promising way to solve semantic problems by providing a formalized representation of geographic entities and relationships between them in a manner understandable to machines. Thus, many efforts have been made to explore ontology-based geospatial data integration and sharing. However, there is a lack of a specialized ontology that would provide a unified description for geospatial data. In this paper, with a focus on the characteristics of geospatial data, we propose a unified framework for geospatial data ontology, denoted GeoDataOnt, to establish a semantic foundation for geospatial data integration and sharing. First, we provide a characteristics hierarchy of geospatial data. Next, we analyze the semantic problems for each characteristic of geospatial data. Subsequently, we propose the general framework of GeoDataOnt, targeting these problems according to the characteristics of geospatial data. GeoDataOnt is then divided into multiple modules, and we show a detailed design and implementation for each module. Key limitations and challenges of GeoDataOnt are identified, and broad applications of GeoDataOnt are discussed.",269 - 296,10.1080/20964471.2019.1661662,https://www.semanticscholar.org/paper/d0e25741c5f840a5783e3536cf732a8415697112,35,79,2019.0
27ff37bd17a5e59149e645184ed4a0c2fb9cf2f2,Semantic Graph Convolutional Networks for 3D Human Pose Regression,"In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression tasks with graph-structured data. SemGCN learns to capture semantic information such as local and global node relationships, which is not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We further investigate applying SemGCN to 3D human pose regression. Our formulation is intuitive and sufficient since both 2D and 3D human poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a human body. We carry out comprehensive studies to validate our method. The results prove that SemGCN outperforms state of the art while using 90% fewer parameters.",3420-3430,10.1109/CVPR.2019.00354,https://www.semanticscholar.org/paper/27ff37bd17a5e59149e645184ed4a0c2fb9cf2f2,411,86,2019.0
64a1dbdd7653eaca25c78e87335ee156b6f6959e,Constrained Language Models Yield Few-Shot Semantic Parsers,"We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.",57-141,10.18653/v1/2021.emnlp-main.608,https://www.semanticscholar.org/paper/64a1dbdd7653eaca25c78e87335ee156b6f6959e,148,64,2021.0
582259e5d9cc29eb0148d6232c5579241f2c4f43,An introduction to latent semantic analysis,"Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual‐usage meaning of words by statistical computations applied to a large corpus of text (Landauer & Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA's reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word‐word and passage‐word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.",259-284,10.1080/01638539809545028,https://www.semanticscholar.org/paper/582259e5d9cc29eb0148d6232c5579241f2c4f43,5134,48,1998.0
5e207a9ebda631d34681c5ad3b53979ee8cb985d,HadoopRDF: A Scalable Semantic Data Analytical Engine,,633-641,10.1007/978-3-642-31576-3_80,https://www.semanticscholar.org/paper/5e207a9ebda631d34681c5ad3b53979ee8cb985d,39,12,2012.0
f64e1106e93928a4f49071f20d80d4a18d184c1d,Discovering Semantic Data of Interest from Un-mappable Memory with Confidence,"Uncovering semantic data of interest in memory pages without memory mapping information is an important capability in computer forensics. Existing memory mappingguided techniques do not work in that scenario as pointers in the un-mappable memory cannot be resolved and navigated. To address this problem, we present a probabilistic inference-based approach called DIMSUM to enable the recognition of data structure instances from un-mappable memory. Given a set of memory pages and the specification of a target data structure, DIMSUM will identify instances of the data structure in those pages with quantifiable confidence. More specifically, it builds graphical models based on boolean constraints generated from the data structure and the memory page contents. Probabilistic inference is performed on the graphical models to generate results ranked with probabilities. Our experiments with realworld applications on both Linux and Android platforms show that DIMSUM achieves higher effectiveness than nonprobabilistic approaches without memory mapping information.",50-109,,https://www.semanticscholar.org/paper/f64e1106e93928a4f49071f20d80d4a18d184c1d,34,30,2012.0
b2324651155468c9b6bef8a2e006272126d17608,Fast-SCNN: Fast Semantic Segmentation Network,"The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.",85-131,,https://www.semanticscholar.org/paper/b2324651155468c9b6bef8a2e006272126d17608,372,40,2019.0
b1e79800ff36c1708d350e019641f423be78b494,Semantic Earth Observation Data Cubes,"There is an increasing amount of free and open Earth observation (EO) data, yet more information is not necessarily being generated from them at the same rate despite high information potential. The main challenge in the big EO analysis domain is producing information from EO data, because numerical, sensory data have no semantic meaning; they lack semantics. We are introducing the concept of a semantic EO data cube as an advancement of state-of-the-art EO data cubes. We define a semantic EO data cube as a spatio-temporal data cube containing EO data, where for each observation at least one nominal (i.e., categorical) interpretation is available and can be queried in the same instance. Here we clarify and share our definition of semantic EO data cubes, demonstrating how they enable different possibilities for data retrieval, semantic queries based on EO data content and semantically enabled analysis. Semantic EO data cubes are the foundation for EO data expert systems, where new information can be inferred automatically in a machine-based way using semantic queries that humans understand. We argue that semantic EO data cubes are better positioned to handle current and upcoming big EO data challenges than non-semantic EO data cubes, while facilitating an ever-diversifying user-base to produce their own information and harness the immense potential of big EO data.",102,10.3390/DATA4030102,https://www.semanticscholar.org/paper/b1e79800ff36c1708d350e019641f423be78b494,29,58,2019.0
0d13dae976c95853039395d8544b7cd31987783f,That’s So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets,"We propose a novel data augmentation approach to enhance computational behavioral analysis using social media text. In particular, we collect a Twitter corpus of the descriptions of annoying behaviors using the #petpeeve hashtags. In the qualitative analysis, we study the language use in these tweets, with a special focus on the fine-grained categories and the geographic variation of the language. In quantitative analysis, we show that lexical and syntactic features are useful for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance.",2557-2563,10.18653/v1/D15-1306,https://www.semanticscholar.org/paper/0d13dae976c95853039395d8544b7cd31987783f,263,0,2015.0
adad17ef6f347440cab909e417920ea3888bf2cb,Ubiquitous web navigation through harvesting embedded semantic data: A mobile scenario,"In this paper, we investigate how the Semantic Web can enhance web navigation and accessibility by following a hybrid approach of document-oriented and data-oriented considerations. Precisely, we propose a methodology for specifying, extracting, and presenting semantic data embedded in XHTML documents with RDFa in order to enable and improve ubiquitous web navigation and accessibility for end-users. In our context, embedded data does not only contain data type property annotations, but also object properties for interlinking, and embedded domain knowledge for enhanced content navigation through ontology reasoning. We provide a prototype implementation, called Semantic Web Component SWC and evaluate our methodology along a concrete scenario for mobile devices and with respect to precision, performance, network traffic, and usability. Evaluation results suggest that our approach decreases network traffic as well as the amount of information presented to a user without requiring significantly more processing time, and that it allows creating a satisfactory navigation experience.",93-109,10.3233/ICA-2012-0393,https://www.semanticscholar.org/paper/adad17ef6f347440cab909e417920ea3888bf2cb,23,34,2012.0
2b1fbf14de0381a48c736e625fbb1329d71c798c,Semantic Autoencoder for Zero-Shot Learning,"Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g. attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g. attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.",4447-4456,10.1109/CVPR.2017.473,https://www.semanticscholar.org/paper/2b1fbf14de0381a48c736e625fbb1329d71c798c,762,69,2017.0
7fc9a268aeebfa25b77a784fb47d0959523cff00,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",171 - 179,10.1107/S2052520616003954,https://www.semanticscholar.org/paper/7fc9a268aeebfa25b77a784fb47d0959523cff00,5108,54,2016.0
948fd800ecdd3c99488dde36b41480ca1b8acce3,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",D442 - D450,10.1093/nar/gky1106,https://www.semanticscholar.org/paper/948fd800ecdd3c99488dde36b41480ca1b8acce3,5488,41,2018.0
95cd83603a0d2b6918a8e34a5637a8f382da96f5,"MIMIC-III, a freely accessible critical care database",,68-123,10.1038/sdata.2016.35,https://www.semanticscholar.org/paper/95cd83603a0d2b6918a8e34a5637a8f382da96f5,5669,24,2016.0
98128fd412ebfa90201a276f2c59020ccc696a75,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",D1074 - D1082,10.1093/nar/gkx1037,https://www.semanticscholar.org/paper/98128fd412ebfa90201a276f2c59020ccc696a75,5207,22,2017.0
da692ee969d9c33986196372c3f7cb87fa6b6f8f,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",D8 - D13,10.1093/nar/gkx1095,https://www.semanticscholar.org/paper/da692ee969d9c33986196372c3f7cb87fa6b6f8f,4774,13,2017.0
6e1e6afb314f9c5a24d744252a30aa5efc313571,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",D362 - D368,10.1093/nar/gkw937,https://www.semanticscholar.org/paper/6e1e6afb314f9c5a24d744252a30aa5efc313571,5723,61,2016.0
0f5c63182b5d40850c741888a89e6c055a3593af,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",D279 - D285,10.1093/nar/gkv1344,https://www.semanticscholar.org/paper/0f5c63182b5d40850c741888a89e6c055a3593af,4809,19,2015.0
51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",1613 - 1617,10.1099/ijsem.0.001755,https://www.semanticscholar.org/paper/51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,5455,22,2017.0
16b0744424f02e01fe2f01b3ea03e2862f1359fc,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",D733 - D745,10.1093/nar/gkv1189,https://www.semanticscholar.org/paper/16b0744424f02e01fe2f01b3ea03e2862f1359fc,4192,72,2015.0
f986968735459e789890f24b6b277b0920a9725d,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",1452-1464,10.1109/TPAMI.2017.2723009,https://www.semanticscholar.org/paper/f986968735459e789890f24b6b277b0920a9725d,3302,48,2018.0
93d5369a0be3134c6018373d5290923f3d718815,The Molecular Signatures Database Hallmark Gene Set Collection,,417-425,10.1016/J.CELS.2015.12.004,https://www.semanticscholar.org/paper/93d5369a0be3134c6018373d5290923f3d718815,5612,40,2015.0
d2c733e34d48784a37d717fe43d9e93277a8c53e,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",248-255,10.1109/CVPR.2009.5206848,https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e,51039,27,2009.0
02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.","
          3389-402
        ",10.1093/NAR/25.17.3389,https://www.semanticscholar.org/paper/02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,60088,97,1997.0
7f19972754ac0c15329666b3a6efbf569b27d8d5,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",D427 - D432,10.1093/nar/gky995,https://www.semanticscholar.org/paper/7f19972754ac0c15329666b3a6efbf569b27d8d5,3487,24,2018.0
3283f9c33e4b3fbd51d58a54dc236a92f5a98f80,The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world's largest data repository of mass spectrometry-based proteomics data. PRIDE is one of the founding members of the global ProteomeXchange (PX) consortium and an ELIXIR core data resource. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2019. The number of submitted datasets to PRIDE Archive (the archival component of PRIDE) has reached on average around 500 datasets per month during 2021. In addition to continuous improvements in PRIDE Archive data pipelines and infrastructure, the PRIDE Spectra Archive has been developed to provide direct access to the submitted mass spectra using Universal Spectrum Identifiers. As a key point, the file format MAGE-TAB for proteomics has been developed to enable the improvement of sample metadata annotation. Additionally, the resource PRIDE Peptidome provides access to aggregated peptide/protein evidences across PRIDE Archive. Furthermore, we will describe how PRIDE has increased its efforts to reuse and disseminate high-quality proteomics data into other added-value resources such as UniProt, Ensembl and Expression Atlas.",D543 - D552,10.1093/nar/gkab1038,https://www.semanticscholar.org/paper/3283f9c33e4b3fbd51d58a54dc236a92f5a98f80,2341,58,2021.0
9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",D412 - D419,10.1093/nar/gkaa913,https://www.semanticscholar.org/paper/9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,2596,28,2020.0
b204970b0503a923359bff532726666f5e0e971b,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",D590 - D596,10.1093/nar/gks1219,https://www.semanticscholar.org/paper/b204970b0503a923359bff532726666f5e0e971b,19392,25,2012.0
3aca912f21d54b3931fa1fdfac0c199c557374a4,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,1925 - 1927,10.1093/bioinformatics/btz848,https://www.semanticscholar.org/paper/3aca912f21d54b3931fa1fdfac0c199c557374a4,2423,19,2019.0
633f318876c41fed36b3905b8af5fdc27f734615,The Molecular Signatures Database (MSigDB) hallmark gene set collection.,"The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include >10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of ""hallmark"" gene sets as part of MSigDB. Each hallmark in this collection consists of a ""refined"" gene set, derived from multiple ""founder"" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.","
          417-425
        ",,https://www.semanticscholar.org/paper/633f318876c41fed36b3905b8af5fdc27f734615,3367,37,2015.0
a98753021c6a076a5307f4dfb7fd1fcb14089910,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",D211 - D222,10.1093/nar/gkp985,https://www.semanticscholar.org/paper/a98753021c6a076a5307f4dfb7fd1fcb14089910,14869,35,2007.0
68c03788224000794d5491ab459be0b2a2c38677,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",39-41,10.1145/219717.219748,https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677,15574,8,1995.0
d87ceda3042f781c341ac17109d1e94a717f5f60,Book Reviews: WordNet: An Electronic Lexical Database,"WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller's foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: ""Nouns in WordNet"" by George Miller, ""Modifiers in WordNet"" by Katherine Miller, ""A semantic network of English verbs"" by Christiane Fellbaum, and ""Design and implementation of the WordNet lexical database and search software"" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller's chapter, a section about adverbs in Katherine Miller's chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum's chapter, and Tengi's description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as ""extensions, enhancements and",13-192,,https://www.semanticscholar.org/paper/d87ceda3042f781c341ac17109d1e94a717f5f60,13033,7,1999.0
54cc1f2e86d1913521b466cef19d72ed02b6c800,Argonaute—a database for gene regulation by mammalian microRNAs,,D115 - D118,10.1186/1471-2105-8-127,https://www.semanticscholar.org/paper/54cc1f2e86d1913521b466cef19d72ed02b6c800,47783,55,2005.0
0e5bccdedb82fbafece8ca71d64b16ff05ec9145,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",D490 - D495,10.1093/nar/gkt1178,https://www.semanticscholar.org/paper/0e5bccdedb82fbafece8ca71d64b16ff05ec9145,4988,23,2013.0
57dfc18815bba1c3737dbc2e5497fd1fc595edb5,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.","
          716-21
        ",10.1099/ijs.0.038075-0,https://www.semanticscholar.org/paper/57dfc18815bba1c3737dbc2e5497fd1fc595edb5,5111,30,2012.0
66470cf9df2f932f80094a309abcc14bcc1b9373,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",D447 - D456,10.1093/nar/gkv1145,https://www.semanticscholar.org/paper/66470cf9df2f932f80094a309abcc14bcc1b9373,3304,65,2015.0
6a074a3fa856e86b2e6bc60e83d66cc488090ae9,The ecoinvent database version 3 (part I): overview and methodology,,1218-1230,10.1007/s11367-016-1087-8,https://www.semanticscholar.org/paper/6a074a3fa856e86b2e6bc60e83d66cc488090ae9,2951,57,2016.0
bbe6e5fcc96e685db714d6aa11ffe6f49567c585,A global database of COVID-19 vaccinations,,947 - 953,10.1038/s41562-021-01122-8,https://www.semanticscholar.org/paper/bbe6e5fcc96e685db714d6aa11ffe6f49567c585,1255,22,2021.0
e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.","
          380-8
        ",10.1107/S0108768102003890,https://www.semanticscholar.org/paper/e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,8987,10,2002.0
c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",14-106,,https://www.semanticscholar.org/paper/c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,5890,49,2008.0
9a1ed876196ec9733acb1daa6d65e35ff0414291,A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.,416-423 vol.2,10.1109/ICCV.2001.937655,https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291,7216,21,2001.0
8b3b8848a311c501e704c45c6d50430ab7068956,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",2556-2563,10.1109/ICCV.2011.6126543,https://www.semanticscholar.org/paper/8b3b8848a311c501e704c45c6d50430ab7068956,3327,35,2011.0
317325439a0ce543d7629848a35adea04b6e7d12,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",D344 - D354,10.1093/nar/gkaa977,https://www.semanticscholar.org/paper/317325439a0ce543d7629848a35adea04b6e7d12,1191,28,2020.0
7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",D517 - D525,10.1093/nar/gkz935,https://www.semanticscholar.org/paper/7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,1826,50,2019.0
f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.","
          1739-40
        ",10.1093/bioinformatics/btr260,https://www.semanticscholar.org/paper/f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,4265,15,2011.0
716000409a3a2e2c75801b3d58b9b17b68eeaef7,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",14-171,10.1002/JOC.1181,https://www.semanticscholar.org/paper/716000409a3a2e2c75801b3d58b9b17b68eeaef7,4094,29,2005.0
76eb8e5688ee2951e5f04fb14956abf93a890149,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",D233 - D238,10.1093/nar/gkn663,https://www.semanticscholar.org/paper/76eb8e5688ee2951e5f04fb14956abf93a890149,7082,22,2008.0
80e394ee3e1834091596e8b55c9ad9bf11456e09,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",,R60 - R60,10.1186/gb-2003-4-9-r60,https://www.semanticscholar.org/paper/80e394ee3e1834091596e8b55c9ad9bf11456e09,8319,13,2003.0
1976c9eeccc7115d18a04f1e7fb5145db6b96002,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",1247-1250,10.1145/1376616.1376746,https://www.semanticscholar.org/paper/1976c9eeccc7115d18a04f1e7fb5145db6b96002,4809,4,2008.0
c07eca9862e0144aa4c0c29c7978caa6eff60e2f,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",D141 - D145,10.1093/nar/gkn879,https://www.semanticscholar.org/paper/c07eca9862e0144aa4c0c29c7978caa6eff60e2f,4689,25,2008.0
8c1a1e761b715b23668b4f850e2bcc958fa21ad2,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.","
          5383-92
        ",10.1021/AC025747H,https://www.semanticscholar.org/paper/8c1a1e761b715b23668b4f850e2bcc958fa21ad2,4683,2,2002.0
298d799da82395a64a3bda38ef9d2a4646828ccb,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",212-219,10.1145/237814.237866,https://www.semanticscholar.org/paper/298d799da82395a64a3bda38ef9d2a4646828ccb,7201,23,1996.0
dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,The mnist database of handwritten digits,Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough.,79-169,,https://www.semanticscholar.org/paper/dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,5964,1,2005.0
80777d42513103bede188b2eebbdce7fb6f91390,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",D608 - D617,10.1093/nar/gkx1089,https://www.semanticscholar.org/paper/80777d42513103bede188b2eebbdce7fb6f91390,2571,39,2017.0
5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,,"
          536-40
        ",10.1016/S0022-2836(05)80134-2,https://www.semanticscholar.org/paper/5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,6511,22,1995.0
ceee6447b291f8052a28c9eb00ca360d6f39f9b1,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",D561 - D568,10.1093/nar/gkq973,https://www.semanticscholar.org/paper/ceee6447b291f8052a28c9eb00ca360d6f39f9b1,3318,70,2010.0
761020759f7e9f84c3ac77f59a42862cc6a6004e,Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems,"cont.): Active database semantics can be supported on an existing SQL Server (we use Oracle SQL Server as the test SQL Server) by the ECA Agent between the SQL Server and multiple clients. ECA rules are completely supported through the ECA Agent without changing applications in the SQL Server. Both primitive and composite events can be detected in the ECA Agent and actions are invoked in SQL Server. All events are persistent in RDBMS. The Java Local Event Detector (Java LED) is used to notify and detect both primitive events and composite events. The ECA Agent uses Java Database Connectivity (JDBC) to connect to the SQL server. The architecture of the ECA Agent and implementation details are shown in this thesis. Alternative approaches are discussed in details, and the features and limitations are identified. Database and Expert Systems Applications This book contains the refereed proceedings of the 8th International Conference on Database and Expert Systems Applications, DEXA '97, held in Toulouse, France, September 1997. The 62 revised full papers presented in the book, together with three invited contributions, were selected from a total of 159 submissions. The papers are organized in sections on modeling, object-oriented databases, active and temporal aspects, images, integrity constraints, multimedia databases, deductive databases and knowledge-based systems, allocation concepts, data interchange, digital libraries, transaction concepts, learning issues, optimization and performance, query languages, maintenance, Page 3/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems fed rated databases, uncertainty handling and qualitative reasoning, and software engineering and reusable software. Advanced Database Systems ""Focused on the latest research on text and document management, this guide addresses the information management needs of organizations by providing the most recent findings. How the need for effective databases to house information is impacting organizations worldwide and how some organizations that possess a vast amount of data are not able to use the data in an economic and efficient manner is demonstrated. A taxonomy for object-oriented databases, metrics for controlling database complexity, and a guide to accommodating hierarchies in relational databases are provided. Also covered is how to apply Java-triggers for X-Link management and how to build signatures."" Flexible and Efficient Information Handling This book is the proceedings of a workshop held at Heriot-Watt University in Edinburgh in August 1993. The central theme of the workshop was rules in database systems, and the papers presented covered a range of different aspects of database rule systems. These aspects are reflected in the sessions of the workshop, which are the same as the sections in this proceedings: Active Databases Architectures Incorporating Temporal Rules Rules and Transactions Analysis and Debugging of Active Rules Integrating Graphs/Objects with Deduction Integrating Deductive and Active Rules Integrity Constraints Deductive Databases The incorporation of rules into database systems is an important area of research, as it is a major component in the integration of behavioural information with the structural data with which commercial databases have traditionally been associated. This integration of the behavioural aspects of an application with the data to which it applies in database systems leads to more straightforward application development and more efficient processing of data. Many novel applications seem to need database systems in which structural and behavioural information are fully integrated. Rules are only one means of expressing behavioural information, but it is clear that different types of rule can be used to capture directly different properties of an application which are cumbersome to support using conventional database architectures. In recent years there has been a surge of research activity focusing upon active database systems, and this volume opens with a collection of papers devoted specifically to this topic. Web Information Systems -WISE 2004 Active database systems enhance traditional database functionality with powerful rule-processing capabilities, providing a uniform and efficient mechanism for many database system applications. Among these applications are integrity constraints, views, authorization, statistics gathering, monitoring and alerting, knowledge-based systems, expert systems, and workflow management. This significant collection focuses on the most prominent research projects in active database systems. The project leaders for each Page 4/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems prototype system provide detailed discussion of their projects and the relevance of their results to the future of active database systems. Features: A broad overview of current active database systems and how they can be extended and improved A comprehensive introduction to the core topics of the field, including its motivation and history Coverage of active database (trigger) capabilities in commercial products Discussion of forthcoming standards Knowledge-Based Intelligent Information and Engineering Systems Knowledge Base Systems are an integration of conventional database systems with Artificial Intelligence techniques. They provide inference capabilities to the database system by encapsulating the knowledge of the application domain within the database. Knowledge is the most valuable of all corporate resources that must be captured, stored, re-used and continuously improved, in much the same way as database systems were important in the previous decade. Flexible, extensible, and yet efficient Knowledge Base Systems are needed to capture the increasing demand for knowledge-based applications which will become a significant market in the next decade. Knowledge can be expressed in many static and dynamic forms; the most prominent being domain objects, their relationships, and their rules of evolution and transformation. It is important to express and seamlessly use all types of knowledge in a single Knowledge Base System. Parallel, Object-Oriented, and Active Knowledge Base Systems presents in detail features that a Knowledge Base System should have in order to fulfill the above requirements. Parallel, Object-Oriented, and Active Knowledge Base Systems covers in detail the following topics: Integration of deductive, production, and active rules in sequential database systems. Integration and inter-operation of multiple rule types into the same Knowledge Base System. Parallel rule matching and execution, for deductive, production, and active rules, in parallel Export, Knowledge Base, and Database Systems. In-depth description of a Parallel, Object-Oriented, and Active Knowledge Base System that integrates all rule paradigms into a single database system without hindering performance. Parallel, Object-Oriented, and Active Knowledge Base Systems is intended as a graduate-level text for a course on Knowledge Base Systems and as a reference for researchers and practitioners in the areas of database systems, knowledge base systems and Artificial Intelligence. Active, Real-Time, and Temporal Database Systems The World Wide Web has become a ubiquitous global tool, used for finding infor mation, communicating ideas, carrying out distributed computation and conducting business, learning and science. The Web is highly dynamic in both the content and quantity of the information that it encompasses. In order to fully exploit its enormous potential as a global repository of information, we need to understand how its size, topology and content are evolv ing. This then allows the development of new techniques for locating and retrieving information that are better able to adapt and scale to its change and growth. The Web's users are highly diverse and can access the Web from a variety of devices and interfaces, at different places and times, and for varying purposes. We thus also need techniques for personalising the presentation and content of Page 5/14 Online Library Active Database Systems Triggers And Rules For Advanced Database Processing The Morgan Kaufmann Series In Data Management Systems Web based information dependi g on how it i being accessed and on the specific user's requirements. As well as being accessed by human users, the Web is also accessed by appli cations. New applications in areas such as ebusiness, sensor networks, and mobile and ubiquitous computing need to be able to detect and react quickly to events and changes in Web-based information. Traditional approaches using query-based 'pull' of information to find out if events or changes of interest have occurred may not be able to scale to the quantity and frequency of events and changes being generated, and new 'push' -based techniques are needed. Advances in Databases and Information Systems This book constitutes the strictly refereed post-workshop proceedings of the International Workshop on Logic in Databases, LID'96, held in San Miniato, Italy, in July 1996, as the final meeting of an EC-US cooperative activity. The volume presents 21 revised full papers selected from 49 submissions as well as 3 invited contributions and a summary of a panel discussion on deductive databases: challenges, opportunities and future directions. The retrospective survey on logic and databases by Jack Minker deserves a special mention: it is a 56-page overview and lists 357 references. The papers are organized in sections on uncertainty, temporal and spatial reasoning, updates, active databases, semantics, advanced applications, query evaluation, language extensions, and logic constructs and expressive power. Data Management Systems This book constitutes the ",75-196,,https://www.semanticscholar.org/paper/761020759f7e9f84c3ac77f59a42862cc6a6004e,471,0,2022.0
cdad2f8ca559f425ab7fa402535354a86b0a370a,CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",40-136,10.1093/nar/gkz991,https://www.semanticscholar.org/paper/cdad2f8ca559f425ab7fa402535354a86b0a370a,1666,11,2019.0
11f647b95a7c9a94c346cd8dc53987105cb0f7c1,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.","
          308-11
        ",10.1093/NAR/29.1.308,https://www.semanticscholar.org/paper/11f647b95a7c9a94c346cd8dc53987105cb0f7c1,6550,7,2001.0
6dd9508b8311852afec88bce55283551da5aa7b7,The IPD-IMGT/HLA Database,"Abstract It is 24 years since the IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, was first released, providing the HLA community with a searchable repository of highly curated HLA sequences. The database now contains over 35 000 alleles of the human Major Histocompatibility Complex (MHC) named by the WHO Nomenclature Committee for Factors of the HLA System. This complex contains the most polymorphic genes in the human genome and is now considered hyperpolymorphic. The IPD-IMGT/HLA Database provides a stable and user-friendly repository for this information. Uptake of Next Generation Sequencing technology in recent years has driven an increase in the number of alleles and the length of sequences submitted. As the size of the database has grown the traditional methods of accessing and presenting this data have been challenged, in response, we have developed a suite of tools providing an enhanced user experience to our traditional web-based users while creating new programmatic access for our bioinformatics user base. This suite of tools is powered by the IPD-API, an Application Programming Interface (API), providing scalable and flexible access to the database. The IPD-API provides a stable platform for our future development allowing us to meet the future challenges of the HLA field and needs of the community.",D1053 - D1060,10.1093/nar/gkac1011,https://www.semanticscholar.org/paper/6dd9508b8311852afec88bce55283551da5aa7b7,341,45,2022.0
d7c78b7071ea150346320e5b43a03824263e0fa9,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",D633 - D642,10.1093/nar/gkt1244,https://www.semanticscholar.org/paper/d7c78b7071ea150346320e5b43a03824263e0fa9,3472,45,2013.0
82524ddee00fa0895dfca43995a7ec8bdb16f0d5,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",81-141,,https://www.semanticscholar.org/paper/82524ddee00fa0895dfca43995a7ec8bdb16f0d5,4468,0,1989.0
d364903a626ad70e6ce057209d9b7e004dafd4be,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.","
          325-7
        ",10.1093/NAR/30.1.325,https://www.semanticscholar.org/paper/d364903a626ad70e6ce057209d9b7e004dafd4be,4494,9,2002.0
b7599c8ba88e7c93edbce57df513152e8f5693e7,The COG database: an updated version includes eukaryotes,,41 - 41,10.1186/1471-2105-4-41,https://www.semanticscholar.org/paper/b7599c8ba88e7c93edbce57df513152e8f5693e7,4205,65,2003.0
1f53996347086be3bd3a32da0976ba2db7687988,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",D127 - D131,10.1093/nar/gkz757,https://www.semanticscholar.org/paper/1f53996347086be3bd3a32da0976ba2db7687988,1543,17,2019.0
bb967168ead7a14adcb0121dcf24a930d1a383b3,The HITRAN 2008 molecular spectroscopic database,,139-204,10.1016/J.JQSRT.2017.06.038,https://www.semanticscholar.org/paper/bb967168ead7a14adcb0121dcf24a930d1a383b3,7544,880,2005.0
4bd970a37c59c97804ff93cbb2c108e081de3a37,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",235-244,10.1093/IJL/3.4.235,https://www.semanticscholar.org/paper/4bd970a37c59c97804ff93cbb2c108e081de3a37,5525,103,1990.0
6ad9053940676fca029dabdc7937e5d854df61e0,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",D290 - D301,10.1093/nar/gkr1065,https://www.semanticscholar.org/paper/6ad9053940676fca029dabdc7937e5d854df61e0,6763,60,2011.0
62f5ffb09a4c9543509c38f005b9c6eb308c6974,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.","
          33-6
        ",10.1093/nar/28.1.33,https://www.semanticscholar.org/paper/62f5ffb09a4c9543509c38f005b9c6eb308c6974,3711,18,2000.0
0e466ea033b982519f351022304dccb64a46b93c,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",D948 - D955,10.1093/nar/gkz950,https://www.semanticscholar.org/paper/0e466ea033b982519f351022304dccb64a46b93c,1512,60,2019.0
072a0db716fb6f8332323f076b71554716a7271c,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",45-50,10.1109/51.932724,https://www.semanticscholar.org/paper/072a0db716fb6f8332323f076b71554716a7271c,3203,17,2001.0
9667f8264745b626c6173b1310e2ff0298b09cfc,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",487-495,,https://www.semanticscholar.org/paper/9667f8264745b626c6173b1310e2ff0298b09cfc,2854,25,2014.0
c369c9a40a36b013be3ef9c19a068f2d6578e4c3,Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible in ARB,"Title: Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible with ARB Authors: DeSantis, T.Z. 1 Hugenholtz, P. 2 Larsen, N. 3 Rojas, M. 4 Brodie, E.L. 1 Keller, K. 5 Huber, T. 6 Dalevi, D. 7 Hu, P. 1 Andersen, G.L. 1 Center for Environmental Biotechnology Lawrence Berkeley National Laboratory 1 Cyclotron Road, Mail Stop 70A-3317 Berkeley, CA 94720 USA Microbial Ecology Program DOE Joint Genome Institute 2800 Mitchell Drive Bldg 400-404 Walnut Creek, CA 94598 USA Danish Genome Institute Gustav Wieds vej 10 C DK-8000 Aarhus C Denmark Department of Bioinformatics Baylor University P.O. Box 97356, 1311 S. 5th St. Waco, TX 76798-7356 USA Department of Bioengineering University of California Berkeley, CA 94720 USA Departments of Biochemistry and Mathematics The University of Queensland Brisbane Qld 4072 Australia Department of Computer Science Chalmers University of Technology",32-188,,https://www.semanticscholar.org/paper/c369c9a40a36b013be3ef9c19a068f2d6578e4c3,5660,0,2006.0
288b317e427c6bf4c94d455049bd1368ff2071eb,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",D339 - D343,10.1093/nar/gky1006,https://www.semanticscholar.org/paper/288b317e427c6bf4c94d455049bd1368ff2071eb,1235,21,2018.0
3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,,13 - 13,10.1186/1758-2946-6-13,https://www.semanticscholar.org/paper/3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,2843,41,2014.0
95162f20fa22a8cfe84b74aa118f18a6f04eb1ab,"Repbase Update, a database of repetitive elements in eukaryotic genomes",,82-107,10.1186/s13100-015-0041-9,https://www.semanticscholar.org/paper/95162f20fa22a8cfe84b74aa118f18a6f04eb1ab,1944,32,2015.0
41abf43dc718e271299457bce65bccfe3feeb9d6,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",5069 - 5072,10.1128/AEM.03006-05,https://www.semanticscholar.org/paper/41abf43dc718e271299457bce65bccfe3feeb9d6,3485,33,2006.0
908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",3485-3492,10.1109/CVPR.2010.5539970,https://www.semanticscholar.org/paper/908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,3014,34,2010.0
46f74231b9afeb0c290d6d550043c55045284e5f,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",141-142,10.1109/MSP.2012.2211477,https://www.semanticscholar.org/paper/46f74231b9afeb0c290d6d550043c55045284e5f,3002,7,2012.0
5a2892f91addeea2f4600d28b23e684be32f5b2c,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",18-31,10.1109/T-AFFC.2011.15,https://www.semanticscholar.org/paper/5a2892f91addeea2f4600d28b23e684be32f5b2c,3057,60,2012.0
092c275005ae49dc1303214f6d02d134457c7053,LabelMe: A Database and Web-Based Tool for Image Annotation,,157-173,10.1007/s11263-007-0090-8,https://www.semanticscholar.org/paper/092c275005ae49dc1303214f6d02d134457c7053,3622,57,2008.0
90bc0ca3feebe0215079cf575b90017170a0089f,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",D222 - D226,10.1093/nar/gku1221,https://www.semanticscholar.org/paper/90bc0ca3feebe0215079cf575b90017170a0089f,2928,13,2014.0
e8104b335a4e499f7b79b913a92d23263c82f6b6,The HITRAN2012 molecular spectroscopic database,,98-191,10.1016/j.jqsrt.2013.07.002,https://www.semanticscholar.org/paper/e8104b335a4e499f7b79b913a92d23263c82f6b6,3246,341,2013.0
cb56121bc38e0f4b44bcb5296a12038626152e96,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",D566 - D573,10.1093/nar/gkw1004,https://www.semanticscholar.org/paper/cb56121bc38e0f4b44bcb5296a12038626152e96,1791,20,2016.0
960e7494ef4ec5964407488080f249104cd218f0,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed® database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 34 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface and NCBI datasets. Additional resources that were updated in the past year include PMC, Bookshelf, Genome Data Viewer, SRA, ClinVar, dbSNP, dbVar, Pathogen Detection, BLAST, Primer-BLAST, IgBLAST, iCn3D and PubChem. All of these resources can be accessed through the NCBI home page at https://www.ncbi.nlm.nih.gov.",27-192,10.1093/nar/gkaa892,https://www.semanticscholar.org/paper/960e7494ef4ec5964407488080f249104cd218f0,777,28,2020.0
6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,The AR face database,,12-139,,https://www.semanticscholar.org/paper/6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,3993,0,1998.0
d71af418eeb9f5a68062929bae12af74773ffcb2,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",D945 - D954,10.1093/nar/gkw1074,https://www.semanticscholar.org/paper/d71af418eeb9f5a68062929bae12af74773ffcb2,1638,61,2016.0
3a2b869533620d2dfa076522321983c537b3c175,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.","
          D258-61
        ",10.1093/nar/gkh036,https://www.semanticscholar.org/paper/3a2b869533620d2dfa076522321983c537b3c175,3987,14,2004.0
a8db50edfe26a6ae33a6787e2049de5bacd18666,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",D1100 - D1107,10.1093/nar/gkr777,https://www.semanticscholar.org/paper/a8db50edfe26a6ae33a6787e2049de5bacd18666,3097,40,2011.0
bc744742f1644c9cab6b9535ab0bd6f2eed320bb,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",D225 - D229,10.1093/nar/gkq1189,https://www.semanticscholar.org/paper/bc744742f1644c9cab6b9535ab0bd6f2eed320bb,3003,13,2010.0
b307d55ba07058d6183991d2d2a81b340d558186,"NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",D501 - D504,10.1093/nar/gki025,https://www.semanticscholar.org/paper/b307d55ba07058d6183991d2d2a81b340d558186,3229,14,2004.0
61533dd9e41f20e2f5deaf22afb04c94b4071eac,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",462 - 467,10.1159/000084979,https://www.semanticscholar.org/paper/61533dd9e41f20e2f5deaf22afb04c94b4071eac,2959,37,2005.0
f8928221d290a9cdd84d1de52e121373bc836caa,New tools in comparative political economy : the database of political institutions,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",165-176,10.1093/WBER/15.1.165,https://www.semanticscholar.org/paper/f8928221d290a9cdd84d1de52e121373bc836caa,2929,9,2001.0
50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.","
          177-82
        ",10.1021/ci049714+,https://www.semanticscholar.org/paper/50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,3237,32,2005.0
3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.","
          297-300
        ",10.1093/nar/27.1.297,https://www.semanticscholar.org/paper/3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,3173,7,1999.0
5cf0d213f3253cd46673d955209f8463db73cc51,IEMOCAP: interactive emotional dyadic motion capture database,,335-359,10.1007/S10579-008-9076-6,https://www.semanticscholar.org/paper/5cf0d213f3253cd46673d955209f8463db73cc51,2573,69,2008.0
804836b8ad86ef8042e3dcbd45442a52f031ee03,A Database and Evaluation Methodology for Optical Flow,,1-31,10.1007/s11263-010-0390-2,https://www.semanticscholar.org/paper/804836b8ad86ef8042e3dcbd45442a52f031ee03,2669,105,2007.0
e274c1b6e17825feab52de205fd0bc4917d5be6c,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,,289-302,10.1023/A:1008392405740,https://www.semanticscholar.org/paper/e274c1b6e17825feab52de205fd0bc4917d5be6c,2883,71,1999.0
58a63e11a45dfdb50d994454ead70243626d8ed1,Encyclopedia of Database Systems,,28-141,10.1007/978-1-4899-7993-3,https://www.semanticscholar.org/paper/58a63e11a45dfdb50d994454ead70243626d8ed1,614,0,2020.0
e3f2391513693647e0ea87bfa86cd89e468f51d0,Comprehensive database for facial expression analysis,"Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",46-53,10.1109/AFGR.2000.840611,https://www.semanticscholar.org/paper/e3f2391513693647e0ea87bfa86cd89e468f51d0,2805,22,2000.0
8c8b7c1adb6f077bb3045928767b8bc6763e0c06,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",D801 - D807,10.1093/nar/gks1065,https://www.semanticscholar.org/paper/8c8b7c1adb6f077bb3045928767b8bc6763e0c06,2659,22,2012.0
73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,The Gene Expression Omnibus Database,,"
          93-110
        ",10.1007/978-1-4939-3578-9_5,https://www.semanticscholar.org/paper/73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,1314,25,2016.0
8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",D529 - D541,10.1093/nar/gky1079,https://www.semanticscholar.org/paper/8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,1040,97,2018.0
ea35cd7fd2c46f86232f21ef73239f34f2d180a6,Database resources of the National Center for Biotechnology Information.,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 35 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface, a sequence database search and a gene orthologs page. Additional resources that were updated in the past year include PMC, Bookshelf, My Bibliography, Assembly, RefSeq, viral genomes, the prokaryotic genome annotation pipeline, Genome Workbench, dbSNP, BLAST, Primer-BLAST, IgBLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",33-178,10.1093/nar/gkz899,https://www.semanticscholar.org/paper/ea35cd7fd2c46f86232f21ef73239f34f2d180a6,617,27,2019.0
eb960b5d56ed1368991eaa4f40cb7afee66edb1f,ONCOMINE: a cancer microarray database and integrated data-mining platform.,,"
          1-6
        ",10.1016/S1476-5586(04)80047-2,https://www.semanticscholar.org/paper/eb960b5d56ed1368991eaa4f40cb7afee66edb1f,3345,47,2004.0
2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",2621 - 2632,10.1104/pp.104.046367,https://www.semanticscholar.org/paper/2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,2538,43,2004.0
ba90ae48b30594b57a5ca7bfd37cae150458ecfa,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",D380 - D386,10.1093/nar/gkx1013,https://www.semanticscholar.org/paper/ba90ae48b30594b57a5ca7bfd37cae150458ecfa,1137,22,2017.0
26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,"The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",D624 - D632,10.1093/nar/gkx1134,https://www.semanticscholar.org/paper/26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,1063,29,2017.0
dc8b25e35a3acb812beb499844734081722319b4,The FERET database and evaluation procedure for face-recognition algorithms,,295-306,10.1016/S0262-8856(97)00070-X,https://www.semanticscholar.org/paper/dc8b25e35a3acb812beb499844734081722319b4,2520,28,1998.0
f89df7381ea8febb419fae473725e44931f6b22c,"Generation and analysis of a 29,745 unique Expressed Sequence Tags from the Pacific oyster (Crassostrea gigas) assembled into a publicly accessible database: the GigasDatabase",,341 - 341,10.1186/1471-2164-10-341,https://www.semanticscholar.org/paper/f89df7381ea8febb419fae473725e44931f6b22c,2860,53,2009.0
